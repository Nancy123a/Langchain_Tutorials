{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7c1f264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fe5dae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (AIMessage,HumanMessage,SystemMessage)\n",
    "os.environ['OPENAI_API_KEY']= os.environ['openai_key']\n",
    "\n",
    "speech=\"\"\"\n",
    "People across the country, involved in government, political, and social activities, are dedicating their time to make the ‘Viksit Bharat Sankalp Yatra’ (Developed India Resolution Journey) successful. Therefore, as a Member of Parliament, it was my responsibility to also contribute my time to this program. So, today, I have come here just as a Member of Parliament and your ‘sevak’, ready to participate in this program, much like you.\n",
    "\n",
    "In our country, governments have come and gone, numerous schemes have been formulated, discussions have taken place, and big promises have been made. However, my experience and observations led me to believe that the most critical aspect that requires attention is ensuring that the government’s plans reach the intended beneficiaries without any hassles. If there is a ‘Pradhan Mantri Awas Yojana’ (Prime Minister’s housing scheme), then those who are living in jhuggis and slums should get their houses. And he should not need to make rounds of the government offices for this purpose. The government should reach him. Since you have assigned this responsibility to me, about four crore families have got their ‘pucca’ houses. However, I have encountered cases where someone is left out of the government benefits. Therefore, I have decided to tour the country again, to listen to people’s experiences with government schemes, to understand whether they received the intended benefits, and to ensure that the programs are reaching everyone as planned without paying any bribes. We will get the real picture if we visit them again. Therefore, this ‘Viksit Bharat Sankalp Yatra’ is, in a way, my own examination. I want to hear from you and the people across the country whether what I envisioned and the work I have been doing aligns with reality and whether it has reached those for whom it was meant.\n",
    "\n",
    "It is crucial to check whether the work that was supposed to happen has indeed taken place. I recently met some individuals who utilized the Ayushman card to get treatment for serious illnesses. One person met with a severe accident, and after using the card, he could afford the necessary operation, and now he is recovering well. When I asked him, he said: “How could I afford this treatment? Now that there is the Ayushman card, I mustered courage and underwent an operation. Now I am perfectly fine.”  Such stories are blessings to me.\n",
    "\n",
    "The bureaucrats, who prepare good schemes, expedite the paperwork and even allocate funds, also feel satisfied that 50 or 100 people who were supposed to get the funds have got it. The funds meant for a thousand villages have been released. But their job satisfaction peaks when they hear that their work has directly impacted someone’s life positively. When they see the tangible results of their efforts, their enthusiasm multiplies. They feel satisfied. Therefore, ‘Viksit Bharat Sankalp Yatra’ has had a positive impact on government officers. It has made them more enthusiastic about their work, especially when they witness the tangible benefits reaching the people. Officers now feel satisfied with their work, saying, “I made a good plan, I created a file, and the intended beneficiaries received the benefits.” When they find that the money has reached a poor widow under the Jeevan Jyoti scheme and it was a great help to her during her crisis, they realise that they have done a good job. When a government officer listens to such stories, he feels very satisfied.\n",
    "\n",
    "There are very few who understand the power and impact of the ‘Viksit Bharat Sankalp Yatra’. When I hear people connected to bureaucratic circles talking about it, expressing their satisfaction, it resonates with me. I’ve heard stories where someone suddenly received 2 lakh rupees after the death of her husband, and a sister mentioned how the arrival of gas in her home transformed her lives. The most significant aspect is when someone says that the line between rich and poor has vanished. While the slogan ‘Garibi Hatao’ (Remove Poverty) is one thing, but the real change happens when a person says, “As soon as the gas stove came to my house, the distinction between poverty and affluence disappeared.\n",
    "\"\"\"\n",
    "\n",
    "chat_messages=[\n",
    "    SystemMessage(content='You are an expert assistant with expertize in summarizing speeches'),\n",
    "    HumanMessage(content=f'Please provide a short and concise summary of the following speech:\\n TEXT: {speech}')\n",
    "]\n",
    "\n",
    "llm=ChatOpenAI(model_name='gpt-3.5-turbo')\n",
    "llm.get_num_tokens(speech)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1604b21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/xbd75fl13wvf24ktcp53cf1h0000gn/T/ipykernel_97602/1441627176.py:1: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  llm(chat_messages).content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The speech highlights the importance of ensuring government schemes reach the intended beneficiaries smoothly. The speaker, a Member of Parliament, discusses the impact of the 'Viksit Bharat Sankalp Yatra' in ensuring government programs reach people effectively. The Yatra aims to assess the real impact of government initiatives, validate success stories like the Ayushman card, and boost morale among bureaucrats by witnessing tangible benefits reaching citizens. The speech emphasizes the significance of bridging the gap between the rich and poor through successful implementation of government schemes.\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(chat_messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f314512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWrite a summary of the following speech:\\nSpeech : `\\nPeople across the country, involved in government, political, and social activities, are dedicating their time to make the ‘Viksit Bharat Sankalp Yatra’ (Developed India Resolution Journey) successful. Therefore, as a Member of Parliament, it was my responsibility to also contribute my time to this program. So, today, I have come here just as a Member of Parliament and your ‘sevak’, ready to participate in this program, much like you.\\n\\nIn our country, governments have come and gone, numerous schemes have been formulated, discussions have taken place, and big promises have been made. However, my experience and observations led me to believe that the most critical aspect that requires attention is ensuring that the government’s plans reach the intended beneficiaries without any hassles. If there is a ‘Pradhan Mantri Awas Yojana’ (Prime Minister’s housing scheme), then those who are living in jhuggis and slums should get their houses. And he should not need to make rounds of the government offices for this purpose. The government should reach him. Since you have assigned this responsibility to me, about four crore families have got their ‘pucca’ houses. However, I have encountered cases where someone is left out of the government benefits. Therefore, I have decided to tour the country again, to listen to people’s experiences with government schemes, to understand whether they received the intended benefits, and to ensure that the programs are reaching everyone as planned without paying any bribes. We will get the real picture if we visit them again. Therefore, this ‘Viksit Bharat Sankalp Yatra’ is, in a way, my own examination. I want to hear from you and the people across the country whether what I envisioned and the work I have been doing aligns with reality and whether it has reached those for whom it was meant.\\n\\nIt is crucial to check whether the work that was supposed to happen has indeed taken place. I recently met some individuals who utilized the Ayushman card to get treatment for serious illnesses. One person met with a severe accident, and after using the card, he could afford the necessary operation, and now he is recovering well. When I asked him, he said: “How could I afford this treatment? Now that there is the Ayushman card, I mustered courage and underwent an operation. Now I am perfectly fine.”  Such stories are blessings to me.\\n\\nThe bureaucrats, who prepare good schemes, expedite the paperwork and even allocate funds, also feel satisfied that 50 or 100 people who were supposed to get the funds have got it. The funds meant for a thousand villages have been released. But their job satisfaction peaks when they hear that their work has directly impacted someone’s life positively. When they see the tangible results of their efforts, their enthusiasm multiplies. They feel satisfied. Therefore, ‘Viksit Bharat Sankalp Yatra’ has had a positive impact on government officers. It has made them more enthusiastic about their work, especially when they witness the tangible benefits reaching the people. Officers now feel satisfied with their work, saying, “I made a good plan, I created a file, and the intended beneficiaries received the benefits.” When they find that the money has reached a poor widow under the Jeevan Jyoti scheme and it was a great help to her during her crisis, they realise that they have done a good job. When a government officer listens to such stories, he feels very satisfied.\\n\\nThere are very few who understand the power and impact of the ‘Viksit Bharat Sankalp Yatra’. When I hear people connected to bureaucratic circles talking about it, expressing their satisfaction, it resonates with me. I’ve heard stories where someone suddenly received 2 lakh rupees after the death of her husband, and a sister mentioned how the arrival of gas in her home transformed her lives. The most significant aspect is when someone says that the line between rich and poor has vanished. While the slogan ‘Garibi Hatao’ (Remove Poverty) is one thing, but the real change happens when a person says, “As soon as the gas stove came to my house, the distinction between poverty and affluence disappeared.\\n`\\nTranslate the precise summary to Hindi.\\n\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prompt template text summarization\n",
    "from langchain.chains import LLMChain\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "generic_template='''\n",
    "Write a summary of the following speech:\n",
    "Speech : `{speech}`\n",
    "Translate the precise summary to {language}.\n",
    "\n",
    "'''\n",
    "prompt=PromptTemplate(\n",
    "    input_variables=['speech','language'],\n",
    "    template=generic_template\n",
    ")\n",
    "\n",
    "prompt.format(speech=speech,language='Hindi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e9e7244",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/xbd75fl13wvf24ktcp53cf1h0000gn/T/ipykernel_97602/1678963002.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
      "/var/folders/r7/xbd75fl13wvf24ktcp53cf1h0000gn/T/ipykernel_97602/1678963002.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  summary=llm_chain.run({'speech':speech,'language':'hindi'})\n"
     ]
    }
   ],
   "source": [
    "complete_prompt=prompt.format(speech=speech,language='Hindi')\n",
    "llm_chain=LLMChain(llm=llm,prompt=prompt)\n",
    "summary=llm_chain.run({'speech':speech,'language':'hindi'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d8d6be5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'भारत भर में लोग, सरकारी, राजनीतिक और सामाजिक गतिविधियों में शामिल लोग, \\'विकसित भारत संकल्प यात्रा\\' को सफल बनाने के लिए अपना समय दे रहे हैं। इसलिए, मैं संसद के सदस्य के रूप में इस कार्यक्रम में अपना समय देने की जिम्मेदारी संभालना चाहता था। इसलिए, आज मैं यहाँ सिर्फ संसद के सदस्य और आपका \\'सेवक\\' के रूप में पहुंचा हूं, इस कार्यक्रम में आपके साथ शामिल होने के लिए तैयार हूं।\\n\\nमेरे देश में, सरकारें आईं और गईं, बेहद स्कीमें तैयार की गईं, चर्चाएं हुईं, और बड़े वादे किए गए हैं। हालांकि, मेरे अनुभव और अवलोकन ने मुझे यह महसूस कराया कि सबसे महत्वपूर्ण पहलू यह है कि सुनिश्चित किया जाए कि सरकार की योजनाएँ इच्छित लाभार्थियों तक कोई परेशानी के बिना पहुंचती हैं। यदि \\'प्रधानमंत्री आवास योजना\\' है, तो जो लोग झुग्गियों और स्लम्स में रह रहे हैं, उन्हें उनके घर मिलने चाहिए। और उसे इस लिए सरकारी दफ्तरों के चक्कर काटने की आवश्यकता नहीं होनी चाहिए। सरकार को उसके पास पहुंचना चाहिए। जब आपने मुझे इस जिम्मेदारी सौंपी है, तो लगभग चार करोड़ परिवारों को उनके \\'पुक्के\\' घर मिल गए हैं। हालांकि, मुझे ऐसे मामले भी आते हैं जहां किसी को सरकारी लाभ से वंचित छोड़ दिया गया है। इसलिए, मैंने फिर से देश की यात्रा करने का निश्चय किया है, लोगों के सरकारी योजनाओं के साथ अनुभव सुनने के लिए, समझने के लिए कि क्या वे इच्छित लाभ प्राप्त कर रहे हैं, और यह सुनिश्चित करने के लिए कि कार्यक्रम सभी को पहुंच रहा है जैसा निर्धारित किया गया था, किसी भी रिश्वत न देकर। हमें यदि हम उन्हें फिर से मिलते हैं, तो हमें असली चित्र मिलेगा। इसलिए, यह \\'विकसित भारत संकल्प यात्रा\\' मेरी एक प्रकार स्वयं की परीक्षा है। मैं आपसे और पूरे देश के लोगों से सुनना चाहता हूं कि मैंने जो दृष्टि दी और काम किया है, क्या यह वास्तविकता के साथ मेल खाता है और क्या यह उन लोगों तक पहुंचा है जिनके लिए यह था।\\n\\nइसे जांचना महत्वपूर्ण है कि वह काम जो होना चाहिए वास्तव में हुआ है या नहीं।हाल ही में मैंने कुछ व्यक्तियों से मिला जो गंभीर बीमारियों का इलाज करवाने के लिए आयुष्मान कार्ड का उपयोग किया। एक व्यक्ति का गंभीर हादसा हुआ, और कार्ड का उपयोग करने के बाद, वह आवश्यक ऑपरेशन का मुआवजा दे सकता था, और अब वह ठीक हो रहा है। जब मैंने उससे पूछा, तो उसने कहा: \"मैं इस इलाज को कैसे संभाल सकता था? अब जब आयुष्मान कार्ड है, तो मुझे हिम्मत जुटाकर ऑपरेशन करवाया। अब मैं पूरी तरह स्वस्थ हूं।\" ऐसी कहानियाँ मेरे लिए आशीर्वाद हैं।\\n\\nब्यूरोक्रेट्स, जो अच्छी योजनाएँ तैयार करते हैं, कागजात की प्रक्रिया को गति देते हैं और धन का आवंटन भी करते हैं, वे भी खुश हो जाते हैं जब उन्हें पता चलता है कि 50 या 100 व्यक्तियों को जिन्हें धन प्राप्त होना चाहिए था, उसे मिल गया है। हजार गांवों के लिए निर्धारित धन विमुक्त किया गया है। लेकिन जब वे यह सुनते हैं कि उनके काम का सीधा प्रभाव किसी के जीवन पर सकारात्मक रूप से पड़ा है, तो उनकी उत्साहिता अधिक हो जाती है। वे खुश हो जाते हैं। इसलिए, \\'विकसित भारत संकल्प यात्रा\\' ने सरकारी अधिकारियों पर सकारात्मक प्रभाव डाला है। इसने उन्हें उनके काम के प्रति अधिक उत्साही बना दिया है, विशेषकर जब उन्हें देखते हैं कि प्राप्त लाभ लोगों तक पहुंच रहा है। अधिकारी अब अपने काम से संतुष्ट महसूस करते हैं, कहते हैं, \"मैंने एक अच्छी योजना बनाई, मैंने एक फाइल बनाई, और इच्छित लाभार्थियों को लाभ पहुंचाया।\" जब उन्हें देखने को मिलता है कि पूर्नजीवन ज्योति योजना के अंतर्गत एक गरीब विधवा के पास पैसे पहुंचे हैं और उसे उसकी संकट के दौरान बड़ी मदद मिली है, तो उन्हें यह अच्छा काम किया है का अहसास होता है। जब कोई सरकारी अधिकारी ऐसी कहानियाँ सुनता है, तो वह बहुत संतुष्ट महसूस करता है।\\n\\nबहुत कम लोग हैं जो \\'विकसित भारत संकल्प यात्रा\\' की शक्ति और प्रभाव को समझते हैं। जब मैं ब्यूरोक्रेटिक वर्गों से जुड़े लोगों को इसके बारे में बात करते हुए सुनता हूं, उनकी संतोषप्राप्ति का एहसास होता है। मुझे कहानियाँ सुनने को मिली हैं जहां किसी के पति की मौत के बाद अचानक किसी को 2 लाख रुपये मिले, और एक बहन ने कैसे उनके घर में गैस का आगमन उनकी जिंदगी को बदल दिया। सबसे महत्वपूर्ण बात यह है जब कोई कहता है कि गरीबी और समृद्धि के बीच की रेखा गायब हो गई है। जबकि नारा \\'गरीबी हटाओ\\' एक बात है, लेकिन असली परिवर्तन तब होता है जब कोई व्यक्ति कहता है, \"मेरे घर में गैस का चूल्हा आते ही, गरीबी और समृद्धि के बीच का अंतःकरण गायब हो गया।\"'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c98de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Final Project: RNN-Transducer-based\\nLosses for Speech Recognition on\\nNoisy Targets\\nVladimir Bataev\\nUniversity of London\\n10 March 2024arXiv:2504.06963v1  [eess.AS]  9 Apr 2025Contents\\n1 Project Concept and Motivation 3\\n1.1 Overview and Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 Related Work: Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.4 Related Work: Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.5 Related Work: Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Literature Review 5\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 CTC and Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.1 Connectionist Temporal Classification (CTC) loss . . . . . . . . . . . 5\\n2.2.2 Wild-card CTC (W-CTC) . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.3 Star Temporal Classification (STC) . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Bypass Temporal Classification (BTC) . . . . . . . . . . . . . . . . . 7\\n2.2.5 Omni-temporal Classification (OTC) . . . . . . . . . . . . . . . . . . 7\\n2.3 RNN-Transducer and Modifications . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.3.1 RNN-T loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.3.2 Graph-based RNN-Transducer framework . . . . . . . . . . . . . . . 8\\n2.3.3 Stateless RNN-T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3.4 Hybrid CTC-Transducer models . . . . . . . . . . . . . . . . . . . . . 9\\n3 Project Design 9\\n3.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.4 Backbone Encoder Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.5 Tools and Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.6 RNN-Transducer Details and Graph-based representation . . . . . . . . . . 11\\n3.7 Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.7.1 Initial Project Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.7.2 Project Plan Reflection . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Implementation 15\\n4.1 Project Implementation Overview . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.3 Model and Training Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.4 Proposed Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.4.1 Star-Transducer (Star-T) . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.4.2 Bypass-Transducer (Bypass-T) . . . . . . . . . . . . . . . . . . . . . 17\\n4.4.3 Target-Robust-Transducer (TRT) . . . . . . . . . . . . . . . . . . . . 19\\n5 Evaluation 20\\n5.1 Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n5.2 Error Impact Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.3 Dealing with Deletions: Star Transducer . . . . . . . . . . . . . . . . . . . . 21\\n5.4 Dealing with Insertions: Bypass Transducer . . . . . . . . . . . . . . . . . . 22\\n5.5 Dealing with Substitutions: Target-Robust-Transducer . . . . . . . . . . . . 22\\n5.6 Arbitrary Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n16 Conclusion 24\\n7 Report Parameters and Additional Notes 24\\n8 Acknowledgments 25\\nReferences 25\\nAppendices 29\\nA Arbitrary Errors - Learning Curve 29\\nB Bypass-Transducer: Extended evaluation of hyperparameters 29\\nC Losses Visualization 30\\nList of Figures\\n1 RNN-Transducer Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2 WFSTs for RNN-Transducer, following [1] . . . . . . . . . . . . . . . . . . . 14\\n3 Project Plan (Gantt Chart) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4 WFSTs for Star-Transducer. ⟨sf⟩is a special symbol indicating skipping\\nthe frame. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5 WFSTs for Bypass-Transducer. ⟨st⟩is a special symbol indicating skipping\\nthe token. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n6 WFSTs for Target-Robust-Transducer. ⟨sf⟩is a special symbol indicating\\nskipping the frame. ⟨st⟩is a special symbol indicating skipping the token. . 20\\n7 Arbitrary errors: learning curves for RNN-T (original and corrupted data)\\nand Target Robust Transducer (corrupted data). . . . . . . . . . . . . . . . . 29\\nList of Tables\\n1 Baseline on LibriSpeech, WER [%]. . . . . . . . . . . . . . . . . . . . . . . 20\\n2 Training RNN-T on data with errors, Fast Conformer, 60 epochs, WER [%]. 21\\n3 Star-Transducer (Star-T) Loss for deletions, Fast Conformer, 60 epochs,\\nWER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4 Bypass-Transducer (Bypass-T) Loss for insertions, Fast Conformer, 60 epochs,\\nWER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n5 Target Robust Transducer (TRT) Loss for substitutions, Fast Conformer, 60\\nepochs, WER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n6 Target Robust Transducer (TRT) Loss for arbitrary errors, Fast Conformer,\\n60 and 100 epochs, WER [%]. 50% of data is corrupted, using 15% for\\neach class of errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n7 Bypass-Transducer Loss for insertions, Fast Conformer, 60 epochs, WER\\n[%]. Extended evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n21 Project Concept and Motivation\\n1.1 Overview and Template\\nIn our project, we will train an automatic speech recognition (ASR) system on noisy\\ntargets. We start with the template \"CM3015 Machine Learning and Neural Networks,\\nTheme 1: Deep Learning on a public dataset,\" which describes the task of choosing a\\npublicly available dataset and training a deep learning model on it. So, we will work with a\\nneural network-based end-to-end ASR system, using LibriSpeech [2] dataset, a popular\\nacademic benchmark. We limit our task to RNN-Transducer [3] systems, which are widely\\nused in production and provide state-of-the-art quality [4] in most cases.\\nWe are going beyond the standard task and focusing our research on making RNN-\\nTransducer systems robust to noisy targets: unlike well-curated datasets, in the indus-\\ntry, the training data contains different errors due to the unreliability of the transcription\\nsources or the inability to transcribe noisy speech accurately. To solve the problem of\\ntraining on the noisy data, we will analyze the impact of different types of errors in training\\ndata on the quality of the RNN-Transducer system and explore different loss modifications\\nto overcome the problem. We will construct the artificial training data by mutating correct\\ntranscripts from the LibriSpeech [2] training part, similar to the approaches used in the\\nrelated work, and try to achieve the best possible quality on the development and test\\ndata standard for LibriSpeech.\\n1.2 Motivation\\nTraining ASR systems usually requires a large amount of well-transcribed audio-text paired\\ndata. The process of dataset preparation often calls for filtering out \"noisy\" transcriptions\\nusing some pretrained model, which results in a lower amount of available data for train-\\ning. It is hard to obtain large, well-transcribed datasets for many languages and scenarios.\\nOn the other hand, data with limited transcript quality is widely available. Developing new\\napproaches for working with weakly supervised setups can be beneficial in the following\\nways:\\n• (1) making it easier to use non-well-curated datasets;\\n• (2) improving the quality of ASR models without tricky filtering pipelines for raw data,\\nmaking an approach fully \"end-to-end,\" and potentially getting benefits from more\\nnon-filtered data;\\n• (3) improving ASR quality for low-resource languages when the data is extremely\\nlimited;\\n• (4) transfer learning scenarios when another (imperfect) model transcribes the data.\\n1.3 Related Work: Systems\\nMost of the modern ASR systems use mel filter bank features extracted from the speech\\nsignal [3] and learn to map a sequence of feature vectors to the correct sequence of the\\nunits derived from the text (e.g., characters, subword units, words, or phonemes). There\\nare three dominating types of end-to-end ASR systems [4]: Connectionist Temporal Clas-\\nsification (CTC) [5], RNN-Transducer (RNN-T) [3] and attention-based encoder-decoder\\n3(AED) [6]. CTC and RNN-Transducer rely on an explicit latent monotonic alignment be-\\ntween the audio and corresponding transcript [7]. CTC is the most straightforward non-\\nautoregressive system, which predicts each text unit independently. RNN-Transducer\\nwas introduced as a solution to fix the wholly conditional independence assumption of\\nCTC and consists of 3 parts: an Encoder, which produces the representation of input\\nfeatures non-autoregressively; autoregressive Prediction network; and a Joint network,\\nwhich combines their output and produces the final prediction [3]. Encoder-Decoder sys-\\ntems with Attention [6] implicitly learn the alignment between the audio and text via the\\nattention mechanism. We are primarily interested in RNN-Transducers since such sys-\\ntems are suitable for streaming by design, widely used in production, and power most of\\nthe state-of-the-art monolingual models [7]. Initially, recurrent neural networks (RNNs)\\nwere used as an autoregressive prediction (thus, RNN-T is named after RNNs), but other\\nnon-recurrent architectures can also be used, e.g., a simple stateless network whose\\noutput depends on a fixed number of previous outputs [8].\\n1.4 Related Work: Models\\nIn ASR, the dominating architecture for the encoder is a Conformer [9, 10, 4], which\\noriginates from a transformer block architecture [11] augmented with convolutional mod-\\nules [9]. The important part is that the original Conformer encoder subsamples the se-\\nquence of input features (derived from the audio signal) 4 times, and recently, multiple\\narchitectures were proposed to subsample the input features 8 times to provide better\\nspeed without performance drop, e.g., Fast Conformer [12]. We will use the Fast Con-\\nformer architecture as a basic architecture for our model, focusing on loss modifications\\nrather than changing the architecture.\\n1.5 Related Work: Losses\\nCTC is the loss that can naturally be represented [13] with weighted finite state transduc-\\ners (WFSTs). Due to the existence of the libraries that allow to construct differentiable\\nWFSTs and use them to train deep learning systems, e.g., k2 [14], different modifications\\nof CTC loss were proposed to solve the problem of different errors in training data. Partic-\\nularly, W-CTC (CTC with Wild Cards) [15] allows missing text at the start and the end of an\\nutterance in transcription, Star Temporal Classification (STC) [16] allows missing labels\\nanywhere, Bypass Temporal Classification (BTC) [17] solves the problem of insertions\\nand substitutions. Recently proposed Omni-temporal Classification (OTC) [18] is a gen-\\neralized loss that combines all the previous work and proposes a CTC loss modification\\nthat is robust to any type of errors in transcripts.\\nRNN-Transducer robustness to the errors in training data is still an unsolved prob-\\nlem. The recent work about a graph-based framework for RNN-Transducer [1] proposes\\na generalized solution to develop different loss modifications based on WFSTs and also\\nproposes a W-Transducer loss that can deal with missing transcripts at the start and end\\nof the transcription (similar to W-CTC [15]). Moreover, an autoregressive prediction net-\\nwork can require modifications since its input in training time is a ground truth transcription\\nand can be sensitive to incorrect text. We are planning to increase the complexity of the\\nproject gradually, starting from the \"under-transcribed\" case, when the training texts can\\ncontain missing words (deletions) similar to STC [16]. Then, we will explore more complex\\ncases with insertions and substitutions, finally providing a general combined solution.\\n42 Literature Review\\n2.1 Introduction\\nWe start our review with the work on the CTC criterion and its modifications. Since both\\nCTC and RNN-T take into account all possible alignments between features extracted\\nfrom audio and text units, thus it is possible to apply some techniques to both of them.\\nThen, we discuss the RNN-T loss and the relevant work to improve its robustness to\\nerrors, along with the structure of the Transducer models and differences between CTC\\nand RNN-T that prevent direct application of CTC-based techniques.\\n2.2 CTC and Modifications\\n2.2.1 Connectionist Temporal Classification (CTC) loss\\nThe Connectionist Temporal Classification (CTC) was originally introduced in [5] as a\\nreplacement to a \"classical\" ASR pipeline based on hidden Markov Models (HMMs) and\\nis historically the first so-called \"end-to-end\" ASR system. The original work proposes a\\nsolution for the automatic learning for the alignment between the target units (phonemes\\nin this work, using the TIMIT [19] dataset) and the representation extracted by the neural\\nnetwork from the audio signal. The system utilizes RNNs (particularly bidirectional Long\\nShort-Term Memory (BLSTMs) networks [20]) as its backbone. Mel-Frequency Cepstrum\\nCoefficients are extracted from the input audio every 10ms, which forms the input for the\\nneural network. The output of the network has a softmax activation and is interpreted\\nfor each label as a probability of observing the label at the current time frame [5], as\\nin the classification task for each frame. The vocabulary is augmented with a special\\n⟨blank⟩symbol, and the algorithm considers all possible variants of transcriptions that\\nmap to the original text units after removing duplicated predictions and the ⟨blank⟩symbol.\\nThe loss is a minus log probability of all possible correct alignments (defined by the rule\\ndescribed above) given the audio features. Thus, training maximizes the log probability for\\nthe possible alignments without requiring a forced alignment, which was used in classical\\nHMM-based systems. The paper also proposes an efficient forward-backward algorithm\\nto calculate the CTC loss and gradients and shows the system’s efficiency in predicting\\nphonemic transcriptions of utterances.\\nThe paper about CTC loss [5] forms a basis for our research since we plan to ap-\\nply techniques to modify the possible alignments in the CTC framework to the RNN-\\nTransducer system. The work shows that it is not necessary to have one perfect align-\\nment as a target, but once we construct a mapping between the ground truth target and\\nthe inner latent alignment (in the paper - sequences with repeated labels and additional\\n⟨blank⟩symbols, that are removed in decoding), we can use a full-sum training (taking\\ninto account all possible alignments), and the system can effectively solve the ASR task.\\n2.2.2 Wild-card CTC (W-CTC)\\nWild-card CTC (W-CTC) was proposed in [15] to solve the problem when the utterance\\nis partially transcribed, and the transcription can be missing at the start, at the end, or\\non both sides. The work introduces a simple but efficient modification of CTC criterion\\ncomputation, using a special \"*\" (star) symbol that can be prepended to each transcrip-\\ntion, and means that at the start of the transcription, any possible sequence (of any size,\\n5including the empty sequence) of symbols can be missing. This allows a simple modi-\\nfication of the dynamic programming algorithm for CTC computation introduced in [5] to\\nhandle the alignments not only when the ground truth is entirely correct but also start and\\nend the alignments between text units and features extracted from audio at any part of the\\naudio, but with the assumption, that the part of the audio fully matches the transcription.\\nThe authors also use a TIMIT [19] dataset, randomly masking a portion of the start/end of\\nthe utterance transcriptions, thus showing the effectiveness of the approach (compared\\nto the original CTC) on partially transcribing ASR data in a synthetic setup. Additionally,\\nthe authors validate the effectiveness of W-CTC on Optical Character Recognition (OCR)\\nand Continuous Sign Language Recognition (CSLR) tasks.\\nThe paper is interesting as the first work which introduces a synthetic setup with under-\\ntranscribed data to study and develop training criteria for ASR robust to corrupted targets.\\nThe proposed W-CTC criterion uses a larger possible amount of alignments but still con-\\nverges and surpasses CTC even when a small portion of the data is corrupted. Also,\\nthe paper shows the application of the techniques to OCR and CSLR tasks, which shows\\nthe potential impact of modifications of losses used for ASR on areas outside the speech\\nrecognition field.\\n2.2.3 Star Temporal Classification (STC)\\nStar Temporal Classification [16] was proposed as a generalization for the previous work\\nwhen the transcription is only partial, and between any pair of labels, an arbitrary number\\nof words can be missing. The work uses the \"*\" star token to represent zero or more\\ntext tokens and considers alignments between the encoded signal features and a target,\\nwhere the \"*\" token is inserted between all labels and also appended to the start and\\nthe end of the utterance. Thus, the loss allows the network to output any sequence of\\nlabels corresponding to the presented corrupted ground truth text after removing some\\nof the words (the ASR system should emit all the words from the target but can insert\\nother words between them). This approach significantly increases the number of possible\\nalignments and does not allow the training of the neural network directly. The problem is\\nsolved by introducing a penalty for the \"*\" token λ, with exponential decay during training,\\nstarting from the high values and decreasing once the network converges. The penalty\\nhyperparameter is also adjusted based on the number of missing words in transcriptions.\\nThe authors demonstrate the approach using synthetic data derived from LibriSpeech [2],\\nusing partially masked transcripts with different probabilities (up to 70%). The approach is\\npractical even when using greedy decoding, but the authors also show that decoding with\\nan n-gram language model (LM) and rescoring hypotheses with Transformer [11] based\\nLM also improves the system’s quality. For the implementation, unlike previous work, the\\nGTN [21] framework for differentiable WFSTs is used, which simplifies the development\\nof the new loss.\\nThis paper is crucial for our work since it allows us to solve the problem with deletions\\nin the transcribed texts for CTC. The critical part is that the direct solution (considering all\\npossible alignments in the loss) leads to failure, and using the adjustable penalty is crucial\\nto solve the issue. Also, the construction of LibriSpeech-based data is an important part.\\nWe will consider an analogous approach for investigating similar cases for RNN-T.\\n62.2.4 Bypass Temporal Classification (BTC)\\nBypass Temporal Classification [17] solves part of a weakly-supervised setup with unre-\\nliable transcripts: substitutions and insertions. Similar to the previous work, the authors\\npropose CTC target graph modification to allow the alignments when some text units\\nfrom the target are changed (substitutions) or not used (insertions in the text, thus allow-\\ning deletion from the alignment). Remarkably, the ground truth is represented as a linear\\nWFST, where the forward arcs contain ground truth labels, and each arc also presents\\na parallel \"bypass\" arc with a penalty, which allows to skip the token by emitting zero or\\nmore tokens. The authors also use the penalty with an exponential decay to make the\\nnetwork learn using the increased number of possible alignments. The authors evaluate\\ntheir solution on both TIMIT [19] and LibriSpeech [2] datasets, constructing the training\\ndata with substitutions and insertions separately and combining them. Interestingly, it is\\nshown that for the CTC criterion, the impact of insertions is larger than for substitutions,\\nand with 50% of insertions, it is impossible to train a system with the pure CTC criterion.\\nThis paper solves the remaining piece of the puzzle of training a CTC-based system\\nwith partially incorrect transcripts. We will also investigate similar approaches for RNN-T.\\nMoreover, this paper encouraged us to start the exploration of the problem by comparing\\nthe impact of different errors on the training criterion behavior and starting our solution\\nfrom the most disruptive problem.\\n2.2.5 Omni-temporal Classification (OTC)\\nThe Omni-temporal Classification [18] finally combines the approaches to construct a\\nuniversal loss to handle all the possible cases of errors. The authors utilize the \"bypass\"\\narcs to skip frames, along with self-loops that represent substitutions and insertions, and\\ntwo separate penalties for them following the previous work. The authors used synthetic\\ndata generated from the train-clean-100 subset of LibriSpeech [2] and also trained a sys-\\ntem on the original subsets from LibriVox [22] that were originally used to construct the\\nLibriSpeech data, using segmentation and filtering with pretrained models, as described\\nin [2]. The combination of the losses leads to significant improvements for all types of\\nerrors and allows training of the ASR system even on the original unsegmented data with\\nerrors without a cleaning pipeline.\\nThis work contains a detailed exploration of the impacts of different penalty values\\non the quality of the network, which can provide some insights for working with similar\\nalignment-based modification approaches for RNN-T. The code is published, and we can\\ntry to reproduce the setup if necessary. Moreover, we can try this criterion as a part of the\\npotential solution with hybrid CTC-RNN-T models for a \"CTC head,\" which we will discuss\\nfurther. Moreover, we found it curious that despite the beneficial impact of the loss when\\ntraining ASR system with unfiltered raw data from LibriVox, the gap still exists for the\\ncarefully filtered LibriSpeech data (e.g., authors report [18] 12.5% vs 8.2% WER on test-\\nother for these setups), which can indicate that the problem is still not fully solved even for\\nCTC criterion, and multi-stage pipeline is still essential to obtain the best performance.\\n2.3 RNN-Transducer and Modifications\\n2.3.1 RNN-T loss\\nRecurrent Neural Network Transducer (RNN-Transducer, RNN-T) [3] was developed as an\\nimprovement of the CTC [5] systems to overcome problems related to conditional inde-\\n7pendent assumption. The system contains three components: (1) transcription network,\\nwhich is usually referred to as an \"Encoder\"(e.g., [23]), that operates on the features\\nderived from audio and is similar to the one used in CTC system; (2) prediction network,\\nthat outputs the predictions based on the previously decoded symbol in inference time,\\nand utilizes a ground truth text in training time; and (3) a combination of the outputs of the\\nprevious two networks that makes final predictions, which is usually referred as a Joint\\nnetwork in the literature [23]. The prediction network is autoregressive and provides a\\ncrucial improvement for the ASR system. The system also uses a ⟨blank⟩symbol, but the\\nmeaning differs from CTC: this symbol means the system should end decoding the current\\nframe and transition to the next frame from the encoder. Unlike CTC, repeated symbols\\nare not allowed, but the advantage is that the system can predict more than one text unit\\nfor each frame. The RNN-T loss function takes into account all possible monotonic align-\\nments between encoder and prediction network outputs, which makes the system similar\\nto CTC with implicit alignment learning.\\nThe crucial difference for the RNN-T system is that the prediction network is autore-\\ngressive (usually uses LSTM as a backbone), making it significantly more challenging to\\ndeal with imperfect transcripts. Since ground truth text is used during training (so-called\\n\"teacher forcing\" algorithm), corrupted tokens can prevent the correct alignment learn-\\ning. The autoregressive nature also prevents applying CTC-based approaches directly to\\nRNN-T and may require modifications of the network itself along with the training criterion.\\n2.3.2 Graph-based RNN-Transducer framework\\nThe recent work about RNN-Transducer modifications [1]1brings the connection between\\nWFSTs and Transducer architecture, allowing representing the RNN-T loss computation\\nwith the graph, where the arc weights are taken from the Joint network output, and thus\\nthe direct application of graph algorithms, including full-sum alignment learning, are pos-\\nsible. The work presents two approaches for representing original and modified RNN-T\\ngraphs, either as a direct grid (\"Grid-Transducer\") or as a composition of acoustic and\\ntextual schemas (\"Compose-Transducer\"), which after the composition and connect oper-\\nations exactly matches the Grid-Transducer representation (\"connect\" operation removes\\nthe states that do not belong to any path from the start and the end state; it is optional\\nfor loss computation since such states do not affect the alignment probabilities since not\\nbelonging to any alignment). The composition is slower to compute but allows for the\\ndevelopment of losses using graphs that can be easily debugged visually. The authors\\nbuild the framework on top of k2 [14] library for differentiable WFSTs and show that the\\nloss computation can be as efficient as the optimized CUDA-based code. Moreover, the\\nauthors present a W-Transducer, which can solve the problem of deletions at the start\\nand the end of the utterance, similar to the W-CTC discussed above. The graph for the\\nloss uses two groups of skip-connections from the start of the time grid to all other time\\nsteps and from each time step in the grid to the last, which allows to use the alignments\\nwhere some frames at the start and the end are skipped, but the network should emit the\\nfull training text. The effectiveness of this approach was demonstrated on LibriSpeech\\ndata by randomly removing 20% and 50% of the labels from text from both sides of each\\nutterance.\\nIn our work, we will use the proposed WFST framework for training RNN-T on noisy\\ntargets to simplify the development of the losses. W-Transducer solves the easiest case\\nwhen the autoregressive prediction network does not use a corrupted input and thus\\n1Disclaimer: the author of the Final Project participated in the development of this approach and is a co-author of the paper.\\n8can remain unmodified. We are planning to work with more complex cases, and such\\nmodifications can be required along with the loss customization. We will discuss this\\napproach in more detail in Section 3.6.\\n2.3.3 Stateless RNN-T\\nThe work [8] proposes \"RNN-T with StateLess Prediction Network (RNNT -SLP),\" revising\\nthe need for recurrent networks for the prediction network part of the RNN-T system.\\nThe authors questioned the popular opinion that the prediction network behaves similarly\\nto the language model in traditional ASR systems. They tried pretraining the recurrent\\nnetwork on text-only data to predict the next symbol and initializing the prediction network\\nwith pretrained weights and found that such pretraining does not lead to any improvement.\\nMoreover, replacing the prediction network with a simple \"stateless\" module, in which\\nprediction is based only on the last symbol (which is similar to 2-gram LM), leads to a\\ncomparable overall system performance with the RNN-based system.\\nDespite there are other works that show that the prediction network can behave like a\\nlanguage model, especially in a factorized architecture [24, 25], we are interested in this\\nwork showing the possibility of using simpler networks for this part of the model. Since\\na stateless prediction network can be significantly more robust for corrupted targets, in\\nwhich prediction is dependent only on a small context, we consider this work an important\\noption for changing the prediction network architecture.\\n2.3.4 Hybrid CTC-Transducer models\\nAs a last item of our review, we will discuss the hybrid architecture, which uses both RNN-\\nT and CTC losses, proposed in [26]. The work proposes training the neural transducer\\nwith an auxiliary CTC loss on top of the encoder (as a separate \"head\"), combining the\\nsystems for further accelerating the decoding: if the CTC head predicts the blank label,\\nsuch frame can be skipped in decoding, resulting in a significant inference acceleration\\nwith tiny quality degradation. The work [27] proposes the use of such hybrid systems\\nto accelerate not only inference but also the training speed, using more lightweight CTC\\nhead prediction to compute RNN-T loss with the smaller number of possible alignments\\n(pruning RNN-T loss lattice).\\nThis work can be relevant for our research due to the discussed above OTC criterion,\\nwhich is robust to noisy targets: in a hybrid training setup, we can use the prediction of the\\nadditional head, trained with the OTC loss, to identify corrupted tokens and thus modify\\nthe input for prediction network or the loss based on this information.\\n3 Project Design\\nIn our project, we plan to investigate the behavior of the RNN-Transducer architecture in\\na weakly supervised setup when part of the data is corrupted and modify the RNN-T loss\\nto improve the system’s quality. We will consider loss modification techniques that do not\\nrequire any change in the overall neural architecture and decoding algorithms to make\\nthem compatible with current production systems.\\n93.1 Objectives\\nWe define the following objectives for our work:\\n• (1) Investigate how using partially incorrect transcripts impacts the quality of the\\nRNN-Transducer system, separating deletions, substitutions, and insertion cases\\n• (2) Investigate techniques that allow to improve the performance of the system in\\nthe conditions described above\\n• (3) Combine the solutions provided in (2) to solve the general case of training the\\nASR Transducer-based system with unreliable transcript and evaluate the final so-\\nlution when it is unknown what type of errors the data contains.\\n3.2 Metrics\\nThe key metric for assessing the quality of an ASR system is word error rate (WER) [28].\\nThe additional metric commonly used to assess the speed of the ASR system is a real-\\ntime factor (RTF), which indicates how much audio the system can process given the\\nfixed time, and usually, the tradeoff between speed and quality is important when consid-\\nering different models. In our project, we focus only on modifications that have a minor\\nimpact on the inference speed after the model is trained; thus, we report only WER in our\\nexperiment.\\nThe word error rate is a specification of a Levenshtein distance [28, 29], defined for\\ntwo sequences of words (hypothesis and ground truth), and is calculated as a number of\\nsubstitutions (SUB), insertions (INS) and deletions (DEL) divided by the number of ground\\ntruth (correct) words.\\nWER =SUB +INS +DEL\\nCORRECT\\nLower WER means that the system makes more accurate predictions. We will also use\\nthe term \"accuracy,\" usually defined as accuracy = 1−WER (higher accuracy value\\nmeans better prediction).\\nSince we are working with the conditions that show the degradation of the standard\\nASR training pipeline, we introduce two additional metrics. WER difference (WERD) in-\\ndicates the system degradation and is a difference between the WER that the system\\nachieves in a particular setup and the WER of the baseline on the non-modified (original)\\ndata.\\nWERD =WER modified _data−WER original _data\\nWe are interested in minimizing the degradation of the system, thus defining the relative\\nimprovement of the system as WERDR.\\nWERDR =WERD RNN−T−WERD Proposed\\nWERD RNN−T\\n3.3 Data\\nThe primary data for the project is a LibriSpeech [2] corpus, which consists of 3 subsets\\nfor training data (960 hours total), two development sets ( dev-clean anddev-other , 5.4 and\\n5.3 hours respectively), and two test sets ( test-clean andtest-other , 5.4 and 5.1 hours).\\nWe will use the full training part to generate artificial training data by corrupting the texts\\n10with artificial deletions, substitutions, and insertions. We will use the dev-other set for\\nvalidation during training and choosing the optimal checkpoints, and we will finally assess\\nour models on the test-other . As it is common in the ASR research, we will report WER\\nfor all development and test sets.\\n3.4 Backbone Encoder Models\\nCurrently, the dominating architecture [7] for the encoder is the Conformer [9] model. The\\noriginal Conformer-encoder processes the input sequence of vectors and produces the\\nrepresentation 4 times smaller by the time dimension (4x subsampling). For our initial\\nexperiments, we will use Fast Conformer [12] (114M parameters), which subsamples\\nthe input by the factor of 8 without accuracy degradation by using depth-wise separable\\nconvolutions [30], which allows faster training and inference.\\n3.5 Tools and Frameworks\\nWe are planning to use NeMo [31] framework for experiments, which is based on PyTorch-\\nLightning [32] for easiness of extending the models, and train them on clusters. NeMo\\nprovides stubs for ASR models and capabilities to use WandB [33] platform for experiment\\nmanagement. When required, we will use pure PyTorch [34] and k2 [14] library to modify\\nlosses and prediction network, also using a WFST framework for RNN-Transducer [1]\\nimplemented in NeMo.\\n3.6 RNN-Transducer Details and Graph-based representation\\nRNN-Trasducer [3] model schema is shown in Fig. 1. The model consists of three parts.\\nEncoder neural network transforms a sequence of input feature vectors derived from an\\naudio into a latent representation - a sequence of vectors ⟨e0, e1, ..., e t−1⟩with the length\\nofT. The number of output vectors is usually smaller than the input due to applying\\nstrided convolutions or pooling layers along the time axis, which reduces the output size\\nby a fixed factor (subsampling) and makes the model more computationally efficient. The\\nPrediction network (usually a recurrent network) in training time takes a ground truth\\nsequence of text units as an input padded with a special start-of-sequence symbol ⟨SOS⟩\\n(in practice usually ⟨blank⟩symbol is reused for this purpose) and produces a latent repre-\\nsentation – sequence of vectors ⟨p0, p1, ..., p u⟩with the length of U+ 1(Uis the number of\\ntext units in the text representation). The Joint network combines all combinations of ei\\nwithpjvectors and produces a 3-dimensional tensor for the size Tx(U+ 1)xV, where Vis\\na vocabulary size augmented with the ⟨b⟩(⟨blank⟩) symbol (in practice, the 4-dimensional\\ntensor is used due to additional batch dimension). Thus, ji,jrepresents the vector pro-\\nduced from a combination of piandpj. In practice, the Joint network is relatively simple\\nand computes the sum of vectors, non-linearity (e.g., ReLU), and a projection into the\\nspace of size V, (e.g., ji,j=Project (ReLU (ei+pi))). If the vectors eiendpjare of differ-\\nent sizes, they are firstly projected to the space with the same dimension. The Softmax\\nactivation produces a distribution over the vocabulary units, and the dynamic program-\\nming algorithm is used to compute the probability over all possible paths in the graph (in\\nthe code, all computations are produced in log scale for numerical stability purposes).\\nEach path represents a possible alignment, where blank symbols can be inserted be-\\ntween labels (e.g., for the Fig.1 1 \"C ⟨b⟩A T⟨b⟩ ⟨b⟩ ⟨b⟩\" is the possible path for the target\\n11text \"CAT\" represented as \"C,\" \"A,\" and \"T\" units). The number of ⟨b⟩labels is the num-\\nber of frames the Encoder produces. The minus log probability of all possible alignments\\nforms an RNN-T loss value.\\nForgreedy decoding , since the ground truth text is unknown, a nested loop is used:\\nfor each encoder vector, the algorithm sequentially obtains the next prediction with the\\nmaximum probability, starting from ⟨SOS⟩symbol, and computes the Prediction network\\noutput for the new decoded symbol, combining with the current encoder vector and com-\\nputing Joint network output. Once the ⟨b⟩symbol is found, this symbol is not fed into the\\nPrediction network, but the inner loop stops, and the decoding process starts for the next\\nencoder vector. More complex decoding algorithms exist, but greedy decoding is widely\\nused in practice due to the best speed and near-optimal quality in most scenarios [7], so\\nwe are focusing on this approach.\\nThe computation of the RNN-T loss can be represented with WFSTs , as shown in [1].\\nThe original work presents the approach more theoretically, and here we will discuss the\\npractical implementation, which is a basis for our work. A weighted finite state transducer\\nis a graph with states and arcs, whose arcs represent transitions from input to output\\nlabels with a corresponding weight. In k2 [14] library, it is allowed to store an arbitrary\\nnumber of labels (not only an input/output label for each arc). Thus, we use a tuple\\nof labels as input labels for some graphs in our representation 2. We can construct a\\ncomputational graph using 3 labels for each transition: output (ground truth) label, unit\\nindex (in a sequence of ground truth units), and encoder vector index, see 2c. Given these\\nlabels by using \"index select\"2operation, we can populate a lattice with the weights from\\nthe Joint network output, and apply a differentiable computation of the full-sum loss3. This\\nis a \"Grid-Transducer,\" described in [1]. To simplify the development, we can construct\\ntwo schemas (\"Compose-Transducer\"). Unit schema 2a represents text units, and each\\ntransition has a tuple of input symbols (Unit :Unit_Index )and an output symbol Unit.⟨b⟩\\nunits represent self-loops, and other units represent transitions over the ground truth text.\\nThe time schema 2b is a simple linear graph representing transition over time and self-\\nloops for each state with all the vocabulary symbols as inputs and time index as output.\\nIt is much easier to visually debug the representation with the separated schemas, and\\ntheir composition matches the final lattice 2c. We are planning to experiment with the\\nseparated schemas to make the RNN-T loss more robust to corrupted targets, but for the\\nfinal stage, we are planning to provide an efficient code for lattice construction.\\n3.7 Plan\\n3.7.1 Initial Project Plan\\nOur work initial work plan published in the \"Draft Report\" (Midterm) is shown in Fig. 3.\\n(a) Minimal RNN-T Implementation . After initial preparation, which was already done\\nbefore finishing this report (initial exploration, project proposal, literature review), we will\\nimplement a minimal codebase to experiment with RNN-T. During our exploration, we\\nfound that RNN-T models in NeMo provide a lot of functionality but are not easy to extend.\\nSo, we will implement the lightweight pipeline with the following capabilities:\\n• Lightweight Joint and Prediction networks\\n• Greedy Decoding\\n2https://pytorch.org/docs/stable/generated/torch.index_select.html\\n3https://k2-fsa.github.io/k2/python_api/api.html?highlight=get_tot_scores#k2.Fsa.get_tot_scores\\n12Figure 1: RNN-Transducer Schema\\n• Full model training pipeline\\n• Compatibility with NeMo encoders (including Conformer-based encoders as de-\\nscribed above), since we are not planning to customize the encoder part.\\nThen, we plan to train the baseline on the original LibriSpeech data with a Fast Con-\\nformer encoder to check that our model can achieve comparable quality with the native\\nimplementation of RNN-T in NeMo.\\n(b) Impact of imperfect transcripts . We will generate the data derived from Lib-\\nriSpeech with 20% and 50% of deletions, substitutions, and insertions separately (6 sets\\nin total) and train the models to study the model’s behavior in such conditions.\\n(c) \"Deletions\" case. Our preliminary studies showed that the deletions in the tran-\\nscripts are the hardest case for RNN-T. We will try to solve this case by modifying the loss\\nfunction. At this stage, we focus on improving the performance and not trying to find the\\nperfect hyperparameters and/or provide the fastest implementation for the loss function.\\n(d) \"Insertions\" and \"substitutions.\" We will explore different approaches discussed\\nin the literature review to improve the system’s performance in such conditions.\\n(e) Combined Solution . Combining the solutions (c) and (d) will allow for solving the\\nuniversal problem of partially correct transcripts. We will generate additional training data\\nfor cases with all types of errors.\\n13(a) RNN-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) RNN-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) RNN-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 2: WFSTs for RNN-Transducer, following [1]\\n(f) Conformer Medium (4x subsampling). At this stage, we will apply our solution to\\nthe Conformer Medium model to study its behavior when the encoder reduces the input 4\\ntimes.\\n(g) Fast loss implementation. This task focuses on the quality and speed of our\\ncode. We will provide a clean and fast \"ready to use\" solution for our loss and prediction\\nnetwork modifications.\\n(h) Study hyperparameters in detail. This task will allow us to get insights about the\\nhyperparameters of the systems proposed in (c)-(e).\\n3.7.2 Project Plan Reflection\\nThe sections (a)-(b) were crucial for our project but imposed minimal risks due to well-\\nexplored existing solutions. The most critical risks came from (c) and (d) cases since,\\naccording to our knowledge, no solutions existed for such tasks. To mitigate these risks,\\nwe also considered using a hybrid CTC-Transducer architecture with OTC loss, which can\\nsolve the problem, at least for the CTC head, and we can use its predictions to train the\\nTransducer part instead of corrupted ground truth labels. Combined solution (e) was a\\nkey part of finalizing our project. We considered parts (f)-(h) as a fair improvement but not\\nan essential contribution to our work and planned to focus on them only after finishing the\\nmain part.\\nIn our final stage, we elaborated a successful solution for all discussed cases, provid-\\n14Figure 3: Project Plan (Gantt Chart)\\ning a \"drop-in\" replacement for the RNN-Transducer loss for the cases without changing\\nthe model architecture at all. We also provided a fast implementation for the losses. Due\\nto a lack of computational resources and difficulties training the system within the novel\\nsetup, we focused more on exploring parameters for Bypass Transducer loss, as dis-\\ncussed in Section 5.4. We omitted experiments with Conformer Medium (f) and left them\\nfor future work.\\n4 Implementation\\nThe implementation is published in the GitHub repository :https://github.com/artbataev/\\nuol_final . In this section, we describe the implementation with the links to the origi-\\nnal files. Additional visualization of the produced lattices for all proposed losses can be\\nfound in the Jupyter Notebook https://github.com/artbataev/uol_final/blob/main/\\nnotebooks/Loss_Demo.ipynb .\\n4.1 Project Implementation Overview\\nThe main goal of the project is to provide a solution to deal with different types of er-\\nrors in the RNN-Transducer framework. We want to make our models comparable and\\ncompatible with the publicly available state-of-the-art models and want further to propose\\nsolutions to the NeMo [31] framework. So, we reuse components from NeMo, focusing\\non customization and modifications of the necessary parts.\\nFirstly, we make a minimal necessary code of RNN-T for our experiments, providing\\nimplementation containing the RNN-T model and customizable Joint and Prediction net-\\nworks. We reuse Conformer [9] blocks from NeMo [31] for the Encoder network.\\nThe Prediction network is a 1-layer LSTM with 640 hidden units, implemented\\ninMinPredictionNetwork class. The Joint network, as discussed in Section 3.6, applies\\n15two projections of the output of Encoder and Prediction networks (2 linear layers) to the\\ndimension of 640, sums the vectors, applies ReLU non-linearity, and projects the out-\\nput (one more linear layer) into 1025-dimensional space (1024 BPE units and ⟨b⟩). It is\\nimplemented in MinJoint class.\\nWe also implement a greedy decoding algorithm for evaluation in min_rnnt/decoding.py .\\nIn our experimental setup, we are following the Fast Conformer [12] training pipeline4.\\nThe encoder has 108.7M parameters, prediction network 3.9M, and Joint 1.4M (totally\\n114M).\\n4.2 Data Preprocessing\\nWe preprocess LibriSpeech [2] data and apply speed perturbation with rates 0.9and 1.1\\n(3x audio data), using the published preprocessing script5to make our pipeline compara-\\nble with published models.\\nWe use log-mel filterbanks extracted from audio every 10ms with the window 25ms\\nand apply SpecAugment [35] in training. We use the vocabulary of 1024 BPE [36] tokens\\nextracted using SentencePiece [37] library for text units.\\n4.3 Model and Training Pipeline\\nWe set up training of our model for 200 epochs using AdamW [38] optimizer with Co-\\nsine annealing [39] learning rate schedule with a linear warmup for 40 epochs and the\\nmaximum learning rate of 5e−3. For experiments except for the baseline, we stopped\\ntraining the model after 60 epochs since we are interested in the relative difference in\\nmodel quality, and achieving the best possible accuracy is not our priority at this stage.\\nWe are reporting the results for the best checkpoint chosen on the dev-other validation\\nset. For all experiments, we maintain a global batch size of 2048. We are training models\\non clusters using NVIDIA A100 (mixed-precision with bfloat16) and V100 GPUs (float32\\nfull-precision), and depending on the availability of the resources varying local batch size\\nfrom 8 to 32 to fit into memory and adjusting gradient accumulation to make the global\\nbatch size constant. We did not observe any difference in quality for a fixed global batch\\nsize when using an arbitrary number of nodes, varying local batch size, and using mixed\\nor full precision. So, we are not reporting these details for each experiment.\\n4.4 Proposed Losses\\nIn our work, we propose three modifications of the RNN-T loss:\\n• Star-Transducer to dial with arbitrary deletions\\n• Bypass-Transducer to solve the case of insertions\\n• Target-Robust-Transducer, which is the combination of the previous modifications,\\nallows to mitigate the problems of substitutions in target texts and also can be used\\nas a universal loss when the type of errors is unknown.\\n4github.com/NVIDIA/NeMo/blob/v1.21.0/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml\\n5https://github.com/NVIDIA/NeMo/blob/v1.21.0/scripts/dataset_processing/get_librispeech_data.py\\n16The modifications are implemented in minrnnt/losses subpackage. All the classes\\nfollow Graph-RNNT [1] framework and inherit GraphRnntLoss class from the NeMo [31]\\nframework and reuse its methods. The implementation uses k2 [14] library. As described\\nin Section 3.6, the computational lattice can be constructed as a composition of temporal\\nand unit schemas, implemented in get_temporal_schema and get_unit_schema respec-\\ntively. The faster implementation constructs the lattice directly in get_grid method. In\\nthe initial development, we used the composition and then made the get_grid implemen-\\ntation as a faster option. We also customize the forward method to assign appropriate\\nscores to the arcs corresponding to special tokens, as described below.\\nFor all losses, we add unit tests (in tests directory) to make the sanity check for the\\nfollowing:\\n• Graphs produced by composition of the temporal and unit schemas are equivalent\\nto the graph produced by get_grid method.\\n• When the weight of special arcs is −∞, this is the equivalent of removing such\\narcs from a computational graph ( e−∞= 0, such a transition does not contribute to\\nloss computation); and the loss should be equivalent to original RNN-T loss. This\\nis tested by comparing the loss value and gradient based on random input for the\\nproposed loss and etalon RNN-T implementation.\\nThe graph construction is debugged visually in the Jupyter Notebook [40] using auto-\\nmatic visualization from the k2 [14] library with GraphViz package [41].\\n4.4.1 Star-Transducer (Star-T)\\nWe propose a simple but effective modification of the RNN-T loss computational graph\\nto solve the problem of deletions. Star Transducer takes into account, along with the\\nalignments with blank labels, the sequences when the blank label is substituted with a\\nspecial \"skip frame\" ⟨sf⟩symbol, which can be viewed as an allowance to skip frames\\nproduced by the encoder in training time. This approach is similar to the \"*\" token used\\nin Star Temporal Classification [16] loss for CTC. For such frames, the transcription is\\nmissing in the ground truth, and the core idea was to allow skipping such frames when\\nconsidering all possible alignments for loss computation. We add parallel arcs to those\\nwith⟨b⟩label to achieve this, as shown in 4c. Unlike other arcs, the weight for this arc is a\\nhyperparameter and assigned directly after populating the lattice with other weights.\\nThe Star-Transducer loss is implemented in the GraphStarTransducerLoss class.\\n4.4.2 Bypass-Transducer (Bypass-T)\\nFor dealing with insertions, we propose a modification of the RNN-T computational graph,\\nadding arcs with a special \"skip token\" ⟨st⟩symbol, inspired by Bypass Temporal Classifi-\\ncation [17] approach. These arcs are parallel to the arcs with tokens. This means that the\\nloss can consider alignments where some tokens are skipped. Fig. 5 shows the temporal\\nand unit schemas and the full constructed lattice.\\nThe Bypass-Transducer loss is implemented in the GraphBypassTransducerLoss class.\\nIn our experiments, we found that assigning constant weight similar to the Star-Transducer\\napproach does not work. With small absolute values (e.g, 0or−3) the model is prone\\nto produce deletions, and the system behaves worse than the original RNN-T. With high\\n17(a) Star-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) Star-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) Star-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 4: WFSTs for Star-Transducer. ⟨sf⟩is a special symbol indicating skipping the\\nframe.\\nabsolute weight values (e.g., −20), such transitions do not contribute to the loss compu-\\ntation: e−20is close to zero, and the loss is close to the original RNN-T. Similar to the ap-\\nproaches applied in the paper about BTC [17], we apply a schedule to the penalty weight\\n(skip_token _penalty ), combined with the probability derived from the output of the Joint\\nnetwork. For the probability we considered different options ( skip_token _mode parameter\\nin the implementation):\\n• \"constant\": only penalty constant, similar to one used in the Star-Transducer loss.\\n• \"mean\": mean probability for all labels (in log scale) excluding blank, similar to\\nBTC [17].\\n• \"max\": maximum log-probability for all labels excluding blank.\\n• \"maxexcl\": maximum of the log probabilities of all labels excluding blank and ground\\ntruth labels.\\n• \"sumexcl\": logarithm of the sum of the probabilities (in log scale) of all labels exclud-\\ning blank and ground truth labels.\\nWe found that the \"mean\" and \"max\" options were not better than the original RNN-T\\nloss. The \"maxexcl\" option was the first working solution used in the Preliminary Report.\\nThe intuition behind the \"sumexcl\" option is to assign the \"unused\" probability of outputs.\\n18(a) Bypass-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) Bypass-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) Bypass-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 5: WFSTs for Bypass-Transducer. ⟨st⟩is a special symbol indicating skipping the\\ntoken.\\nThe \"sumexcl\" option allows for the alignments to be considered when the network outputs\\na high probability for any token other than the target as \"appropriate.\" We found that the\\n\"sumexcl\" option outperforms other cases, as discussed further in Section 5.4.\\n4.4.3 Target-Robust-Transducer (TRT)\\nTarget-Robust-Transducer loss is a combination of Star-Transducer and Bypass-Transducer.\\nWe add both types of arcs that allow skipping frames and tokens, as shown in Fig. 6. It\\nis worth mentioning that assigning −∞weight for \"skip frame\" arcs makes the loss iden-\\ntical to Bypass-Transducer (skipping frames is not allowed in this case), and −∞ weight\\nfor \"skip token\" arcs makes it similar to Star-Transducer (skipping tokens is not allowed).\\nThis makes this loss a universal replacement for the previous two modifications (but the\\nsystem makes more computations since the arcs are still present, even with −∞weight).\\nWe also test this behavior in unit tests.\\nThe Target-Robust-Transducer loss is implemented in the class\\nGraphTargetRobustTransducerLoss . The implementation combines hyperparameters and\\ncode for GraphStarTransducerLoss and GraphBypassTransducerLoss .\\n19(a) Target-Robust-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) Target-Robust-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) Target-Robust-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 6: WFSTs for Target-Robust-Transducer. ⟨sf⟩is a special symbol indicating skip-\\nping the frame. ⟨st⟩is a special symbol indicating skipping the token.\\nTable 1: Baseline on LibriSpeech, WER [%].\\nSourcedev test\\nclean other clean other\\nNeMo 2.0 5.0 2.2 5.0\\nOurs (200 epochs) 2.1 4.9 2.2 5.1\\nOurs (60 epochs) 2.6 6.8 2.8 6.8\\nOurs (100 epochs) 2.4 5.9 2.5 6.0\\n5 Evaluation\\n5.1 Baseline\\nThe results for our implementation are shown in Table 1. For comparison, we use a\\npublicly available Fast Conformer checkpoint6trained on LibriSpeech data for 200 epochs.\\nOur implementation provides results comparable to those of the state-of-the-art pipeline.\\n6https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_fastconformer_transducer_large_ls\\n20Table 2: Training RNN-T on data with errors, Fast Conformer, 60 epochs, WER [%].\\nType Corrupt %dev test WERD ↓\\nclean other clean other\\n– 2.6 6.8 2.8 6.8\\nDEL 20% 4.3 9.9 4.7 10.3 3.5\\nDEL 50% 79.2 81.7 80.3 81.4 74.6\\nSUB 20% 4.0 9.4 3.9 9.7 2.9\\nSUB 50% 11.5 23.2 11.2 23.8 17.0\\nINS 20% 4.0 10.3 4.2 10.2 3.4\\nINS 50% 5.1 12.7 5.3 13.5 6.7\\nSo we can proceed further and investigate the system behavior of corrupted targets.\\nAdditionally, we show the results for 60 and 100 epochs (also using the best checkpoint\\nselected on dev-other for these epochs): to save computational resources, we evaluate\\ndifferent cases, training the models for 60 epochs, and for the final case with arbitrary\\nerrors we train the system for 100 epochs.\\n5.2 Error Impact Exploration\\nTo explore the training pipeline on partially incorrect transcripts, we generate additional\\ntraining data sets by mutating the original training texts with the mutation probability pm\\nof 20% and 50%. We are randomly removing words for the \"deletions\" case. We use\\nrandomly selected words from the training vocabulary for substitutions and insertions,\\nsubstituting/inserting words with the probability pm.\\nTable 2 shows the training results on corrupted transcripts. With a small amount of\\ncorruption, all cases lead to system degradation, but the difference between cases is\\ntiny (from 2.9% to 3.5% absolute WER degradation on test-other). We found that the\\ndeletions are most disruptive for the high corruption rate of 50%, and the ASR system\\ncan not achieve a reasonable quality (81.4% WER on test-other compared to 6.8% on\\noriginal data). Thus, we prioritized the work with this part of the problem. Substitutions\\nare the next hard case for RNN-T, which is the opposite of observations for the behavior\\nof CTC systems in [18].\\n5.3 Dealing with Deletions: Star Transducer\\nFor the setup when the ground truth transcripts contain deletions, we apply Star-Transducer\\nloss as a drop-in replacement for the RNN-T loss. The results of training the model are\\nshown in Table 3. In both scenarios, we can close the gap between the baseline for more\\nthan 70%: 77.1% WERDR for 20% deletions and 94.4% for 50%. We found that the train-\\ning is stable even without penalty, but applying the small constant penalty for the \"skip\\nframe\" transition ( −0.5) improves the quality when the number of deletions is low. We\\nwere surprised that such a simple solution works and that modifying the autoregressive\\nprediction network is unnecessary. This can mean that the encoder and joint are more\\nsensitive to incorrect transcripts than the prediction network.\\n21Table 3: Star-Transducer (Star-T) Loss for deletions, Fast Conformer, 60 epochs, WER\\n[%].\\nLossSkipDEL %dev test WERD ↓WERDR ↑\\nWeight clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T - 20% 4.3 9.9 4.7 10.3 3.5\\nStar-T 0 20% 3.9 8.2 4.3 8.5 1.7 51.4%\\nStar-T -0.5 20% 3.1 7.5 3.4 7.6 0.8 77.1%\\nRNN-T - 50% 79.2 81.7 80.3 81.4 74.6\\nStar-T 0 50% 5.1 10.6 5.2 11.0 4.2 94.4%\\nStar-T -0.5 50% 5.4 12.4 5.9 12.5 5.7 92.4%\\nTable 4: Bypass-Transducer (Bypass-T) Loss for insertions, Fast Conformer, 60 epochs,\\nWER [%].\\nLossSkipINSdev test WERD ↓WERDR ↑\\nWeight clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T – 20% 4.0 10.3 4.2 10.2 3.4\\nBypass-T -6 20% 3.0 7.5 3.3 7.9 1.1 67.6%\\nRNN-T – 50% 5.1 12.7 5.3 13.5 6.7\\nBypass-T -6 50% 3.9 10.3 4.3 10.5 3.7 44.8%\\nBypass-T -5 50% 3.6 9.2 4.0 9.4 2.6 61.2%\\n5.4 Dealing with Insertions: Bypass Transducer\\nFor insertions case, we apply the Bypass-Transducer loss described in Section 4.4.2. The\\nresults are shown in the Table 4. The transition weight for the \"skip token\" arcs is a sum of\\nthe constant weight and the total log-probability of all outputs excluding blank and target\\n(\"sumexcl\" option), as discussed in the Section 4.4.2. The training starts with a constant\\nweight of −20.0and is adjusted with the decay after each epoch (starting 3rd epoch):\\nweight next =min (max_weight, weight ∗decay ). We use decay = 0.9for all experiments.\\nTable 4 also reports the maximum constant penalty applied in training. The proposed loss\\ncan restore more than 60% of the system quality for the texts with insertions. Further\\nevaluation of the \"sumexcl\" and \"maxexcl\" options for assigning the weight can be found\\nin Appendix B.\\n5.5 Dealing with Substitutions: Target-Robust-Transducer\\nWe apply Target-Robust-Transducer for the case with substitutions since any \"substitu-\\ntion\" can be viewed as a combination of \"deletion\" and \"insertion.\" The results are shown\\nin Table 5. In the preliminary experiments, we found that assigning low absolute values\\nfor weights ( 0for skip frame as in Star-Transducer, and −5or−6for skip token as in\\n22Table 5: Target Robust Transducer (TRT) Loss for substitutions, Fast Conformer, 60\\nepochs, WER [%].\\nLossSkipSUBdev test WERD ↓WERDR ↑\\ntoken,frame clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T - 20% 4.0 9.4 3.9 9.7 2.9\\nTRT -8,-0.5 20% 3.4 8.2 3.8 8.5 1.7 41.4%\\nRNN-T - 50% 11.5 23.2 11.2 23.8 17.0\\nTRT -8,-0.5 50% 8.2 16.3 8.5 17.0 10.2 45.3%\\nTRT -8,-1 50% 6.9 16.0 7.1 15.8 9.0 47.1%\\nTable 6: Target Robust Transducer (TRT) Loss for arbitrary errors, Fast Conformer, 60\\nand 100 epochs, WER [%]. 50% of data is corrupted, using 15% for each class of errors.\\nLoss Epochs ERRdev test WERD ↓WERDR ↑\\nclean other clean other\\nRNN-T 60 - 2.6 6.8 2.8 6.8\\nRNN-T 60 50% 4.2 10.2 4.3 10.1 3.3\\nTRT 60 50% 3.3 8.0 3.6 8.4 1.6 51.5%\\nRNN-T 100 – 2.4 5.9 2.5 6.0\\nRNN-T 100 50% 3.5 9.4 3.8 9.5 3.5\\nTRT 100 50% 2.9 7.0 3.2 7.0 1.0 71.4%\\nBypass-Transducer) results in fast model overfitting, but when the penalty is more sig-\\nnificant, the training is stable. Since the loss can skip both frames and tokens, apply-\\ning a more significant penalty is reasonable. We use −8for skip frame penalty and the\\n\"sumexcl\" option for assigning the weight, which we found the best when experimenting\\nwith Bypass-Transducer, along with the penalty schedule, as discussed above. With the\\nproposed loss, we can restore more than 40% of the system degradation on test-other :\\nwe achieve WERDR of 41.4% for 20% substitutions and 47.1% for 50%.\\n5.6 Arbitrary Errors\\nFor evaluating the system trained with Target-Robust-Transducer loss, we construct the\\nextra training data by corrupting only 50% of all utterances. For each corrupted utterance,\\nwe apply random substitutions, insertions, and deletions with probability for each type of\\n15%. We consider such case closer to the actual conditions used for production systems\\nwhen the well-curated datasets are mixed with unreliable data from different sources.\\nAlso, we train the system for longer (100 epochs). As shown in Table 6, we are able to re-\\nstore more than 51% system quality when the system is trained for 60 epochs (compared\\nto RNN-T baseline also trained for 60 epochs). When trained for an extra 40 epochs,\\nthe system can restore more than 71% quality (WERDR 71.4%). In the Appendix A, we\\n23also publish the learning curves for the system demonstrating its effectiveness in reducing\\nsubstitutions and deletions on the dev-clean data.\\n6 Conclusion\\nIn our project, we trained speech recognition neural systems on the LibriSpeech [2]\\ndataset. We explored the system’s robustness to errors in target texts by artificially cor-\\nrupting the ground truth target texts from the dataset. We also explored different RNN-T\\nloss modifications to solve the problem of quality degradation in the discussed scenarios\\nand proposed three losses:\\n•Star-Transducer , which mitigates the effect of missing words in transcripts and is\\nable to restore more than 90% of the system quality in such case\\n•Bypass-Transducer , which allows insertions (extra words) in the transcripts and\\nallows the restoration of more than 60% of the quality in such cases compared to\\n\"clean\" transcripts\\n•Target-Robust-Transducer , which combines the approaches applied in the previ-\\nous two losses. This loss can deal with arbitrary types of errors. It improves the\\nsystem’s quality when some words of transcripts are incorrect (substitutions), miti-\\ngating more than 40% of the quality loss for this case. For arbitrary types of errors,\\nwe also show that it can restore more than 70% of the quality compared to the\\nbaseline with the well-transcribed data.\\nOur work is based on the previous solutions for CTC loss [16, 17, 18] and Graph-RNN-\\nT framework [1], and proposes a novel valuable solution for RNN-Transducer-based ASR\\nsystems. We demonstrated the effectiveness of the losses using the Fast Conformer [12]\\nmodel.\\nThe proposed Target-Robust-Transducer system can be applied in real-world scenar-\\nios when training models on a large amount of data from unreliable sources that usually\\ncontain transcription errors.\\nWe also see direct applications for Star-Transducer beyond the discussed case with\\nmissing words in transcripts. Modern ASR systems are trained not only to provide tran-\\nscription (in words) but also to provide punctuation, e.g., Whisper [42]. Since many cu-\\nrated ASR corpora do not contain punctuation (e.g., LibriSpeech [2] which we use in our\\nwork), such missing punctuation can be viewed as \"deletions\" in the transcripts, and with\\nStar-Transducer loss the model can be trained directly on a mixture of datasets with and\\nwithout punctuation.\\nIn further work, we plan to investigate other models (e.g., Conformer [9] with 4x sub-\\nsampling) and datasets. We also plan to apply the losses to train ASR systems on a large\\nscale for production usage.\\nThe losses are planned to be proposed to the open-source NeMo [31] framework.\\n7 Report Parameters and Additional Notes\\nThe implementation is published in the GitHub repository: https://github.com/artbataev/\\nuol_final .\\nThe report contains 6 tables and 6 figures. The appendix contains an additional 1\\ntable and 1 figure. We comply with word limits for each section.\\n248 Acknowledgments\\nI want to express my gratitude to my employer, NVIDIA Corporation, for providing compu-\\ntational resources for this research.\\nReferences\\n[1] A. Laptev, V. Bataev, I. Gitman, and B. Ginsburg, “Powerful and extensible wfst frame-\\nwork for rnn-transducer losses,” in ICASSP 2023 - 2023 IEEE International Confer-\\nence on Acoustics, Speech and Signal Processing (ICASSP) , 2023.\\n[2] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: an ASR corpus\\nbased on public domain audio books,” in ICASSP , 2015.\\n[3] A. Graves, “Sequence transduction with Recurrent Neural Networks,” in ICML: work-\\nshop on representation learning , 2012.\\n[4] J. Li, “Recent advances in end-to-end automatic speech recognition,” APSIPA Trans-\\nactions on Signal and Information Processing , vol. 11, no. 1, 2022.\\n[5] A. Graves, S. Fernández, F . Gomez, and J. Schmidhuber, “Connectionist temporal\\nclassification: labelling unsegmented sequence data with recurrent neural networks,”\\ninProceedings of the 23rd international conference on Machine learning , 2006, pp.\\n369–376.\\n[6] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network\\nfor large vocabulary conversational speech recognition,” in 2016 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP) . IEEE, 2016, pp.\\n4960–4964.\\n[7] R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schluter, and S. Watanabe, “End-to-end\\nspeech recognition: A survey,” IEEE/ACM Transactions on Audio, Speech, and Lan-\\nguage Processing , vol. 32, pp. 325–351, 2023.\\n[8] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “Rnn-transducer with\\nstateless prediction network,” in ICASSP 2020 - 2020 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP) , 2020, pp. 7049–7053.\\n[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han, S. Wang, Z. Zhang,\\nY . Wu, and R. Pang, “Conformer: Convolution-augmented Transformer for speech\\nrecognition,” in Interspeech , 2020.\\n[10] Huggingface: Open ASR leaderboard. [Online]. Available: https://huggingface.co/\\nspaces/hf-audio/open_asr_leaderboard\\n[11] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\\nand I. Polosukhin, “Attention is all you need,” in Neural Information Processing Sys-\\ntems , 2017.\\n[12] D. Rekesh, N. R. Koluguri, S. Kriman, S. Majumdar, V. Noroozi, H. Huang,\\nO. Hrinchuk, K. Puvvada, A. Kumar, J. Balam et al. , “Fast conformer with linearly\\nscalable attention for efficient speech recognition,” in 2023 IEEE Automatic Speech\\nRecognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1–8.\\n25[13] A. Laptev, S. Majumdar, and B. Ginsburg, “Ctc variations through new wfst topolo-\\ngies,” in Interspeech , 2021.\\n[14] D. Povey, P . ˙Zelasko, and S. Khudanpur, “Speech recognition with next-generation\\nKaldi (k2, Lhotse, Icefall),” Interspeech: tutorials , 2021.\\n[15] X. Cai, J. Yuan, Y . Bian, G. Xun, J. Huang, and K. Church, “W-CTC: a connectionist\\ntemporal classification loss with wild cards,” in ICLR , 2022.\\n[16] V. Pratap, A. Hannun, G. Synnaeve, and R. Collobert, “Star Temporal Classification:\\nSequence classification with partially labeled data,” in NeurIPS , 2022.\\n[17] D. Gao, M. Wiesner, H. Xu, L. P . Garcia, D. Povey, and S. Khudanpur, “Bypass Tem-\\nporal Classification: Weakly Supervised Automatic Speech Recognition with Imper-\\nfect Transcripts,” in Proc. INTERSPEECH 2023 , 2023, pp. 924–928.\\n[18] D. Gao, H. Xu, D. Raj, L. P . G. Perera, D. Povey, and S. Khudanpur, “Learning\\nfrom flawed data: Weakly supervised automatic speech recognition,” arXiv preprint\\narXiv:2309.15796 , 2023.\\n[19] J. S. Garofolo, “Timit acoustic phonetic continuous speech corpus,” Linguistic Data\\nConsortium, 1993 , 1993.\\n[20] A. Graves and J. Schmidhuber, “Framewise phoneme classification with bidirectional\\nlstm and other neural network architectures,” Neural networks , vol. 18, no. 5-6, pp.\\n602–610, 2005.\\n[21] A. Y . Hannun, V. Pratap, J. Kahn, and W.-N. Hsu, “Differentiable weighted finite-state\\ntransducers,” ArXiv , vol. abs/2010.01003, 2020.\\n[22] Librivox: Free public domain audiobooks. [Online]. Available: https://librivox.org\\n[23] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Álvarez, D. Zhao, D. Rybach,\\nA. Kannan, Y . Wu, R. Pang, Q. Liang, D. Bhatia, Y . Shangguan, B. Li, G. Pundak,\\nK. C. Sim, T. Bagby, S. yiin Chang, K. Rao, and A. Gruenstein, “Streaming end-to-\\nend speech recognition for mobile devices,” ICASSP 2019 - 2019 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 6381–6385,\\n2018.\\n[24] E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid autoregressive transducer\\n(hat),” ICASSP , 2020.\\n[25] Z. Meng, T. Chen, R. Prabhavalkar, Y . Zhang, G. Wang, K. Audhkhasi, J. Emond,\\nT. Strohman, B. Ramabhadran, W. R. Huang, E. Variani, Y . Huang, and P . J. Moreno,\\n“Modular hybrid autoregressive transducer,” SLT, 2022.\\n[26] Z. Tian, J. Yi, Y . Bai, J. Tao, S. Zhang, and Z. Wen, “Fsr: Accelerating the infer-\\nence process of transducer-based models by applying fast-skip regularization,” arXiv\\npreprint arXiv:2104.02882 , 2021.\\n[27] Y . Wang, Z. Chen, C. yong Zheng, Y . Zhang, W. Han, and P . Haghani, “Accelerating\\nrnn-t training and inference using ctc guidance,” ICASSP 2023 - 2023 IEEE Interna-\\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 1–5,\\n2022.\\n26[28] Word Error Rate, “Word error rate — Wikipedia, the free encyclopedia,” 2023,\\n[Online; accessed 2024-01-05]. [Online]. Available: https://en.wikipedia.org/wiki/\\nWord_error_rate\\n[29] Levenshtein Distance, “Levenshtein distance — Wikipedia, the free encyclopedia,”\\n2023, [Online; accessed 2024-01-05]. [Online]. Available: https://en.wikipedia.org/\\nwiki/Levenshtein_distance\\n[30] F . Chollet, “Xception: Deep learning with depthwise separable convolutions,” 2017\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1800–\\n1807, 2016.\\n[31] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman,\\nS. Beliaev, V. Lavrukhin, J. Cook, P . Castonguay, M. Popova, J. Huang, and\\nJ. Cohen, “NeMo: a toolkit for building AI applications using neural modules,”\\narXiv:1909.09577 , 2019.\\n[32] W. Falcon and The PyTorch Lightning team, “PyTorch Lightning,” Mar. 2019.\\n[Online]. Available: https://github.com/Lightning-AI/lightning\\n[33] Weights & biases: The developer-first mlops platform. [Online]. Available:\\nhttps://wandb.ai/\\n[34] A. Paszke, S. Gross, F . Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga et al. , “Pytorch: An imperative style, high-performance deep\\nlearning library,” Advances in neural information processing systems , vol. 32, 2019.\\n[35] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le,\\n“SpecAugment: A simple data augmentation method for automatic speech recogni-\\ntion,” Interspeech , 2019.\\n[36] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words\\nwith subword units,” in Proceedings of the 54th Annual Meeting of the Association\\nfor Computational Linguistics , 2016.\\n[37] T. Kudo and J. Richardson, “SentencePiece: A simple and language independent\\nsubword tokenizer and detokenizer for neural text processing,” in Proceedings\\nof the 2018 Conference on Empirical Methods in Natural Language Processing:\\nSystem Demonstrations , E. Blanco and W. Lu, Eds. Brussels, Belgium:\\nAssociation for Computational Linguistics, Nov. 2018, pp. 66–71. [Online]. Available:\\nhttps://aclanthology.org/D18-2012\\n[38] I. Loshchilov and F . Hutter, “Decoupled weight decay regularization,” in ICLR , 2019.\\n[39] ——, “SGDR: Stochastic gradient descent with warm restarts,” in ICLR , 2017.\\n[40] T. Kluyver, B. Ragan-Kelley, F . Pérez, B. Granger, M. Bussonnier, J. Frederic, K. Kel-\\nley, J. Hamrick, J. Grout, S. Corlay, P . Ivanov, D. Avila, S. Abdalla, and C. Willing,\\n“Jupyter notebooks – a publishing format for reproducible computational workflows,”\\ninPositioning and Power in Academic Publishing: Players, Agents and Agendas ,\\nF . Loizides and B. Schmidt, Eds. IOS Press, 2016, pp. 87 – 90.\\n27[41] J. Ellson, E. Gansner, L. Koutsofios, S. C. North, and G. Woodhull, “Graphviz— open\\nsource graph drawing tools,” in Graph Drawing , P . Mutzel, M. Jünger, and S. Leipert,\\nEds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2002, pp. 483–484.\\n[42] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust\\nspeech recognition via large-scale weak supervision,” in International Conference on\\nMachine Learning . PMLR, 2023, pp. 28 492–28 518.\\n28Appendices\\nA Arbitrary Errors - Learning Curve\\nWe provide an additional plot with the learning curve, demonstrating WER and its com-\\nponents for RNN-T and Target Robust Transducer training with arbitrary errors for 100\\nepochs, as discussed in Section 5.6. We can see in Figure 7 that the number of in-\\nsertions produced in all cases is similar, but the number of deletions and substitutions\\nproduced by the system trained on corrupted data with the TRT loss is significantly lower\\nthan for RNN-T and is close to the number of errors produced by the RNN-T on the original\\nnon-corrupted data. The screenshot is produced by WandB [33].\\nFigure 7: Arbitrary errors: learning curves for RNN-T (original and corrupted data) and\\nTarget Robust Transducer (corrupted data).\\nB Bypass-Transducer: Extended evaluation of hyperpa-\\nrameters\\nIn this appendix section, we show the extended hyperparameter evaluation of the op-\\ntions for Bypass-Transducer loss regarding assigning weights for skip token transitions.\\nIn initial experiments, as discussed in Section 4.4.2, we tried different options and found\\nthat the system is trainable only with \"maxexcl\" and \"sumexcl\" options. In the system\\nexploration process, we found the \"sumexcl\" option, which considers total \"unassigned\"\\nlog-probability (log-probability for all outputs excluding blank and target labels) to provide\\nthe best value. We provide an extended version of the Table 4. The results in 7 show that\\nthe \"sumexcl\" option outperforms the \"maxexcl\" by a significant margin.\\n29Table 7: Bypass-Transducer Loss for insertions, Fast Conformer, 60 epochs, WER [%].\\nExtended evaluation.\\nLossSkipINSdev test WERD ↓WERDR ↑\\nWeight,Mode clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T – 20% 4.0 10.3 4.2 10.2 3.4\\nBypass-T -6,maxexcl 20% 3.2 7.9 3.3 8.0 1.2 65.7%\\nBypass-T -6,sumexcl 20% 3.0 7.5 3.3 7.9 1.1 67.6%\\nRNN-T – 50% 5.1 12.7 5.3 13.5 6.7\\nBypass-T -6,maxexcl 50% 4.4 10.3 4.1 10.7 3.9 41.8%\\nBypass-T -6,sumexcl 50% 3.9 10.3 4.3 10.5 3.7 44.8%\\nBypass-T -5,maxexcl 50% 4.1 10.2 4.5 10.4 3.6 46.3%\\nBypass-T -5,sumexcl 50% 3.6 9.2 4.0 9.4 2.6 61.2%\\nC Losses Visualization\\nWe additionally publish the Jupyter Notebook, which visualizes the lattices of the pro-\\nposed losses. The notebook can be found in the repository\\nhttps://github.com/artbataev/uol_final/blob/main/notebooks/Loss_Demo.ipynb .\\n30')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## text summarization\n",
    "from PyPDF2 import PdfReader\n",
    "pdfreader = PdfReader('documents/starrnnt.pdf')\n",
    "from typing_extensions import Concatenate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "# read text from pdf\n",
    "text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        text += content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4f3406a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "## Splittting the text\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=20)\n",
    "chunks = text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7df3bb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa940134",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## stuff: send the whole document to llm and get summary\n",
    "## map reduce: take a document, divide to chunks, get summary of each chunk, combine summaries\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type='map_reduce',\n",
    "    verbose=False\n",
    ")\n",
    "summary = chain.run(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ba7fcc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications. The study proposes three new loss functions, Star-Transducer, Bypass-Transducer, and Target-Robust-Transducer, to address errors in target texts, such as deletions, insertions, and substitutions. These modifications aim to improve system quality and performance in handling corrupted data. The project also evaluates the impact of errors on system performance and plans to integrate these losses into the open-source NeMo framework for large-scale production usage.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b2f66a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title: \"Empowering Speech Recognition Systems: A Journey of Innovation and Resilience\"\\n\\nSummary:\\n1. The speech focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems to improve robustness to errors in training data.\\n2. Various loss modifications such as Wild-card CTC, Star Temporal Classification, and Bypass Temporal Classification are introduced to handle missing or incorrect transcriptions in ASR tasks.\\n3. Penalties with exponential decay are used to improve network learning with increased possible alignments, with a focus on insertions having a larger impact than substitutions on the CTC criterion.\\n4. The project aims to investigate the impact of using partially incorrect transcripts on the quality of the RNN-Transducer system and proposes solutions to handle deletions, insertions, and substitutions.\\n5. Different loss functions like Bypass-Transducer and Target-Robust-Transducer are proposed to improve system quality in scenarios with errors in target texts, showing promising results in restoring system quality.\\n6. The speech also discusses resources and tools related to deep learning, neural networks, and natural language processing, providing insights into training models with corrupted data and evaluating hyperparameters for improved performance.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## apply to each chunk\n",
    "## Map Reduce With Custom Prompts\n",
    "chunks_prompt=\"\"\"\n",
    "Please summarize the below speech:\n",
    "Speech:`{text}'\n",
    "Summary:\n",
    "\"\"\"\n",
    "map_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                    template=chunks_prompt)\n",
    "\n",
    "## apply to the final summarization chunks\n",
    "final_combine_prompt='''\n",
    "Provide a final summary of the entire speech with these important points.\n",
    "Add a Generic Motivational Title,\n",
    "Start the precise summary with an introduction and provide the\n",
    "summary in number points for the speech.\n",
    "Speech: `{text}`\n",
    "'''\n",
    "final_combine_prompt_template=PromptTemplate(input_variables=['text'],\n",
    "                                             template=final_combine_prompt)\n",
    "\n",
    "summary_chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='map_reduce',\n",
    "    map_prompt=map_prompt_template,\n",
    "    combine_prompt=final_combine_prompt_template,\n",
    "    verbose=False\n",
    ")\n",
    "output = summary_chain.run(chunks)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29731dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RefineDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"Final Project: RNN-Transducer-based\n",
      "Losses for Speech Recognition on\n",
      "Noisy Targets\n",
      "Vladimir Bataev\n",
      "University of London\n",
      "10 March 2024arXiv:2504.06963v1  [eess.AS]  9 Apr 2025Contents\n",
      "1 Project Concept and Motivation 3\n",
      "1.1 Overview and Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "1.3 Related Work: Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n",
      "1.4 Related Work: Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n",
      "1.5 Related Work: Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n",
      "2 Literature Review 5\n",
      "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "2.2 CTC and Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "2.2.1 Connectionist Temporal Classification (CTC) loss . . . . . . . . . . . 5\n",
      "2.2.2 Wild-card CTC (W-CTC) . . . . . . . . . . . . . . . . . . . . . . . . . 5\n",
      "2.2.3 Star Temporal Classification (STC) . . . . . . . . . . . . . . . . . . . 6\n",
      "2.2.4 Bypass Temporal Classification (BTC) . . . . . . . . . . . . . . . . . 7\n",
      "2.2.5 Omni-temporal Classification (OTC) . . . . . . . . . . . . . . . . . . 7\n",
      "2.3 RNN-Transducer and Modifications . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "2.3.1 RNN-T loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n",
      "2.3.2 Graph-based RNN-Transducer framework . . . . . . . . . . . . . . . 8\n",
      "2.3.3 Stateless RNN-T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n",
      "2.3.4 Hybrid CTC-Transducer models . . . . . . . . . . . . . . . . . . . . . 9\n",
      "3 Project Design 9\n",
      "3.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
      "3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
      "3.3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n",
      "3.4 Backbone Encoder Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "3.5 Tools and Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n",
      "3.6 RNN-Transducer Details and Graph-based representation . . . . . . . . . . 11\n",
      "3.7 Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "3.7.1 Initial Project Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n",
      "3.7.2 Project Plan Reflection . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n",
      "4 Implementation 15\n",
      "4.1 Project Implementation Overview . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "4.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "4.3 Model and Training Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "4.4 Proposed Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n",
      "4.4.1 Star-Transducer (Star-T) . . . . . . . . . . . . . . . . . . . . . . . . . 17\n",
      "4.4.2 Bypass-Transducer (Bypass-T) . . . . . . . . . . . . . . . . . . . . . 17\n",
      "4.4.3 Target-Robust-Transducer (TRT) . . . . . . . . . . . . . . . . . . . . 19\n",
      "5 Evaluation 20\n",
      "5.1 Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "5.2 Error Impact Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n",
      "5.3 Dealing with Deletions: Star Transducer . . . . . . . . . . . . . . . . . . . . 21\n",
      "5.4 Dealing with Insertions: Bypass Transducer . . . . . . . . . . . . . . . . . . 22\n",
      "5.5 Dealing with Substitutions: Target-Robust-Transducer . . . . . . . . . . . . 22\n",
      "5.6 Arbitrary Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "16 Conclusion 24\n",
      "7 Report Parameters and Additional Notes 24\n",
      "8 Acknowledgments 25\n",
      "References 25\n",
      "Appendices 29\n",
      "A Arbitrary Errors - Learning Curve 29\n",
      "B Bypass-Transducer: Extended evaluation of hyperparameters 29\n",
      "C Losses Visualization 30\n",
      "List of Figures\n",
      "1 RNN-Transducer Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n",
      "2 WFSTs for RNN-Transducer, following [1] . . . . . . . . . . . . . . . . . . . 14\n",
      "3 Project Plan (Gantt Chart) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n",
      "4 WFSTs for Star-Transducer. ⟨sf⟩is a special symbol indicating skipping\n",
      "the frame. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n",
      "5 WFSTs for Bypass-Transducer. ⟨st⟩is a special symbol indicating skipping\n",
      "the token. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n",
      "6 WFSTs for Target-Robust-Transducer. ⟨sf⟩is a special symbol indicating\n",
      "skipping the frame. ⟨st⟩is a special symbol indicating skipping the token. . 20\n",
      "7 Arbitrary errors: learning curves for RNN-T (original and corrupted data)\n",
      "and Target Robust Transducer (corrupted data). . . . . . . . . . . . . . . . . 29\n",
      "List of Tables\n",
      "1 Baseline on LibriSpeech, WER [%]. . . . . . . . . . . . . . . . . . . . . . . 20\n",
      "2 Training RNN-T on data with errors, Fast Conformer, 60 epochs, WER [%]. 21\n",
      "3 Star-Transducer (Star-T) Loss for deletions, Fast Conformer, 60 epochs,\n",
      "WER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "4 Bypass-Transducer (Bypass-T) Loss for insertions, Fast Conformer, 60 epochs,\n",
      "WER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n",
      "5 Target Robust Transducer (TRT) Loss for substitutions, Fast Conformer, 60\n",
      "epochs, WER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "6 Target Robust Transducer (TRT) Loss for arbitrary errors, Fast Conformer,\n",
      "60 and 100 epochs, WER [%]. 50% of data is corrupted, using 15% for\n",
      "each class of errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n",
      "7 Bypass-Transducer Loss for insertions, Fast Conformer, 60 epochs, WER\n",
      "[%]. Extended evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n",
      "21 Project Concept and Motivation\n",
      "1.1 Overview and Template\n",
      "In our project, we will train an automatic speech recognition (ASR) system on noisy\n",
      "targets. We start with the template \"CM3015 Machine Learning and Neural Networks,\n",
      "Theme 1: Deep Learning on a public dataset,\" which describes the task of choosing a\n",
      "publicly available dataset and training a deep learning model on it. So, we will work with a\n",
      "neural network-based end-to-end ASR system, using LibriSpeech [2] dataset, a popular\n",
      "academic benchmark. We limit our task to RNN-Transducer [3] systems, which are widely\n",
      "used in production and provide state-of-the-art quality [4] in most cases.\n",
      "We are going beyond the standard task and focusing our research on making RNN-\n",
      "Transducer systems robust to noisy targets: unlike well-curated datasets, in the indus-\n",
      "try, the training data contains different errors due to the unreliability of the transcription\n",
      "sources or the inability to transcribe noisy speech accurately. To solve the problem of\n",
      "training on the noisy data, we will analyze the impact of different types of errors in training\n",
      "data on the quality of the RNN-Transducer system and explore different loss modifications\n",
      "to overcome the problem. We will construct the artificial training data by mutating correct\n",
      "transcripts from the LibriSpeech [2] training part, similar to the approaches used in the\n",
      "related work, and try to achieve the best possible quality on the development and test\n",
      "data standard for LibriSpeech.\n",
      "1.2 Motivation\n",
      "Training ASR systems usually requires a large amount of well-transcribed audio-text paired\n",
      "data. The process of dataset preparation often calls for filtering out \"noisy\" transcriptions\n",
      "using some pretrained model, which results in a lower amount of available data for train-\n",
      "ing. It is hard to obtain large, well-transcribed datasets for many languages and scenarios.\n",
      "On the other hand, data with limited transcript quality is widely available. Developing new\n",
      "approaches for working with weakly supervised setups can be beneficial in the following\n",
      "ways:\n",
      "• (1) making it easier to use non-well-curated datasets;\n",
      "• (2) improving the quality of ASR models without tricky filtering pipelines for raw data,\n",
      "making an approach fully \"end-to-end,\" and potentially getting benefits from more\n",
      "non-filtered data;\n",
      "• (3) improving ASR quality for low-resource languages when the data is extremely\n",
      "limited;\n",
      "• (4) transfer learning scenarios when another (imperfect) model transcribes the data.\n",
      "1.3 Related Work: Systems\n",
      "Most of the modern ASR systems use mel filter bank features extracted from the speech\n",
      "signal [3] and learn to map a sequence of feature vectors to the correct sequence of the\n",
      "units derived from the text (e.g., characters, subword units, words, or phonemes). There\n",
      "are three dominating types of end-to-end ASR systems [4]: Connectionist Temporal Clas-\n",
      "sification (CTC) [5], RNN-Transducer (RNN-T) [3] and attention-based encoder-decoder\n",
      "3(AED) [6]. CTC and RNN-Transducer rely on an explicit latent monotonic alignment be-\n",
      "tween the audio and corresponding transcript [7]. CTC is the most straightforward non-\n",
      "autoregressive system, which predicts each text unit independently. RNN-Transducer\n",
      "was introduced as a solution to fix the wholly conditional independence assumption of\n",
      "CTC and consists of 3 parts: an Encoder, which produces the representation of input\n",
      "features non-autoregressively; autoregressive Prediction network; and a Joint network,\n",
      "which combines their output and produces the final prediction [3]. Encoder-Decoder sys-\n",
      "tems with Attention [6] implicitly learn the alignment between the audio and text via the\n",
      "attention mechanism. We are primarily interested in RNN-Transducers since such sys-\n",
      "tems are suitable for streaming by design, widely used in production, and power most of\n",
      "the state-of-the-art monolingual models [7]. Initially, recurrent neural networks (RNNs)\n",
      "were used as an autoregressive prediction (thus, RNN-T is named after RNNs), but other\n",
      "non-recurrent architectures can also be used, e.g., a simple stateless network whose\n",
      "output depends on a fixed number of previous outputs [8].\n",
      "1.4 Related Work: Models\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications. The motivation behind the project includes the difficulty of obtaining well-transcribed datasets and the potential benefits of working with weakly supervised setups. The project reviews related work on ASR systems, including CTC, RNN-Transducer, and attention-based encoder-decoder systems. The emphasis is on RNN-Transducer systems due to their suitability for streaming and widespread use in production.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "In ASR, the dominating architecture for the encoder is a Conformer [9, 10, 4], which\n",
      "originates from a transformer block architecture [11] augmented with convolutional mod-\n",
      "ules [9]. The important part is that the original Conformer encoder subsamples the se-\n",
      "quence of input features (derived from the audio signal) 4 times, and recently, multiple\n",
      "architectures were proposed to subsample the input features 8 times to provide better\n",
      "speed without performance drop, e.g., Fast Conformer [12]. We will use the Fast Con-\n",
      "former architecture as a basic architecture for our model, focusing on loss modifications\n",
      "rather than changing the architecture.\n",
      "1.5 Related Work: Losses\n",
      "CTC is the loss that can naturally be represented [13] with weighted finite state transduc-\n",
      "ers (WFSTs). Due to the existence of the libraries that allow to construct differentiable\n",
      "WFSTs and use them to train deep learning systems, e.g., k2 [14], different modifications\n",
      "of CTC loss were proposed to solve the problem of different errors in training data. Partic-\n",
      "ularly, W-CTC (CTC with Wild Cards) [15] allows missing text at the start and the end of an\n",
      "utterance in transcription, Star Temporal Classification (STC) [16] allows missing labels\n",
      "anywhere, Bypass Temporal Classification (BTC) [17] solves the problem of insertions\n",
      "and substitutions. Recently proposed Omni-temporal Classification (OTC) [18] is a gen-\n",
      "eralized loss that combines all the previous work and proposes a CTC loss modification\n",
      "that is robust to any type of errors in transcripts.\n",
      "RNN-Transducer robustness to the errors in training data is still an unsolved prob-\n",
      "lem. The recent work about a graph-based framework for RNN-Transducer [1] proposes\n",
      "a generalized solution to develop different loss modifications based on WFSTs and also\n",
      "proposes a W-Transducer loss that can deal with missing transcripts at the start and end\n",
      "of the transcription (similar to W-CTC [15]). Moreover, an autoregressive prediction net-\n",
      "work can require modifications since its input in training time is a ground truth transcription\n",
      "and can be sensitive to incorrect text. We are planning to increase the complexity of the\n",
      "project gradually, starting from the \"under-transcribed\" case, when the training texts can\n",
      "contain missing words (deletions) similar to STC [16]. Then, we will explore more complex\n",
      "cases with insertions and substitutions, finally providing a general combined solution.\n",
      "42 Literature Review\n",
      "2.1 Introduction\n",
      "We start our review with the work on the CTC criterion and its modifications. Since both\n",
      "CTC and RNN-T take into account all possible alignments between features extracted\n",
      "from audio and text units, thus it is possible to apply some techniques to both of them.\n",
      "Then, we discuss the RNN-T loss and the relevant work to improve its robustness to\n",
      "errors, along with the structure of the Transducer models and differences between CTC\n",
      "and RNN-T that prevent direct application of CTC-based techniques.\n",
      "2.2 CTC and Modifications\n",
      "2.2.1 Connectionist Temporal Classification (CTC) loss\n",
      "The Connectionist Temporal Classification (CTC) was originally introduced in [5] as a\n",
      "replacement to a \"classical\" ASR pipeline based on hidden Markov Models (HMMs) and\n",
      "is historically the first so-called \"end-to-end\" ASR system. The original work proposes a\n",
      "solution for the automatic learning for the alignment between the target units (phonemes\n",
      "in this work, using the TIMIT [19] dataset) and the representation extracted by the neural\n",
      "network from the audio signal. The system utilizes RNNs (particularly bidirectional Long\n",
      "Short-Term Memory (BLSTMs) networks [20]) as its backbone. Mel-Frequency Cepstrum\n",
      "Coefficients are extracted from the input audio every 10ms, which forms the input for the\n",
      "neural network. The output of the network has a softmax activation and is interpreted\n",
      "for each label as a probability of observing the label at the current time frame [5], as\n",
      "in the classification task for each frame. The vocabulary is augmented with a special\n",
      "⟨blank⟩symbol, and the algorithm considers all possible variants of transcriptions that\n",
      "map to the original text units after removing duplicated predictions and the ⟨blank⟩symbol.\n",
      "The loss is a minus log probability of all possible correct alignments (defined by the rule\n",
      "described above) given the audio features. Thus, training maximizes the log probability for\n",
      "the possible alignments without requiring a forced alignment, which was used in classical\n",
      "HMM-based systems. The paper also proposes an efficient forward-backward algorithm\n",
      "to calculate the CTC loss and gradients and shows the system’s efficiency in predicting\n",
      "phonemic transcriptions of utterances.\n",
      "The paper about CTC loss [5] forms a basis for our research since we plan to ap-\n",
      "ply techniques to modify the possible alignments in the CTC framework to the RNN-\n",
      "Transducer system. The work shows that it is not necessary to have one perfect align-\n",
      "ment as a target, but once we construct a mapping between the ground truth target and\n",
      "the inner latent alignment (in the paper - sequences with repeated labels and additional\n",
      "⟨blank⟩symbols, that are removed in decoding), we can use a full-sum training (taking\n",
      "into account all possible alignments), and the system can effectively solve the ASR task.\n",
      "2.2.2 Wild-card CTC (W-CTC)\n",
      "Wild-card CTC (W-CTC) was proposed in [15] to solve the problem when the utterance\n",
      "is partially transcribed, and the transcription can be missing at the start, at the end, or\n",
      "on both sides. The work introduces a simple but efficient modification of CTC criterion\n",
      "computation, using a special \"*\" (star) symbol that can be prepended to each transcrip-\n",
      "tion, and means that at the start of the transcription, any possible sequence (of any size,\n",
      "5including the empty sequence) of symbols can be missing. This allows a simple modi-\n",
      "fication of the dynamic programming algorithm for CTC computation introduced in [5] to\n",
      "handle the alignments not only when the ground truth is entirely correct but also start and\n",
      "end the alignments between text units and features extracted from audio at any part of the\n",
      "audio, but with the assumption, that the part of the audio fully matches the transcription.\n",
      "The authors also use a TIMIT [19] dataset, randomly masking a portion of the start/end of\n",
      "the utterance transcriptions, thus showing the effectiveness of the approach (compared\n",
      "to the original CTC) on partially transcribing ASR data in a synthetic setup. Additionally,\n",
      "the authors validate the effectiveness of W-CTC on Optical Character Recognition (OCR)\n",
      "and Continuous Sign Language Recognition (CSLR) tasks.\n",
      "The paper is interesting as the first work which introduces a synthetic setup with under-\n",
      "transcribed data to study and develop training criteria for ASR robust to corrupted targets.\n",
      "The proposed W-CTC criterion uses a larger possible amount of alignments but still con-\n",
      "verges and surpasses CTC even when a small portion of the data is corrupted. Also,\n",
      "the paper shows the application of the techniques to OCR and CSLR tasks, which shows\n",
      "the potential impact of modifications of losses used for ASR on areas outside the speech\n",
      "recognition field.\n",
      "2.2.3 Star Temporal Classification (STC)\n",
      "Star Temporal Classification [16] was proposed as a generalization for the previous work\n",
      "when the transcription is only partial, and between any pair of labels, an arbitrary number\n",
      "of words can be missing. The work uses the \"*\" star token to represent zero or more\n",
      "text tokens and considers alignments between the encoded signal features and a target,\n",
      "where the \"*\" token is inserted between all labels and also appended to the start and\n",
      "the end of the utterance. Thus, the loss allows the network to output any sequence of\n",
      "labels corresponding to the presented corrupted ground truth text after removing some\n",
      "of the words (the ASR system should emit all the words from the target but can insert\n",
      "other words between them). This approach significantly increases the number of possible\n",
      "alignments and does not allow the training of the neural network directly. The problem is\n",
      "solved by introducing a penalty for the \"*\" token λ, with exponential decay during training,\n",
      "starting from the high values and decreasing once the network converges. The penalty\n",
      "hyperparameter is also adjusted based on the number of missing words in transcriptions.\n",
      "The authors demonstrate the approach using synthetic data derived from LibriSpeech [2],\n",
      "using partially masked transcripts with different probabilities (up to 70%). The approach is\n",
      "practical even when using greedy decoding, but the authors also show that decoding with\n",
      "an n-gram language model (LM) and rescoring hypotheses with Transformer [11] based\n",
      "LM also improves the system’s quality. For the implementation, unlike previous work, the\n",
      "GTN [21] framework for differentiable WFSTs is used, which simplifies the development\n",
      "of the new loss.\n",
      "This paper is crucial for our work since it allows us to solve the problem with deletions\n",
      "in the transcribed texts for CTC. The critical part is that the direct solution (considering all\n",
      "possible alignments in the loss) leads to failure, and using the adjustable penalty is crucial\n",
      "to solve the issue. Also, the construction of LibriSpeech-based data is an important part.\n",
      "We will consider an analogous approach for investigating similar cases for RNN-T.\n",
      "62.2.4 Bypass Temporal Classification (BTC)\n",
      "Bypass Temporal Classification [17] solves part of a weakly-supervised setup with unre-\n",
      "liable transcripts: substitutions and insertions. Similar to the previous work, the authors\n",
      "propose CTC target graph modification to allow the alignments when some text units\n",
      "from the target are changed (substitutions) or not used (insertions in the text, thus allow-\n",
      "ing deletion from the alignment). Remarkably, the ground truth is represented as a linear\n",
      "WFST, where the forward arcs contain ground truth labels, and each arc also presents\n",
      "a parallel \"bypass\" arc with a penalty, which allows to skip the token by emitting zero or\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications. The motivation behind the project includes the difficulty of obtaining well-transcribed datasets and the potential benefits of working with weakly supervised setups. The project reviews related work on ASR systems, including CTC, RNN-Transducer, and attention-based encoder-decoder systems. The emphasis is on RNN-Transducer systems due to their suitability for streaming and widespread use in production. Additionally, the project will utilize the Fast Conformer architecture for the encoder, focusing on loss modifications rather than changing the architecture. Various loss modifications such as W-CTC, STC, and BTC have been explored to address errors in training data and improve the robustness of the RNN-Transducer system. The project aims to gradually increase the complexity by starting with under-transcribed cases and eventually providing a general combined solution for handling different types of errors in transcripts.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "more tokens. The authors also use the penalty with an exponential decay to make the\n",
      "network learn using the increased number of possible alignments. The authors evaluate\n",
      "their solution on both TIMIT [19] and LibriSpeech [2] datasets, constructing the training\n",
      "data with substitutions and insertions separately and combining them. Interestingly, it is\n",
      "shown that for the CTC criterion, the impact of insertions is larger than for substitutions,\n",
      "and with 50% of insertions, it is impossible to train a system with the pure CTC criterion.\n",
      "This paper solves the remaining piece of the puzzle of training a CTC-based system\n",
      "with partially incorrect transcripts. We will also investigate similar approaches for RNN-T.\n",
      "Moreover, this paper encouraged us to start the exploration of the problem by comparing\n",
      "the impact of different errors on the training criterion behavior and starting our solution\n",
      "from the most disruptive problem.\n",
      "2.2.5 Omni-temporal Classification (OTC)\n",
      "The Omni-temporal Classification [18] finally combines the approaches to construct a\n",
      "universal loss to handle all the possible cases of errors. The authors utilize the \"bypass\"\n",
      "arcs to skip frames, along with self-loops that represent substitutions and insertions, and\n",
      "two separate penalties for them following the previous work. The authors used synthetic\n",
      "data generated from the train-clean-100 subset of LibriSpeech [2] and also trained a sys-\n",
      "tem on the original subsets from LibriVox [22] that were originally used to construct the\n",
      "LibriSpeech data, using segmentation and filtering with pretrained models, as described\n",
      "in [2]. The combination of the losses leads to significant improvements for all types of\n",
      "errors and allows training of the ASR system even on the original unsegmented data with\n",
      "errors without a cleaning pipeline.\n",
      "This work contains a detailed exploration of the impacts of different penalty values\n",
      "on the quality of the network, which can provide some insights for working with similar\n",
      "alignment-based modification approaches for RNN-T. The code is published, and we can\n",
      "try to reproduce the setup if necessary. Moreover, we can try this criterion as a part of the\n",
      "potential solution with hybrid CTC-RNN-T models for a \"CTC head,\" which we will discuss\n",
      "further. Moreover, we found it curious that despite the beneficial impact of the loss when\n",
      "training ASR system with unfiltered raw data from LibriVox, the gap still exists for the\n",
      "carefully filtered LibriSpeech data (e.g., authors report [18] 12.5% vs 8.2% WER on test-\n",
      "other for these setups), which can indicate that the problem is still not fully solved even for\n",
      "CTC criterion, and multi-stage pipeline is still essential to obtain the best performance.\n",
      "2.3 RNN-Transducer and Modifications\n",
      "2.3.1 RNN-T loss\n",
      "Recurrent Neural Network Transducer (RNN-Transducer, RNN-T) [3] was developed as an\n",
      "improvement of the CTC [5] systems to overcome problems related to conditional inde-\n",
      "7pendent assumption. The system contains three components: (1) transcription network,\n",
      "which is usually referred to as an \"Encoder\"(e.g., [23]), that operates on the features\n",
      "derived from audio and is similar to the one used in CTC system; (2) prediction network,\n",
      "that outputs the predictions based on the previously decoded symbol in inference time,\n",
      "and utilizes a ground truth text in training time; and (3) a combination of the outputs of the\n",
      "previous two networks that makes final predictions, which is usually referred as a Joint\n",
      "network in the literature [23]. The prediction network is autoregressive and provides a\n",
      "crucial improvement for the ASR system. The system also uses a ⟨blank⟩symbol, but the\n",
      "meaning differs from CTC: this symbol means the system should end decoding the current\n",
      "frame and transition to the next frame from the encoder. Unlike CTC, repeated symbols\n",
      "are not allowed, but the advantage is that the system can predict more than one text unit\n",
      "for each frame. The RNN-T loss function takes into account all possible monotonic align-\n",
      "ments between encoder and prediction network outputs, which makes the system similar\n",
      "to CTC with implicit alignment learning.\n",
      "The crucial difference for the RNN-T system is that the prediction network is autore-\n",
      "gressive (usually uses LSTM as a backbone), making it significantly more challenging to\n",
      "deal with imperfect transcripts. Since ground truth text is used during training (so-called\n",
      "\"teacher forcing\" algorithm), corrupted tokens can prevent the correct alignment learn-\n",
      "ing. The autoregressive nature also prevents applying CTC-based approaches directly to\n",
      "RNN-T and may require modifications of the network itself along with the training criterion.\n",
      "2.3.2 Graph-based RNN-Transducer framework\n",
      "The recent work about RNN-Transducer modifications [1]1brings the connection between\n",
      "WFSTs and Transducer architecture, allowing representing the RNN-T loss computation\n",
      "with the graph, where the arc weights are taken from the Joint network output, and thus\n",
      "the direct application of graph algorithms, including full-sum alignment learning, are pos-\n",
      "sible. The work presents two approaches for representing original and modified RNN-T\n",
      "graphs, either as a direct grid (\"Grid-Transducer\") or as a composition of acoustic and\n",
      "textual schemas (\"Compose-Transducer\"), which after the composition and connect oper-\n",
      "ations exactly matches the Grid-Transducer representation (\"connect\" operation removes\n",
      "the states that do not belong to any path from the start and the end state; it is optional\n",
      "for loss computation since such states do not affect the alignment probabilities since not\n",
      "belonging to any alignment). The composition is slower to compute but allows for the\n",
      "development of losses using graphs that can be easily debugged visually. The authors\n",
      "build the framework on top of k2 [14] library for differentiable WFSTs and show that the\n",
      "loss computation can be as efficient as the optimized CUDA-based code. Moreover, the\n",
      "authors present a W-Transducer, which can solve the problem of deletions at the start\n",
      "and the end of the utterance, similar to the W-CTC discussed above. The graph for the\n",
      "loss uses two groups of skip-connections from the start of the time grid to all other time\n",
      "steps and from each time step in the grid to the last, which allows to use the alignments\n",
      "where some frames at the start and the end are skipped, but the network should emit the\n",
      "full training text. The effectiveness of this approach was demonstrated on LibriSpeech\n",
      "data by randomly removing 20% and 50% of the labels from text from both sides of each\n",
      "utterance.\n",
      "In our work, we will use the proposed WFST framework for training RNN-T on noisy\n",
      "targets to simplify the development of the losses. W-Transducer solves the easiest case\n",
      "when the autoregressive prediction network does not use a corrupted input and thus\n",
      "1Disclaimer: the author of the Final Project participated in the development of this approach and is a co-author of the paper.\n",
      "8can remain unmodified. We are planning to work with more complex cases, and such\n",
      "modifications can be required along with the loss customization. We will discuss this\n",
      "approach in more detail in Section 3.6.\n",
      "2.3.3 Stateless RNN-T\n",
      "The work [8] proposes \"RNN-T with StateLess Prediction Network (RNNT -SLP),\" revising\n",
      "the need for recurrent networks for the prediction network part of the RNN-T system.\n",
      "The authors questioned the popular opinion that the prediction network behaves similarly\n",
      "to the language model in traditional ASR systems. They tried pretraining the recurrent\n",
      "network on text-only data to predict the next symbol and initializing the prediction network\n",
      "with pretrained weights and found that such pretraining does not lead to any improvement.\n",
      "Moreover, replacing the prediction network with a simple \"stateless\" module, in which\n",
      "prediction is based only on the last symbol (which is similar to 2-gram LM), leads to a\n",
      "comparable overall system performance with the RNN-based system.\n",
      "Despite there are other works that show that the prediction network can behave like a\n",
      "language model, especially in a factorized architecture [24, 25], we are interested in this\n",
      "work showing the possibility of using simpler networks for this part of the model. Since\n",
      "a stateless prediction network can be significantly more robust for corrupted targets, in\n",
      "which prediction is dependent only on a small context, we consider this work an important\n",
      "option for changing the prediction network architecture.\n",
      "2.3.4 Hybrid CTC-Transducer models\n",
      "As a last item of our review, we will discuss the hybrid architecture, which uses both RNN-\n",
      "T and CTC losses, proposed in [26]. The work proposes training the neural transducer\n",
      "with an auxiliary CTC loss on top of the encoder (as a separate \"head\"), combining the\n",
      "systems for further accelerating the decoding: if the CTC head predicts the blank label,\n",
      "such frame can be skipped in decoding, resulting in a significant inference acceleration\n",
      "with tiny quality degradation. The work [27] proposes the use of such hybrid systems\n",
      "to accelerate not only inference but also the training speed, using more lightweight CTC\n",
      "head prediction to compute RNN-T loss with the smaller number of possible alignments\n",
      "(pruning RNN-T loss lattice).\n",
      "This work can be relevant for our research due to the discussed above OTC criterion,\n",
      "which is robust to noisy targets: in a hybrid training setup, we can use the prediction of the\n",
      "additional head, trained with the OTC loss, to identify corrupted tokens and thus modify\n",
      "the input for prediction network or the loss based on this information.\n",
      "3 Project Design\n",
      "In our project, we plan to investigate the behavior of the RNN-Transducer architecture in\n",
      "a weakly supervised setup when part of the data is corrupted and modify the RNN-T loss\n",
      "to improve the system’s quality. We will consider loss modification techniques that do not\n",
      "require any change in the overall neural architecture and decoding algorithms to make\n",
      "them compatible with current production systems.\n",
      "93.1 Objectives\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications. The motivation behind the project includes the difficulty of obtaining well-transcribed datasets and the potential benefits of working with weakly supervised setups. The project reviews related work on ASR systems, including CTC, RNN-Transducer, and attention-based encoder-decoder systems. The emphasis is on RNN-Transducer systems due to their suitability for streaming and widespread use in production. Additionally, the project will utilize the Fast Conformer architecture for the encoder, focusing on loss modifications rather than changing the architecture. Various loss modifications such as W-CTC, STC, and BTC have been explored to address errors in training data and improve the robustness of the RNN-Transducer system. The project aims to gradually increase the complexity by starting with under-transcribed cases and eventually providing a general combined solution for handling different types of errors in transcripts. The project will also investigate the impacts of different penalty values on the quality of the network and explore graph-based RNN-Transducer frameworks for training on noisy targets. Additionally, the project will consider stateless RNN-T models and hybrid CTC-Transducer architectures to improve system performance in weakly supervised setups.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "93.1 Objectives\n",
      "We define the following objectives for our work:\n",
      "• (1) Investigate how using partially incorrect transcripts impacts the quality of the\n",
      "RNN-Transducer system, separating deletions, substitutions, and insertion cases\n",
      "• (2) Investigate techniques that allow to improve the performance of the system in\n",
      "the conditions described above\n",
      "• (3) Combine the solutions provided in (2) to solve the general case of training the\n",
      "ASR Transducer-based system with unreliable transcript and evaluate the final so-\n",
      "lution when it is unknown what type of errors the data contains.\n",
      "3.2 Metrics\n",
      "The key metric for assessing the quality of an ASR system is word error rate (WER) [28].\n",
      "The additional metric commonly used to assess the speed of the ASR system is a real-\n",
      "time factor (RTF), which indicates how much audio the system can process given the\n",
      "fixed time, and usually, the tradeoff between speed and quality is important when consid-\n",
      "ering different models. In our project, we focus only on modifications that have a minor\n",
      "impact on the inference speed after the model is trained; thus, we report only WER in our\n",
      "experiment.\n",
      "The word error rate is a specification of a Levenshtein distance [28, 29], defined for\n",
      "two sequences of words (hypothesis and ground truth), and is calculated as a number of\n",
      "substitutions (SUB), insertions (INS) and deletions (DEL) divided by the number of ground\n",
      "truth (correct) words.\n",
      "WER =SUB +INS +DEL\n",
      "CORRECT\n",
      "Lower WER means that the system makes more accurate predictions. We will also use\n",
      "the term \"accuracy,\" usually defined as accuracy = 1−WER (higher accuracy value\n",
      "means better prediction).\n",
      "Since we are working with the conditions that show the degradation of the standard\n",
      "ASR training pipeline, we introduce two additional metrics. WER difference (WERD) in-\n",
      "dicates the system degradation and is a difference between the WER that the system\n",
      "achieves in a particular setup and the WER of the baseline on the non-modified (original)\n",
      "data.\n",
      "WERD =WER modified _data−WER original _data\n",
      "We are interested in minimizing the degradation of the system, thus defining the relative\n",
      "improvement of the system as WERDR.\n",
      "WERDR =WERD RNN−T−WERD Proposed\n",
      "WERD RNN−T\n",
      "3.3 Data\n",
      "The primary data for the project is a LibriSpeech [2] corpus, which consists of 3 subsets\n",
      "for training data (960 hours total), two development sets ( dev-clean anddev-other , 5.4 and\n",
      "5.3 hours respectively), and two test sets ( test-clean andtest-other , 5.4 and 5.1 hours).\n",
      "We will use the full training part to generate artificial training data by corrupting the texts\n",
      "10with artificial deletions, substitutions, and insertions. We will use the dev-other set for\n",
      "validation during training and choosing the optimal checkpoints, and we will finally assess\n",
      "our models on the test-other . As it is common in the ASR research, we will report WER\n",
      "for all development and test sets.\n",
      "3.4 Backbone Encoder Models\n",
      "Currently, the dominating architecture [7] for the encoder is the Conformer [9] model. The\n",
      "original Conformer-encoder processes the input sequence of vectors and produces the\n",
      "representation 4 times smaller by the time dimension (4x subsampling). For our initial\n",
      "experiments, we will use Fast Conformer [12] (114M parameters), which subsamples\n",
      "the input by the factor of 8 without accuracy degradation by using depth-wise separable\n",
      "convolutions [30], which allows faster training and inference.\n",
      "3.5 Tools and Frameworks\n",
      "We are planning to use NeMo [31] framework for experiments, which is based on PyTorch-\n",
      "Lightning [32] for easiness of extending the models, and train them on clusters. NeMo\n",
      "provides stubs for ASR models and capabilities to use WandB [33] platform for experiment\n",
      "management. When required, we will use pure PyTorch [34] and k2 [14] library to modify\n",
      "losses and prediction network, also using a WFST framework for RNN-Transducer [1]\n",
      "implemented in NeMo.\n",
      "3.6 RNN-Transducer Details and Graph-based representation\n",
      "RNN-Trasducer [3] model schema is shown in Fig. 1. The model consists of three parts.\n",
      "Encoder neural network transforms a sequence of input feature vectors derived from an\n",
      "audio into a latent representation - a sequence of vectors ⟨e0, e1, ..., e t−1⟩with the length\n",
      "ofT. The number of output vectors is usually smaller than the input due to applying\n",
      "strided convolutions or pooling layers along the time axis, which reduces the output size\n",
      "by a fixed factor (subsampling) and makes the model more computationally efficient. The\n",
      "Prediction network (usually a recurrent network) in training time takes a ground truth\n",
      "sequence of text units as an input padded with a special start-of-sequence symbol ⟨SOS⟩\n",
      "(in practice usually ⟨blank⟩symbol is reused for this purpose) and produces a latent repre-\n",
      "sentation – sequence of vectors ⟨p0, p1, ..., p u⟩with the length of U+ 1(Uis the number of\n",
      "text units in the text representation). The Joint network combines all combinations of ei\n",
      "withpjvectors and produces a 3-dimensional tensor for the size Tx(U+ 1)xV, where Vis\n",
      "a vocabulary size augmented with the ⟨b⟩(⟨blank⟩) symbol (in practice, the 4-dimensional\n",
      "tensor is used due to additional batch dimension). Thus, ji,jrepresents the vector pro-\n",
      "duced from a combination of piandpj. In practice, the Joint network is relatively simple\n",
      "and computes the sum of vectors, non-linearity (e.g., ReLU), and a projection into the\n",
      "space of size V, (e.g., ji,j=Project (ReLU (ei+pi))). If the vectors eiendpjare of differ-\n",
      "ent sizes, they are firstly projected to the space with the same dimension. The Softmax\n",
      "activation produces a distribution over the vocabulary units, and the dynamic program-\n",
      "ming algorithm is used to compute the probability over all possible paths in the graph (in\n",
      "the code, all computations are produced in log scale for numerical stability purposes).\n",
      "Each path represents a possible alignment, where blank symbols can be inserted be-\n",
      "tween labels (e.g., for the Fig.1 1 \"C ⟨b⟩A T⟨b⟩ ⟨b⟩ ⟨b⟩\" is the possible path for the target\n",
      "11text \"CAT\" represented as \"C,\" \"A,\" and \"T\" units). The number of ⟨b⟩labels is the num-\n",
      "ber of frames the Encoder produces. The minus log probability of all possible alignments\n",
      "forms an RNN-T loss value.\n",
      "Forgreedy decoding , since the ground truth text is unknown, a nested loop is used:\n",
      "for each encoder vector, the algorithm sequentially obtains the next prediction with the\n",
      "maximum probability, starting from ⟨SOS⟩symbol, and computes the Prediction network\n",
      "output for the new decoded symbol, combining with the current encoder vector and com-\n",
      "puting Joint network output. Once the ⟨b⟩symbol is found, this symbol is not fed into the\n",
      "Prediction network, but the inner loop stops, and the decoding process starts for the next\n",
      "encoder vector. More complex decoding algorithms exist, but greedy decoding is widely\n",
      "used in practice due to the best speed and near-optimal quality in most scenarios [7], so\n",
      "we are focusing on this approach.\n",
      "The computation of the RNN-T loss can be represented with WFSTs , as shown in [1].\n",
      "The original work presents the approach more theoretically, and here we will discuss the\n",
      "practical implementation, which is a basis for our work. A weighted finite state transducer\n",
      "is a graph with states and arcs, whose arcs represent transitions from input to output\n",
      "labels with a corresponding weight. In k2 [14] library, it is allowed to store an arbitrary\n",
      "number of labels (not only an input/output label for each arc). Thus, we use a tuple\n",
      "of labels as input labels for some graphs in our representation 2. We can construct a\n",
      "computational graph using 3 labels for each transition: output (ground truth) label, unit\n",
      "index (in a sequence of ground truth units), and encoder vector index, see 2c. Given these\n",
      "labels by using \"index select\"2operation, we can populate a lattice with the weights from\n",
      "the Joint network output, and apply a differentiable computation of the full-sum loss3. This\n",
      "is a \"Grid-Transducer,\" described in [1]. To simplify the development, we can construct\n",
      "two schemas (\"Compose-Transducer\"). Unit schema 2a represents text units, and each\n",
      "transition has a tuple of input symbols (Unit :Unit_Index )and an output symbol Unit.⟨b⟩\n",
      "units represent self-loops, and other units represent transitions over the ground truth text.\n",
      "The time schema 2b is a simple linear graph representing transition over time and self-\n",
      "loops for each state with all the vocabulary symbols as inputs and time index as output.\n",
      "It is much easier to visually debug the representation with the separated schemas, and\n",
      "their composition matches the final lattice 2c. We are planning to experiment with the\n",
      "separated schemas to make the RNN-T loss more robust to corrupted targets, but for the\n",
      "final stage, we are planning to provide an efficient code for lattice construction.\n",
      "3.7 Plan\n",
      "3.7.1 Initial Project Plan\n",
      "Our work initial work plan published in the \"Draft Report\" (Midterm) is shown in Fig. 3.\n",
      "(a) Minimal RNN-T Implementation . After initial preparation, which was already done\n",
      "before finishing this report (initial exploration, project proposal, literature review), we will\n",
      "implement a minimal codebase to experiment with RNN-T. During our exploration, we\n",
      "found that RNN-T models in NeMo provide a lot of functionality but are not easy to extend.\n",
      "So, we will implement the lightweight pipeline with the following capabilities:\n",
      "• Lightweight Joint and Prediction networks\n",
      "• Greedy Decoding\n",
      "2https://pytorch.org/docs/stable/generated/torch.index_select.html\n",
      "3https://k2-fsa.github.io/k2/python_api/api.html?highlight=get_tot_scores#k2.Fsa.get_tot_scores\n",
      "12Figure 1: RNN-Transducer Schema\n",
      "• Full model training pipeline\n",
      "• Compatibility with NeMo encoders (including Conformer-based encoders as de-\n",
      "scribed above), since we are not planning to customize the encoder part.\n",
      "Then, we plan to train the baseline on the original LibriSpeech data with a Fast Con-\n",
      "former encoder to check that our model can achieve comparable quality with the native\n",
      "implementation of RNN-T in NeMo.\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications. The motivation behind the project includes the difficulty of obtaining well-transcribed datasets and the potential benefits of working with weakly supervised setups. The project reviews related work on ASR systems, including CTC, RNN-Transducer, and attention-based encoder-decoder systems, with a focus on RNN-Transducer systems due to their suitability for streaming and widespread use in production. The project will utilize the Fast Conformer architecture for the encoder and explore various loss modifications such as W-CTC, STC, and BTC to address errors in training data and improve system robustness. The project aims to gradually increase complexity by starting with under-transcribed cases and providing a general combined solution for handling different types of errors in transcripts. Additionally, the project will investigate the impacts of different penalty values on network quality and explore graph-based RNN-Transducer frameworks for training on noisy targets. State-of-the-art tools and frameworks like NeMo and PyTorch will be used for experiments, with a focus on modifying losses and prediction networks. The project will also consider stateless RNN-T models and hybrid CTC-Transducer architectures to improve system performance in weakly supervised setups.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "(b) Impact of imperfect transcripts . We will generate the data derived from Lib-\n",
      "riSpeech with 20% and 50% of deletions, substitutions, and insertions separately (6 sets\n",
      "in total) and train the models to study the model’s behavior in such conditions.\n",
      "(c) \"Deletions\" case. Our preliminary studies showed that the deletions in the tran-\n",
      "scripts are the hardest case for RNN-T. We will try to solve this case by modifying the loss\n",
      "function. At this stage, we focus on improving the performance and not trying to find the\n",
      "perfect hyperparameters and/or provide the fastest implementation for the loss function.\n",
      "(d) \"Insertions\" and \"substitutions.\" We will explore different approaches discussed\n",
      "in the literature review to improve the system’s performance in such conditions.\n",
      "(e) Combined Solution . Combining the solutions (c) and (d) will allow for solving the\n",
      "universal problem of partially correct transcripts. We will generate additional training data\n",
      "for cases with all types of errors.\n",
      "13(a) RNN-Transducer Unit Schema. Labels:\n",
      "(text_unit, unit _position ) :text_unit\n",
      "(b) RNN-Transducer Time Schema. Labels:\n",
      "text_unit :frame _number\n",
      "(c) RNN-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\n",
      "Figure 2: WFSTs for RNN-Transducer, following [1]\n",
      "(f) Conformer Medium (4x subsampling). At this stage, we will apply our solution to\n",
      "the Conformer Medium model to study its behavior when the encoder reduces the input 4\n",
      "times.\n",
      "(g) Fast loss implementation. This task focuses on the quality and speed of our\n",
      "code. We will provide a clean and fast \"ready to use\" solution for our loss and prediction\n",
      "network modifications.\n",
      "(h) Study hyperparameters in detail. This task will allow us to get insights about the\n",
      "hyperparameters of the systems proposed in (c)-(e).\n",
      "3.7.2 Project Plan Reflection\n",
      "The sections (a)-(b) were crucial for our project but imposed minimal risks due to well-\n",
      "explored existing solutions. The most critical risks came from (c) and (d) cases since,\n",
      "according to our knowledge, no solutions existed for such tasks. To mitigate these risks,\n",
      "we also considered using a hybrid CTC-Transducer architecture with OTC loss, which can\n",
      "solve the problem, at least for the CTC head, and we can use its predictions to train the\n",
      "Transducer part instead of corrupted ground truth labels. Combined solution (e) was a\n",
      "key part of finalizing our project. We considered parts (f)-(h) as a fair improvement but not\n",
      "an essential contribution to our work and planned to focus on them only after finishing the\n",
      "main part.\n",
      "In our final stage, we elaborated a successful solution for all discussed cases, provid-\n",
      "14Figure 3: Project Plan (Gantt Chart)\n",
      "ing a \"drop-in\" replacement for the RNN-Transducer loss for the cases without changing\n",
      "the model architecture at all. We also provided a fast implementation for the losses. Due\n",
      "to a lack of computational resources and difficulties training the system within the novel\n",
      "setup, we focused more on exploring parameters for Bypass Transducer loss, as dis-\n",
      "cussed in Section 5.4. We omitted experiments with Conformer Medium (f) and left them\n",
      "for future work.\n",
      "4 Implementation\n",
      "The implementation is published in the GitHub repository :https://github.com/artbataev/\n",
      "uol_final . In this section, we describe the implementation with the links to the origi-\n",
      "nal files. Additional visualization of the produced lattices for all proposed losses can be\n",
      "found in the Jupyter Notebook https://github.com/artbataev/uol_final/blob/main/\n",
      "notebooks/Loss_Demo.ipynb .\n",
      "4.1 Project Implementation Overview\n",
      "The main goal of the project is to provide a solution to deal with different types of er-\n",
      "rors in the RNN-Transducer framework. We want to make our models comparable and\n",
      "compatible with the publicly available state-of-the-art models and want further to propose\n",
      "solutions to the NeMo [31] framework. So, we reuse components from NeMo, focusing\n",
      "on customization and modifications of the necessary parts.\n",
      "Firstly, we make a minimal necessary code of RNN-T for our experiments, providing\n",
      "implementation containing the RNN-T model and customizable Joint and Prediction net-\n",
      "works. We reuse Conformer [9] blocks from NeMo [31] for the Encoder network.\n",
      "The Prediction network is a 1-layer LSTM with 640 hidden units, implemented\n",
      "inMinPredictionNetwork class. The Joint network, as discussed in Section 3.6, applies\n",
      "15two projections of the output of Encoder and Prediction networks (2 linear layers) to the\n",
      "dimension of 640, sums the vectors, applies ReLU non-linearity, and projects the out-\n",
      "put (one more linear layer) into 1025-dimensional space (1024 BPE units and ⟨b⟩). It is\n",
      "implemented in MinJoint class.\n",
      "We also implement a greedy decoding algorithm for evaluation in min_rnnt/decoding.py .\n",
      "In our experimental setup, we are following the Fast Conformer [12] training pipeline4.\n",
      "The encoder has 108.7M parameters, prediction network 3.9M, and Joint 1.4M (totally\n",
      "114M).\n",
      "4.2 Data Preprocessing\n",
      "We preprocess LibriSpeech [2] data and apply speed perturbation with rates 0.9and 1.1\n",
      "(3x audio data), using the published preprocessing script5to make our pipeline compara-\n",
      "ble with published models.\n",
      "We use log-mel filterbanks extracted from audio every 10ms with the window 25ms\n",
      "and apply SpecAugment [35] in training. We use the vocabulary of 1024 BPE [36] tokens\n",
      "extracted using SentencePiece [37] library for text units.\n",
      "4.3 Model and Training Pipeline\n",
      "We set up training of our model for 200 epochs using AdamW [38] optimizer with Co-\n",
      "sine annealing [39] learning rate schedule with a linear warmup for 40 epochs and the\n",
      "maximum learning rate of 5e−3. For experiments except for the baseline, we stopped\n",
      "training the model after 60 epochs since we are interested in the relative difference in\n",
      "model quality, and achieving the best possible accuracy is not our priority at this stage.\n",
      "We are reporting the results for the best checkpoint chosen on the dev-other validation\n",
      "set. For all experiments, we maintain a global batch size of 2048. We are training models\n",
      "on clusters using NVIDIA A100 (mixed-precision with bfloat16) and V100 GPUs (float32\n",
      "full-precision), and depending on the availability of the resources varying local batch size\n",
      "from 8 to 32 to fit into memory and adjusting gradient accumulation to make the global\n",
      "batch size constant. We did not observe any difference in quality for a fixed global batch\n",
      "size when using an arbitrary number of nodes, varying local batch size, and using mixed\n",
      "or full precision. So, we are not reporting these details for each experiment.\n",
      "4.4 Proposed Losses\n",
      "In our work, we propose three modifications of the RNN-T loss:\n",
      "• Star-Transducer to dial with arbitrary deletions\n",
      "• Bypass-Transducer to solve the case of insertions\n",
      "• Target-Robust-Transducer, which is the combination of the previous modifications,\n",
      "allows to mitigate the problems of substitutions in target texts and also can be used\n",
      "as a universal loss when the type of errors is unknown.\n",
      "4github.com/NVIDIA/NeMo/blob/v1.21.0/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml\n",
      "5https://github.com/NVIDIA/NeMo/blob/v1.21.0/scripts/dataset_processing/get_librispeech_data.py\n",
      "16The modifications are implemented in minrnnt/losses subpackage. All the classes\n",
      "follow Graph-RNNT [1] framework and inherit GraphRnntLoss class from the NeMo [31]\n",
      "framework and reuse its methods. The implementation uses k2 [14] library. As described\n",
      "in Section 3.6, the computational lattice can be constructed as a composition of temporal\n",
      "and unit schemas, implemented in get_temporal_schema and get_unit_schema respec-\n",
      "tively. The faster implementation constructs the lattice directly in get_grid method. In\n",
      "the initial development, we used the composition and then made the get_grid implemen-\n",
      "tation as a faster option. We also customize the forward method to assign appropriate\n",
      "scores to the arcs corresponding to special tokens, as described below.\n",
      "For all losses, we add unit tests (in tests directory) to make the sanity check for the\n",
      "following:\n",
      "• Graphs produced by composition of the temporal and unit schemas are equivalent\n",
      "to the graph produced by get_grid method.\n",
      "• When the weight of special arcs is −∞, this is the equivalent of removing such\n",
      "arcs from a computational graph ( e−∞= 0, such a transition does not contribute to\n",
      "loss computation); and the loss should be equivalent to original RNN-T loss. This\n",
      "is tested by comparing the loss value and gradient based on random input for the\n",
      "proposed loss and etalon RNN-T implementation.\n",
      "The graph construction is debugged visually in the Jupyter Notebook [40] using auto-\n",
      "matic visualization from the k2 [14] library with GraphViz package [41].\n",
      "4.4.1 Star-Transducer (Star-T)\n",
      "We propose a simple but effective modification of the RNN-T loss computational graph\n",
      "to solve the problem of deletions. Star Transducer takes into account, along with the\n",
      "alignments with blank labels, the sequences when the blank label is substituted with a\n",
      "special \"skip frame\" ⟨sf⟩symbol, which can be viewed as an allowance to skip frames\n",
      "produced by the encoder in training time. This approach is similar to the \"*\" token used\n",
      "in Star Temporal Classification [16] loss for CTC. For such frames, the transcription is\n",
      "missing in the ground truth, and the core idea was to allow skipping such frames when\n",
      "considering all possible alignments for loss computation. We add parallel arcs to those\n",
      "with⟨b⟩label to achieve this, as shown in 4c. Unlike other arcs, the weight for this arc is a\n",
      "hyperparameter and assigned directly after populating the lattice with other weights.\n",
      "The Star-Transducer loss is implemented in the GraphStarTransducerLoss class.\n",
      "4.4.2 Bypass-Transducer (Bypass-T)\n",
      "For dealing with insertions, we propose a modification of the RNN-T computational graph,\n",
      "adding arcs with a special \"skip token\" ⟨st⟩symbol, inspired by Bypass Temporal Classifi-\n",
      "cation [17] approach. These arcs are parallel to the arcs with tokens. This means that the\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications, including Star-Transducer for deletions and Bypass-Transducer for insertions. The project also proposes a Target-Robust-Transducer as a universal loss for handling various types of errors in transcripts. The motivation behind the project includes the challenges of obtaining well-transcribed datasets and the benefits of working with weakly supervised setups. The project reviews related work on ASR systems, with a focus on RNN-Transducer systems due to their suitability for streaming and widespread use in production. The project utilizes the Fast Conformer architecture for the encoder and explores various loss modifications such as W-CTC, STC, and BTC. State-of-the-art tools and frameworks like NeMo and PyTorch are used for experiments, with a focus on modifying losses and prediction networks. Additionally, the project considers stateless RNN-T models and hybrid CTC-Transducer architectures to improve system performance in weakly supervised setups. The project also includes a detailed implementation overview, data preprocessing steps, model training pipeline, and proposed loss modifications for improving system performance in handling errors in transcripts.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "loss can consider alignments where some tokens are skipped. Fig. 5 shows the temporal\n",
      "and unit schemas and the full constructed lattice.\n",
      "The Bypass-Transducer loss is implemented in the GraphBypassTransducerLoss class.\n",
      "In our experiments, we found that assigning constant weight similar to the Star-Transducer\n",
      "approach does not work. With small absolute values (e.g, 0or−3) the model is prone\n",
      "to produce deletions, and the system behaves worse than the original RNN-T. With high\n",
      "17(a) Star-Transducer Unit Schema. Labels:\n",
      "(text_unit, unit _position ) :text_unit\n",
      "(b) Star-Transducer Time Schema. Labels:\n",
      "text_unit :frame _number\n",
      "(c) Star-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\n",
      "Figure 4: WFSTs for Star-Transducer. ⟨sf⟩is a special symbol indicating skipping the\n",
      "frame.\n",
      "absolute weight values (e.g., −20), such transitions do not contribute to the loss compu-\n",
      "tation: e−20is close to zero, and the loss is close to the original RNN-T. Similar to the ap-\n",
      "proaches applied in the paper about BTC [17], we apply a schedule to the penalty weight\n",
      "(skip_token _penalty ), combined with the probability derived from the output of the Joint\n",
      "network. For the probability we considered different options ( skip_token _mode parameter\n",
      "in the implementation):\n",
      "• \"constant\": only penalty constant, similar to one used in the Star-Transducer loss.\n",
      "• \"mean\": mean probability for all labels (in log scale) excluding blank, similar to\n",
      "BTC [17].\n",
      "• \"max\": maximum log-probability for all labels excluding blank.\n",
      "• \"maxexcl\": maximum of the log probabilities of all labels excluding blank and ground\n",
      "truth labels.\n",
      "• \"sumexcl\": logarithm of the sum of the probabilities (in log scale) of all labels exclud-\n",
      "ing blank and ground truth labels.\n",
      "We found that the \"mean\" and \"max\" options were not better than the original RNN-T\n",
      "loss. The \"maxexcl\" option was the first working solution used in the Preliminary Report.\n",
      "The intuition behind the \"sumexcl\" option is to assign the \"unused\" probability of outputs.\n",
      "18(a) Bypass-Transducer Unit Schema. Labels:\n",
      "(text_unit, unit _position ) :text_unit\n",
      "(b) Bypass-Transducer Time Schema. Labels:\n",
      "text_unit :frame _number\n",
      "(c) Bypass-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\n",
      "Figure 5: WFSTs for Bypass-Transducer. ⟨st⟩is a special symbol indicating skipping the\n",
      "token.\n",
      "The \"sumexcl\" option allows for the alignments to be considered when the network outputs\n",
      "a high probability for any token other than the target as \"appropriate.\" We found that the\n",
      "\"sumexcl\" option outperforms other cases, as discussed further in Section 5.4.\n",
      "4.4.3 Target-Robust-Transducer (TRT)\n",
      "Target-Robust-Transducer loss is a combination of Star-Transducer and Bypass-Transducer.\n",
      "We add both types of arcs that allow skipping frames and tokens, as shown in Fig. 6. It\n",
      "is worth mentioning that assigning −∞weight for \"skip frame\" arcs makes the loss iden-\n",
      "tical to Bypass-Transducer (skipping frames is not allowed in this case), and −∞ weight\n",
      "for \"skip token\" arcs makes it similar to Star-Transducer (skipping tokens is not allowed).\n",
      "This makes this loss a universal replacement for the previous two modifications (but the\n",
      "system makes more computations since the arcs are still present, even with −∞weight).\n",
      "We also test this behavior in unit tests.\n",
      "The Target-Robust-Transducer loss is implemented in the class\n",
      "GraphTargetRobustTransducerLoss . The implementation combines hyperparameters and\n",
      "code for GraphStarTransducerLoss and GraphBypassTransducerLoss .\n",
      "19(a) Target-Robust-Transducer Unit Schema. Labels:\n",
      "(text_unit, unit _position ) :text_unit\n",
      "(b) Target-Robust-Transducer Time Schema. Labels:\n",
      "text_unit :frame _number\n",
      "(c) Target-Robust-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\n",
      "Figure 6: WFSTs for Target-Robust-Transducer. ⟨sf⟩is a special symbol indicating skip-\n",
      "ping the frame. ⟨st⟩is a special symbol indicating skipping the token.\n",
      "Table 1: Baseline on LibriSpeech, WER [%].\n",
      "Sourcedev test\n",
      "clean other clean other\n",
      "NeMo 2.0 5.0 2.2 5.0\n",
      "Ours (200 epochs) 2.1 4.9 2.2 5.1\n",
      "Ours (60 epochs) 2.6 6.8 2.8 6.8\n",
      "Ours (100 epochs) 2.4 5.9 2.5 6.0\n",
      "5 Evaluation\n",
      "5.1 Baseline\n",
      "The results for our implementation are shown in Table 1. For comparison, we use a\n",
      "publicly available Fast Conformer checkpoint6trained on LibriSpeech data for 200 epochs.\n",
      "Our implementation provides results comparable to those of the state-of-the-art pipeline.\n",
      "6https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_fastconformer_transducer_large_ls\n",
      "20Table 2: Training RNN-T on data with errors, Fast Conformer, 60 epochs, WER [%].\n",
      "Type Corrupt %dev test WERD ↓\n",
      "clean other clean other\n",
      "– 2.6 6.8 2.8 6.8\n",
      "DEL 20% 4.3 9.9 4.7 10.3 3.5\n",
      "DEL 50% 79.2 81.7 80.3 81.4 74.6\n",
      "SUB 20% 4.0 9.4 3.9 9.7 2.9\n",
      "SUB 50% 11.5 23.2 11.2 23.8 17.0\n",
      "INS 20% 4.0 10.3 4.2 10.2 3.4\n",
      "INS 50% 5.1 12.7 5.3 13.5 6.7\n",
      "So we can proceed further and investigate the system behavior of corrupted targets.\n",
      "Additionally, we show the results for 60 and 100 epochs (also using the best checkpoint\n",
      "selected on dev-other for these epochs): to save computational resources, we evaluate\n",
      "different cases, training the models for 60 epochs, and for the final case with arbitrary\n",
      "errors we train the system for 100 epochs.\n",
      "5.2 Error Impact Exploration\n",
      "To explore the training pipeline on partially incorrect transcripts, we generate additional\n",
      "training data sets by mutating the original training texts with the mutation probability pm\n",
      "of 20% and 50%. We are randomly removing words for the \"deletions\" case. We use\n",
      "randomly selected words from the training vocabulary for substitutions and insertions,\n",
      "substituting/inserting words with the probability pm.\n",
      "Table 2 shows the training results on corrupted transcripts. With a small amount of\n",
      "corruption, all cases lead to system degradation, but the difference between cases is\n",
      "tiny (from 2.9% to 3.5% absolute WER degradation on test-other). We found that the\n",
      "deletions are most disruptive for the high corruption rate of 50%, and the ASR system\n",
      "can not achieve a reasonable quality (81.4% WER on test-other compared to 6.8% on\n",
      "original data). Thus, we prioritized the work with this part of the problem. Substitutions\n",
      "are the next hard case for RNN-T, which is the opposite of observations for the behavior\n",
      "of CTC systems in [18].\n",
      "5.3 Dealing with Deletions: Star Transducer\n",
      "For the setup when the ground truth transcripts contain deletions, we apply Star-Transducer\n",
      "loss as a drop-in replacement for the RNN-T loss. The results of training the model are\n",
      "shown in Table 3. In both scenarios, we can close the gap between the baseline for more\n",
      "than 70%: 77.1% WERDR for 20% deletions and 94.4% for 50%. We found that the train-\n",
      "ing is stable even without penalty, but applying the small constant penalty for the \"skip\n",
      "frame\" transition ( −0.5) improves the quality when the number of deletions is low. We\n",
      "were surprised that such a simple solution works and that modifying the autoregressive\n",
      "prediction network is unnecessary. This can mean that the encoder and joint are more\n",
      "sensitive to incorrect transcripts than the prediction network.\n",
      "21Table 3: Star-Transducer (Star-T) Loss for deletions, Fast Conformer, 60 epochs, WER\n",
      "[%].\n",
      "LossSkipDEL %dev test WERD ↓WERDR ↑\n",
      "Weight clean other clean other\n",
      "RNN-T - – 2.6 6.8 2.8 6.8\n",
      "RNN-T - 20% 4.3 9.9 4.7 10.3 3.5\n",
      "Star-T 0 20% 3.9 8.2 4.3 8.5 1.7 51.4%\n",
      "Star-T -0.5 20% 3.1 7.5 3.4 7.6 0.8 77.1%\n",
      "RNN-T - 50% 79.2 81.7 80.3 81.4 74.6\n",
      "Star-T 0 50% 5.1 10.6 5.2 11.0 4.2 94.4%\n",
      "Star-T -0.5 50% 5.4 12.4 5.9 12.5 5.7 92.4%\n",
      "Table 4: Bypass-Transducer (Bypass-T) Loss for insertions, Fast Conformer, 60 epochs,\n",
      "WER [%].\n",
      "LossSkipINSdev test WERD ↓WERDR ↑\n",
      "Weight clean other clean other\n",
      "RNN-T - – 2.6 6.8 2.8 6.8\n",
      "RNN-T – 20% 4.0 10.3 4.2 10.2 3.4\n",
      "Bypass-T -6 20% 3.0 7.5 3.3 7.9 1.1 67.6%\n",
      "RNN-T – 50% 5.1 12.7 5.3 13.5 6.7\n",
      "Bypass-T -6 50% 3.9 10.3 4.3 10.5 3.7 44.8%\n",
      "Bypass-T -5 50% 3.6 9.2 4.0 9.4 2.6 61.2%\n",
      "5.4 Dealing with Insertions: Bypass Transducer\n",
      "For insertions case, we apply the Bypass-Transducer loss described in Section 4.4.2. The\n",
      "results are shown in the Table 4. The transition weight for the \"skip token\" arcs is a sum of\n",
      "the constant weight and the total log-probability of all outputs excluding blank and target\n",
      "(\"sumexcl\" option), as discussed in the Section 4.4.2. The training starts with a constant\n",
      "weight of −20.0and is adjusted with the decay after each epoch (starting 3rd epoch):\n",
      "weight next =min (max_weight, weight ∗decay ). We use decay = 0.9for all experiments.\n",
      "Table 4 also reports the maximum constant penalty applied in training. The proposed loss\n",
      "can restore more than 60% of the system quality for the texts with insertions. Further\n",
      "evaluation of the \"sumexcl\" and \"maxexcl\" options for assigning the weight can be found\n",
      "in Appendix B.\n",
      "5.5 Dealing with Substitutions: Target-Robust-Transducer\n",
      "We apply Target-Robust-Transducer for the case with substitutions since any \"substitu-\n",
      "tion\" can be viewed as a combination of \"deletion\" and \"insertion.\" The results are shown\n",
      "in Table 5. In the preliminary experiments, we found that assigning low absolute values\n",
      "for weights ( 0for skip frame as in Star-Transducer, and −5or−6for skip token as in\n",
      "22Table 5: Target Robust Transducer (TRT) Loss for substitutions, Fast Conformer, 60\n",
      "epochs, WER [%].\n",
      "LossSkipSUBdev test WERD ↓WERDR ↑\n",
      "token,frame clean other clean other\n",
      "RNN-T - – 2.6 6.8 2.8 6.8\n",
      "RNN-T - 20% 4.0 9.4 3.9 9.7 2.9\n",
      "TRT -8,-0.5 20% 3.4 8.2 3.8 8.5 1.7 41.4%\n",
      "RNN-T - 50% 11.5 23.2 11.2 23.8 17.0\n",
      "TRT -8,-0.5 50% 8.2 16.3 8.5 17.0 10.2 45.3%\n",
      "TRT -8,-1 50% 6.9 16.0 7.1 15.8 9.0 47.1%\n",
      "Table 6: Target Robust Transducer (TRT) Loss for arbitrary errors, Fast Conformer, 60\n",
      "and 100 epochs, WER [%]. 50% of data is corrupted, using 15% for each class of errors.\n",
      "Loss Epochs ERRdev test WERD ↓WERDR ↑\n",
      "clean other clean other\n",
      "RNN-T 60 - 2.6 6.8 2.8 6.8\n",
      "RNN-T 60 50% 4.2 10.2 4.3 10.1 3.3\n",
      "TRT 60 50% 3.3 8.0 3.6 8.4 1.6 51.5%\n",
      "RNN-T 100 – 2.4 5.9 2.5 6.0\n",
      "RNN-T 100 50% 3.5 9.4 3.8 9.5 3.5\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications, including Star-Transducer for deletions and Bypass-Transducer for insertions. Additionally, the project introduces a Target-Robust-Transducer as a universal loss for handling various types of errors in transcripts. The motivation behind the project includes the challenges of obtaining well-transcribed datasets and the benefits of working with weakly supervised setups. The project reviews related work on ASR systems, with a focus on RNN-Transducer systems and utilizes the Fast Conformer architecture for the encoder. Various loss modifications such as W-CTC, STC, and BTC are explored, and state-of-the-art tools and frameworks like NeMo and PyTorch are used for experiments. The project also considers stateless RNN-T models and hybrid CTC-Transducer architectures to improve system performance in weakly supervised setups. The implementation overview includes data preprocessing steps, model training pipeline, and proposed loss modifications for improving system performance in handling errors in transcripts. Further exploration includes the impact of errors on system performance, with specific focus on deletions, insertions, and substitutions, and the application of Star-Transducer, Bypass-Transducer, and Target-Robust-Transducer losses to address these issues. The results show promising improvements in system performance, especially in handling deletions and insertions, showcasing the effectiveness of the proposed modifications in enhancing the robustness of the ASR system.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "TRT 100 50% 2.9 7.0 3.2 7.0 1.0 71.4%\n",
      "Bypass-Transducer) results in fast model overfitting, but when the penalty is more sig-\n",
      "nificant, the training is stable. Since the loss can skip both frames and tokens, apply-\n",
      "ing a more significant penalty is reasonable. We use −8for skip frame penalty and the\n",
      "\"sumexcl\" option for assigning the weight, which we found the best when experimenting\n",
      "with Bypass-Transducer, along with the penalty schedule, as discussed above. With the\n",
      "proposed loss, we can restore more than 40% of the system degradation on test-other :\n",
      "we achieve WERDR of 41.4% for 20% substitutions and 47.1% for 50%.\n",
      "5.6 Arbitrary Errors\n",
      "For evaluating the system trained with Target-Robust-Transducer loss, we construct the\n",
      "extra training data by corrupting only 50% of all utterances. For each corrupted utterance,\n",
      "we apply random substitutions, insertions, and deletions with probability for each type of\n",
      "15%. We consider such case closer to the actual conditions used for production systems\n",
      "when the well-curated datasets are mixed with unreliable data from different sources.\n",
      "Also, we train the system for longer (100 epochs). As shown in Table 6, we are able to re-\n",
      "store more than 51% system quality when the system is trained for 60 epochs (compared\n",
      "to RNN-T baseline also trained for 60 epochs). When trained for an extra 40 epochs,\n",
      "the system can restore more than 71% quality (WERDR 71.4%). In the Appendix A, we\n",
      "23also publish the learning curves for the system demonstrating its effectiveness in reducing\n",
      "substitutions and deletions on the dev-clean data.\n",
      "6 Conclusion\n",
      "In our project, we trained speech recognition neural systems on the LibriSpeech [2]\n",
      "dataset. We explored the system’s robustness to errors in target texts by artificially cor-\n",
      "rupting the ground truth target texts from the dataset. We also explored different RNN-T\n",
      "loss modifications to solve the problem of quality degradation in the discussed scenarios\n",
      "and proposed three losses:\n",
      "•Star-Transducer , which mitigates the effect of missing words in transcripts and is\n",
      "able to restore more than 90% of the system quality in such case\n",
      "•Bypass-Transducer , which allows insertions (extra words) in the transcripts and\n",
      "allows the restoration of more than 60% of the quality in such cases compared to\n",
      "\"clean\" transcripts\n",
      "•Target-Robust-Transducer , which combines the approaches applied in the previ-\n",
      "ous two losses. This loss can deal with arbitrary types of errors. It improves the\n",
      "system’s quality when some words of transcripts are incorrect (substitutions), miti-\n",
      "gating more than 40% of the quality loss for this case. For arbitrary types of errors,\n",
      "we also show that it can restore more than 70% of the quality compared to the\n",
      "baseline with the well-transcribed data.\n",
      "Our work is based on the previous solutions for CTC loss [16, 17, 18] and Graph-RNN-\n",
      "T framework [1], and proposes a novel valuable solution for RNN-Transducer-based ASR\n",
      "systems. We demonstrated the effectiveness of the losses using the Fast Conformer [12]\n",
      "model.\n",
      "The proposed Target-Robust-Transducer system can be applied in real-world scenar-\n",
      "ios when training models on a large amount of data from unreliable sources that usually\n",
      "contain transcription errors.\n",
      "We also see direct applications for Star-Transducer beyond the discussed case with\n",
      "missing words in transcripts. Modern ASR systems are trained not only to provide tran-\n",
      "scription (in words) but also to provide punctuation, e.g., Whisper [42]. Since many cu-\n",
      "rated ASR corpora do not contain punctuation (e.g., LibriSpeech [2] which we use in our\n",
      "work), such missing punctuation can be viewed as \"deletions\" in the transcripts, and with\n",
      "Star-Transducer loss the model can be trained directly on a mixture of datasets with and\n",
      "without punctuation.\n",
      "In further work, we plan to investigate other models (e.g., Conformer [9] with 4x sub-\n",
      "sampling) and datasets. We also plan to apply the losses to train ASR systems on a large\n",
      "scale for production usage.\n",
      "The losses are planned to be proposed to the open-source NeMo [31] framework.\n",
      "7 Report Parameters and Additional Notes\n",
      "The implementation is published in the GitHub repository: https://github.com/artbataev/\n",
      "uol_final .\n",
      "The report contains 6 tables and 6 figures. The appendix contains an additional 1\n",
      "table and 1 figure. We comply with word limits for each section.\n",
      "248 Acknowledgments\n",
      "I want to express my gratitude to my employer, NVIDIA Corporation, for providing compu-\n",
      "tational resources for this research.\n",
      "References\n",
      "[1] A. Laptev, V. Bataev, I. Gitman, and B. Ginsburg, “Powerful and extensible wfst frame-\n",
      "work for rnn-transducer losses,” in ICASSP 2023 - 2023 IEEE International Confer-\n",
      "ence on Acoustics, Speech and Signal Processing (ICASSP) , 2023.\n",
      "[2] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: an ASR corpus\n",
      "based on public domain audio books,” in ICASSP , 2015.\n",
      "[3] A. Graves, “Sequence transduction with Recurrent Neural Networks,” in ICML: work-\n",
      "shop on representation learning , 2012.\n",
      "[4] J. Li, “Recent advances in end-to-end automatic speech recognition,” APSIPA Trans-\n",
      "actions on Signal and Information Processing , vol. 11, no. 1, 2022.\n",
      "[5] A. Graves, S. Fernández, F . Gomez, and J. Schmidhuber, “Connectionist temporal\n",
      "classification: labelling unsegmented sequence data with recurrent neural networks,”\n",
      "inProceedings of the 23rd international conference on Machine learning , 2006, pp.\n",
      "369–376.\n",
      "[6] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network\n",
      "for large vocabulary conversational speech recognition,” in 2016 IEEE international\n",
      "conference on acoustics, speech and signal processing (ICASSP) . IEEE, 2016, pp.\n",
      "4960–4964.\n",
      "[7] R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schluter, and S. Watanabe, “End-to-end\n",
      "speech recognition: A survey,” IEEE/ACM Transactions on Audio, Speech, and Lan-\n",
      "guage Processing , vol. 32, pp. 325–351, 2023.\n",
      "[8] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “Rnn-transducer with\n",
      "stateless prediction network,” in ICASSP 2020 - 2020 IEEE International Conference\n",
      "on Acoustics, Speech and Signal Processing (ICASSP) , 2020, pp. 7049–7053.\n",
      "[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han, S. Wang, Z. Zhang,\n",
      "Y . Wu, and R. Pang, “Conformer: Convolution-augmented Transformer for speech\n",
      "recognition,” in Interspeech , 2020.\n",
      "[10] Huggingface: Open ASR leaderboard. [Online]. Available: https://huggingface.co/\n",
      "spaces/hf-audio/open_asr_leaderboard\n",
      "[11] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\n",
      "and I. Polosukhin, “Attention is all you need,” in Neural Information Processing Sys-\n",
      "tems , 2017.\n",
      "[12] D. Rekesh, N. R. Koluguri, S. Kriman, S. Majumdar, V. Noroozi, H. Huang,\n",
      "O. Hrinchuk, K. Puvvada, A. Kumar, J. Balam et al. , “Fast conformer with linearly\n",
      "scalable attention for efficient speech recognition,” in 2023 IEEE Automatic Speech\n",
      "Recognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1–8.\n",
      "25[13] A. Laptev, S. Majumdar, and B. Ginsburg, “Ctc variations through new wfst topolo-\n",
      "gies,” in Interspeech , 2021.\n",
      "[14] D. Povey, P . ˙Zelasko, and S. Khudanpur, “Speech recognition with next-generation\n",
      "Kaldi (k2, Lhotse, Icefall),” Interspeech: tutorials , 2021.\n",
      "[15] X. Cai, J. Yuan, Y . Bian, G. Xun, J. Huang, and K. Church, “W-CTC: a connectionist\n",
      "temporal classification loss with wild cards,” in ICLR , 2022.\n",
      "[16] V. Pratap, A. Hannun, G. Synnaeve, and R. Collobert, “Star Temporal Classification:\n",
      "Sequence classification with partially labeled data,” in NeurIPS , 2022.\n",
      "[17] D. Gao, M. Wiesner, H. Xu, L. P . Garcia, D. Povey, and S. Khudanpur, “Bypass Tem-\n",
      "poral Classification: Weakly Supervised Automatic Speech Recognition with Imper-\n",
      "fect Transcripts,” in Proc. INTERSPEECH 2023 , 2023, pp. 924–928.\n",
      "[18] D. Gao, H. Xu, D. Raj, L. P . G. Perera, D. Povey, and S. Khudanpur, “Learning\n",
      "from flawed data: Weakly supervised automatic speech recognition,” arXiv preprint\n",
      "arXiv:2309.15796 , 2023.\n",
      "[19] J. S. Garofolo, “Timit acoustic phonetic continuous speech corpus,” Linguistic Data\n",
      "Consortium, 1993 , 1993.\n",
      "[20] A. Graves and J. Schmidhuber, “Framewise phoneme classification with bidirectional\n",
      "lstm and other neural network architectures,” Neural networks , vol. 18, no. 5-6, pp.\n",
      "602–610, 2005.\n",
      "[21] A. Y . Hannun, V. Pratap, J. Kahn, and W.-N. Hsu, “Differentiable weighted finite-state\n",
      "transducers,” ArXiv , vol. abs/2010.01003, 2020.\n",
      "[22] Librivox: Free public domain audiobooks. [Online]. Available: https://librivox.org\n",
      "[23] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Álvarez, D. Zhao, D. Rybach,\n",
      "A. Kannan, Y . Wu, R. Pang, Q. Liang, D. Bhatia, Y . Shangguan, B. Li, G. Pundak,\n",
      "K. C. Sim, T. Bagby, S. yiin Chang, K. Rao, and A. Gruenstein, “Streaming end-to-\n",
      "end speech recognition for mobile devices,” ICASSP 2019 - 2019 IEEE International\n",
      "Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 6381–6385,\n",
      "2018.\n",
      "[24] E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid autoregressive transducer\n",
      "(hat),” ICASSP , 2020.\n",
      "[25] Z. Meng, T. Chen, R. Prabhavalkar, Y . Zhang, G. Wang, K. Audhkhasi, J. Emond,\n",
      "T. Strohman, B. Ramabhadran, W. R. Huang, E. Variani, Y . Huang, and P . J. Moreno,\n",
      "“Modular hybrid autoregressive transducer,” SLT, 2022.\n",
      "[26] Z. Tian, J. Yi, Y . Bai, J. Tao, S. Zhang, and Z. Wen, “Fsr: Accelerating the infer-\n",
      "ence process of transducer-based models by applying fast-skip regularization,” arXiv\n",
      "preprint arXiv:2104.02882 , 2021.\n",
      "[27] Y . Wang, Z. Chen, C. yong Zheng, Y . Zhang, W. Han, and P . Haghani, “Accelerating\n",
      "rnn-t training and inference using ctc guidance,” ICASSP 2023 - 2023 IEEE Interna-\n",
      "tional Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 1–5,\n",
      "2022.\n",
      "26[28] Word Error Rate, “Word error rate — Wikipedia, the free encyclopedia,” 2023,\n",
      "[Online; accessed 2024-01-05]. [Online]. Available: https://en.wikipedia.org/wiki/\n",
      "Word_error_rate\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYour job is to produce a final summary.\n",
      "We have provided an existing summary up to a certain point: The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications, including Star-Transducer for deletions and Bypass-Transducer for insertions. Additionally, the project introduces a Target-Robust-Transducer as a universal loss for handling various types of errors in transcripts. The motivation behind the project includes the challenges of obtaining well-transcribed datasets and the benefits of working with weakly supervised setups. The project reviews related work on ASR systems, with a focus on RNN-Transducer systems and utilizes the Fast Conformer architecture for the encoder. Various loss modifications such as W-CTC, STC, and BTC are explored, and state-of-the-art tools and frameworks like NeMo and PyTorch are used for experiments. The project also considers stateless RNN-T models and hybrid CTC-Transducer architectures to improve system performance in weakly supervised setups. The implementation overview includes data preprocessing steps, model training pipeline, and proposed loss modifications for improving system performance in handling errors in transcripts. Further exploration includes the impact of errors on system performance, with specific focus on deletions, insertions, and substitutions, and the application of Star-Transducer, Bypass-Transducer, and Target-Robust-Transducer losses to address these issues. The results show promising improvements in system performance, especially in handling deletions and insertions, showcasing the effectiveness of the proposed modifications in enhancing the robustness of the ASR system. The project also introduces new loss modifications such as Star-Transducer, Bypass-Transducer, and Target-Robust-Transducer to address specific error types and improve system performance. The proposed Target-Robust-Transducer system shows significant improvements in handling arbitrary errors and can be applied in real-world scenarios with unreliable data sources. Future work includes exploring other models and datasets, as well as applying the proposed losses to train ASR systems on a larger scale for production usage. The project's implementation details and additional resources can be found in the provided GitHub repository.\n",
      "We have the opportunity to refine the existing summary (only if needed) with some more context below.\n",
      "------------\n",
      "Word_error_rate\n",
      "[29] Levenshtein Distance, “Levenshtein distance — Wikipedia, the free encyclopedia,”\n",
      "2023, [Online; accessed 2024-01-05]. [Online]. Available: https://en.wikipedia.org/\n",
      "wiki/Levenshtein_distance\n",
      "[30] F . Chollet, “Xception: Deep learning with depthwise separable convolutions,” 2017\n",
      "IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1800–\n",
      "1807, 2016.\n",
      "[31] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman,\n",
      "S. Beliaev, V. Lavrukhin, J. Cook, P . Castonguay, M. Popova, J. Huang, and\n",
      "J. Cohen, “NeMo: a toolkit for building AI applications using neural modules,”\n",
      "arXiv:1909.09577 , 2019.\n",
      "[32] W. Falcon and The PyTorch Lightning team, “PyTorch Lightning,” Mar. 2019.\n",
      "[Online]. Available: https://github.com/Lightning-AI/lightning\n",
      "[33] Weights & biases: The developer-first mlops platform. [Online]. Available:\n",
      "https://wandb.ai/\n",
      "[34] A. Paszke, S. Gross, F . Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\n",
      "N. Gimelshein, L. Antiga et al. , “Pytorch: An imperative style, high-performance deep\n",
      "learning library,” Advances in neural information processing systems , vol. 32, 2019.\n",
      "[35] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le,\n",
      "“SpecAugment: A simple data augmentation method for automatic speech recogni-\n",
      "tion,” Interspeech , 2019.\n",
      "[36] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words\n",
      "with subword units,” in Proceedings of the 54th Annual Meeting of the Association\n",
      "for Computational Linguistics , 2016.\n",
      "[37] T. Kudo and J. Richardson, “SentencePiece: A simple and language independent\n",
      "subword tokenizer and detokenizer for neural text processing,” in Proceedings\n",
      "of the 2018 Conference on Empirical Methods in Natural Language Processing:\n",
      "System Demonstrations , E. Blanco and W. Lu, Eds. Brussels, Belgium:\n",
      "Association for Computational Linguistics, Nov. 2018, pp. 66–71. [Online]. Available:\n",
      "https://aclanthology.org/D18-2012\n",
      "[38] I. Loshchilov and F . Hutter, “Decoupled weight decay regularization,” in ICLR , 2019.\n",
      "[39] ——, “SGDR: Stochastic gradient descent with warm restarts,” in ICLR , 2017.\n",
      "[40] T. Kluyver, B. Ragan-Kelley, F . Pérez, B. Granger, M. Bussonnier, J. Frederic, K. Kel-\n",
      "ley, J. Hamrick, J. Grout, S. Corlay, P . Ivanov, D. Avila, S. Abdalla, and C. Willing,\n",
      "“Jupyter notebooks – a publishing format for reproducible computational workflows,”\n",
      "inPositioning and Power in Academic Publishing: Players, Agents and Agendas ,\n",
      "F . Loizides and B. Schmidt, Eds. IOS Press, 2016, pp. 87 – 90.\n",
      "27[41] J. Ellson, E. Gansner, L. Koutsofios, S. C. North, and G. Woodhull, “Graphviz— open\n",
      "source graph drawing tools,” in Graph Drawing , P . Mutzel, M. Jünger, and S. Leipert,\n",
      "Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2002, pp. 483–484.\n",
      "[42] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust\n",
      "speech recognition via large-scale weak supervision,” in International Conference on\n",
      "Machine Learning . PMLR, 2023, pp. 28 492–28 518.\n",
      "28Appendices\n",
      "A Arbitrary Errors - Learning Curve\n",
      "We provide an additional plot with the learning curve, demonstrating WER and its com-\n",
      "ponents for RNN-T and Target Robust Transducer training with arbitrary errors for 100\n",
      "epochs, as discussed in Section 5.6. We can see in Figure 7 that the number of in-\n",
      "sertions produced in all cases is similar, but the number of deletions and substitutions\n",
      "produced by the system trained on corrupted data with the TRT loss is significantly lower\n",
      "than for RNN-T and is close to the number of errors produced by the RNN-T on the original\n",
      "non-corrupted data. The screenshot is produced by WandB [33].\n",
      "Figure 7: Arbitrary errors: learning curves for RNN-T (original and corrupted data) and\n",
      "Target Robust Transducer (corrupted data).\n",
      "B Bypass-Transducer: Extended evaluation of hyperpa-\n",
      "rameters\n",
      "In this appendix section, we show the extended hyperparameter evaluation of the op-\n",
      "tions for Bypass-Transducer loss regarding assigning weights for skip token transitions.\n",
      "In initial experiments, as discussed in Section 4.4.2, we tried different options and found\n",
      "that the system is trainable only with \"maxexcl\" and \"sumexcl\" options. In the system\n",
      "exploration process, we found the \"sumexcl\" option, which considers total \"unassigned\"\n",
      "log-probability (log-probability for all outputs excluding blank and target labels) to provide\n",
      "the best value. We provide an extended version of the Table 4. The results in 7 show that\n",
      "the \"sumexcl\" option outperforms the \"maxexcl\" by a significant margin.\n",
      "29Table 7: Bypass-Transducer Loss for insertions, Fast Conformer, 60 epochs, WER [%].\n",
      "Extended evaluation.\n",
      "LossSkipINSdev test WERD ↓WERDR ↑\n",
      "Weight,Mode clean other clean other\n",
      "RNN-T - – 2.6 6.8 2.8 6.8\n",
      "RNN-T – 20% 4.0 10.3 4.2 10.2 3.4\n",
      "Bypass-T -6,maxexcl 20% 3.2 7.9 3.3 8.0 1.2 65.7%\n",
      "Bypass-T -6,sumexcl 20% 3.0 7.5 3.3 7.9 1.1 67.6%\n",
      "RNN-T – 50% 5.1 12.7 5.3 13.5 6.7\n",
      "Bypass-T -6,maxexcl 50% 4.4 10.3 4.1 10.7 3.9 41.8%\n",
      "Bypass-T -6,sumexcl 50% 3.9 10.3 4.3 10.5 3.7 44.8%\n",
      "Bypass-T -5,maxexcl 50% 4.1 10.2 4.5 10.4 3.6 46.3%\n",
      "Bypass-T -5,sumexcl 50% 3.6 9.2 4.0 9.4 2.6 61.2%\n",
      "C Losses Visualization\n",
      "We additionally publish the Jupyter Notebook, which visualizes the lattices of the pro-\n",
      "posed losses. The notebook can be found in the repository\n",
      "https://github.com/artbataev/uol_final/blob/main/notebooks/Loss_Demo.ipynb .\n",
      "30\n",
      "------------\n",
      "Given the new context, refine the original summary.\n",
      "If the context isn't useful, return the original summary.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The final project focuses on training an automatic speech recognition system on noisy targets using RNN-Transducer systems. The project aims to make the system robust to errors in training data by exploring different loss modifications, including Star-Transducer for deletions and Bypass-Transducer for insertions. Additionally, the project introduces a Target-Robust-Transducer as a universal loss for handling various types of errors in transcripts. The motivation behind the project includes the challenges of obtaining well-transcribed datasets and the benefits of working with weakly supervised setups. The project reviews related work on ASR systems, with a focus on RNN-Transducer systems and utilizes the Fast Conformer architecture for the encoder. Various loss modifications such as W-CTC, STC, and BTC are explored, and state-of-the-art tools and frameworks like NeMo and PyTorch are used for experiments. The project also considers stateless RNN-T models and hybrid CTC-Transducer architectures to improve system performance in weakly supervised setups. The implementation overview includes data preprocessing steps, model training pipeline, and proposed loss modifications for improving system performance in handling errors in transcripts. Further exploration includes the impact of errors on system performance, with specific focus on deletions, insertions, and substitutions, and the application of Star-Transducer, Bypass-Transducer, and Target-Robust-Transducer losses to address these issues. The results show promising improvements in system performance, especially in handling deletions and insertions, showcasing the effectiveness of the proposed modifications in enhancing the robustness of the ASR system. The project also introduces new loss modifications such as Star-Transducer, Bypass-Transducer, and Target-Robust-Transducer to address specific error types and improve system performance. The proposed Target-Robust-Transducer system shows significant improvements in handling arbitrary errors and can be applied in real-world scenarios with unreliable data sources. Future work includes exploring other models and datasets, as well as applying the proposed losses to train ASR systems on a larger scale for production usage. The project's implementation details, including additional resources such as the Jupyter Notebook for visualizing losses, can be found in the provided GitHub repository.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## refine chain for summarization:\n",
    "\n",
    "## take chunk 1 and send to llm and get summarization\n",
    "## take summary of chunk 1 combine with chunk 2 and send to llm and get summarization\n",
    "## take summary of chunk1, summary of chunk2 and chunk 3, combine, send to llm model \n",
    "## and so on\n",
    "\n",
    "chain = load_summarize_chain(\n",
    "    llm=llm,\n",
    "    chain_type='refine',\n",
    "    verbose=True\n",
    ")\n",
    "output_summary = chain.run(chunks)\n",
    "output_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
