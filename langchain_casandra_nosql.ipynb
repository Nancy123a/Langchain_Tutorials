{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0dcc5c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# LangChain components to use\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "# Support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
    "# you will also initialize the DB connection:\n",
    "import cassio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d73a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f55c1942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0262abe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_db_app_token= os.environ['ASTRA_DB_APPLICATION_TOKEN']\n",
    "astra_db_id=os.environ['ASTRA_DB_ID']\n",
    "\n",
    "pdfreader=PdfReader(\"documents/starrnnt.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "014fcc8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Final Project: RNN-Transducer-based\\nLosses for Speech Recognition on\\nNoisy Targets\\nVladimir Bataev\\nUniversity of London\\n10 March 2024arXiv:2504.06963v1  [eess.AS]  9 Apr 2025Contents\\n1 Project Concept and Motivation 3\\n1.1 Overview and Template . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.2 Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.3 Related Work: Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\\n1.4 Related Work: Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.5 Related Work: Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Literature Review 5\\n2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 CTC and Modifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.1 Connectionist Temporal Classification (CTC) loss . . . . . . . . . . . 5\\n2.2.2 Wild-card CTC (W-CTC) . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.3 Star Temporal Classification (STC) . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Bypass Temporal Classification (BTC) . . . . . . . . . . . . . . . . . 7\\n2.2.5 Omni-temporal Classification (OTC) . . . . . . . . . . . . . . . . . . 7\\n2.3 RNN-Transducer and Modifications . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.3.1 RNN-T loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\\n2.3.2 Graph-based RNN-Transducer framework . . . . . . . . . . . . . . . 8\\n2.3.3 Stateless RNN-T . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3.4 Hybrid CTC-Transducer models . . . . . . . . . . . . . . . . . . . . . 9\\n3 Project Design 9\\n3.1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.2 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.3 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\\n3.4 Backbone Encoder Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.5 Tools and Frameworks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\\n3.6 RNN-Transducer Details and Graph-based representation . . . . . . . . . . 11\\n3.7 Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.7.1 Initial Project Plan . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\\n3.7.2 Project Plan Reflection . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Implementation 15\\n4.1 Project Implementation Overview . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4.2 Data Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.3 Model and Training Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.4 Proposed Losses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\\n4.4.1 Star-Transducer (Star-T) . . . . . . . . . . . . . . . . . . . . . . . . . 17\\n4.4.2 Bypass-Transducer (Bypass-T) . . . . . . . . . . . . . . . . . . . . . 17\\n4.4.3 Target-Robust-Transducer (TRT) . . . . . . . . . . . . . . . . . . . . 19\\n5 Evaluation 20\\n5.1 Baseline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\\n5.2 Error Impact Exploration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\\n5.3 Dealing with Deletions: Star Transducer . . . . . . . . . . . . . . . . . . . . 21\\n5.4 Dealing with Insertions: Bypass Transducer . . . . . . . . . . . . . . . . . . 22\\n5.5 Dealing with Substitutions: Target-Robust-Transducer . . . . . . . . . . . . 22\\n5.6 Arbitrary Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n16 Conclusion 24\\n7 Report Parameters and Additional Notes 24\\n8 Acknowledgments 25\\nReferences 25\\nAppendices 29\\nA Arbitrary Errors - Learning Curve 29\\nB Bypass-Transducer: Extended evaluation of hyperparameters 29\\nC Losses Visualization 30\\nList of Figures\\n1 RNN-Transducer Schema . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n2 WFSTs for RNN-Transducer, following [1] . . . . . . . . . . . . . . . . . . . 14\\n3 Project Plan (Gantt Chart) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n4 WFSTs for Star-Transducer. ⟨sf⟩is a special symbol indicating skipping\\nthe frame. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\\n5 WFSTs for Bypass-Transducer. ⟨st⟩is a special symbol indicating skipping\\nthe token. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\\n6 WFSTs for Target-Robust-Transducer. ⟨sf⟩is a special symbol indicating\\nskipping the frame. ⟨st⟩is a special symbol indicating skipping the token. . 20\\n7 Arbitrary errors: learning curves for RNN-T (original and corrupted data)\\nand Target Robust Transducer (corrupted data). . . . . . . . . . . . . . . . . 29\\nList of Tables\\n1 Baseline on LibriSpeech, WER [%]. . . . . . . . . . . . . . . . . . . . . . . 20\\n2 Training RNN-T on data with errors, Fast Conformer, 60 epochs, WER [%]. 21\\n3 Star-Transducer (Star-T) Loss for deletions, Fast Conformer, 60 epochs,\\nWER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n4 Bypass-Transducer (Bypass-T) Loss for insertions, Fast Conformer, 60 epochs,\\nWER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\\n5 Target Robust Transducer (TRT) Loss for substitutions, Fast Conformer, 60\\nepochs, WER [%]. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n6 Target Robust Transducer (TRT) Loss for arbitrary errors, Fast Conformer,\\n60 and 100 epochs, WER [%]. 50% of data is corrupted, using 15% for\\neach class of errors. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\\n7 Bypass-Transducer Loss for insertions, Fast Conformer, 60 epochs, WER\\n[%]. Extended evaluation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\\n21 Project Concept and Motivation\\n1.1 Overview and Template\\nIn our project, we will train an automatic speech recognition (ASR) system on noisy\\ntargets. We start with the template \"CM3015 Machine Learning and Neural Networks,\\nTheme 1: Deep Learning on a public dataset,\" which describes the task of choosing a\\npublicly available dataset and training a deep learning model on it. So, we will work with a\\nneural network-based end-to-end ASR system, using LibriSpeech [2] dataset, a popular\\nacademic benchmark. We limit our task to RNN-Transducer [3] systems, which are widely\\nused in production and provide state-of-the-art quality [4] in most cases.\\nWe are going beyond the standard task and focusing our research on making RNN-\\nTransducer systems robust to noisy targets: unlike well-curated datasets, in the indus-\\ntry, the training data contains different errors due to the unreliability of the transcription\\nsources or the inability to transcribe noisy speech accurately. To solve the problem of\\ntraining on the noisy data, we will analyze the impact of different types of errors in training\\ndata on the quality of the RNN-Transducer system and explore different loss modifications\\nto overcome the problem. We will construct the artificial training data by mutating correct\\ntranscripts from the LibriSpeech [2] training part, similar to the approaches used in the\\nrelated work, and try to achieve the best possible quality on the development and test\\ndata standard for LibriSpeech.\\n1.2 Motivation\\nTraining ASR systems usually requires a large amount of well-transcribed audio-text paired\\ndata. The process of dataset preparation often calls for filtering out \"noisy\" transcriptions\\nusing some pretrained model, which results in a lower amount of available data for train-\\ning. It is hard to obtain large, well-transcribed datasets for many languages and scenarios.\\nOn the other hand, data with limited transcript quality is widely available. Developing new\\napproaches for working with weakly supervised setups can be beneficial in the following\\nways:\\n• (1) making it easier to use non-well-curated datasets;\\n• (2) improving the quality of ASR models without tricky filtering pipelines for raw data,\\nmaking an approach fully \"end-to-end,\" and potentially getting benefits from more\\nnon-filtered data;\\n• (3) improving ASR quality for low-resource languages when the data is extremely\\nlimited;\\n• (4) transfer learning scenarios when another (imperfect) model transcribes the data.\\n1.3 Related Work: Systems\\nMost of the modern ASR systems use mel filter bank features extracted from the speech\\nsignal [3] and learn to map a sequence of feature vectors to the correct sequence of the\\nunits derived from the text (e.g., characters, subword units, words, or phonemes). There\\nare three dominating types of end-to-end ASR systems [4]: Connectionist Temporal Clas-\\nsification (CTC) [5], RNN-Transducer (RNN-T) [3] and attention-based encoder-decoder\\n3(AED) [6]. CTC and RNN-Transducer rely on an explicit latent monotonic alignment be-\\ntween the audio and corresponding transcript [7]. CTC is the most straightforward non-\\nautoregressive system, which predicts each text unit independently. RNN-Transducer\\nwas introduced as a solution to fix the wholly conditional independence assumption of\\nCTC and consists of 3 parts: an Encoder, which produces the representation of input\\nfeatures non-autoregressively; autoregressive Prediction network; and a Joint network,\\nwhich combines their output and produces the final prediction [3]. Encoder-Decoder sys-\\ntems with Attention [6] implicitly learn the alignment between the audio and text via the\\nattention mechanism. We are primarily interested in RNN-Transducers since such sys-\\ntems are suitable for streaming by design, widely used in production, and power most of\\nthe state-of-the-art monolingual models [7]. Initially, recurrent neural networks (RNNs)\\nwere used as an autoregressive prediction (thus, RNN-T is named after RNNs), but other\\nnon-recurrent architectures can also be used, e.g., a simple stateless network whose\\noutput depends on a fixed number of previous outputs [8].\\n1.4 Related Work: Models\\nIn ASR, the dominating architecture for the encoder is a Conformer [9, 10, 4], which\\noriginates from a transformer block architecture [11] augmented with convolutional mod-\\nules [9]. The important part is that the original Conformer encoder subsamples the se-\\nquence of input features (derived from the audio signal) 4 times, and recently, multiple\\narchitectures were proposed to subsample the input features 8 times to provide better\\nspeed without performance drop, e.g., Fast Conformer [12]. We will use the Fast Con-\\nformer architecture as a basic architecture for our model, focusing on loss modifications\\nrather than changing the architecture.\\n1.5 Related Work: Losses\\nCTC is the loss that can naturally be represented [13] with weighted finite state transduc-\\ners (WFSTs). Due to the existence of the libraries that allow to construct differentiable\\nWFSTs and use them to train deep learning systems, e.g., k2 [14], different modifications\\nof CTC loss were proposed to solve the problem of different errors in training data. Partic-\\nularly, W-CTC (CTC with Wild Cards) [15] allows missing text at the start and the end of an\\nutterance in transcription, Star Temporal Classification (STC) [16] allows missing labels\\nanywhere, Bypass Temporal Classification (BTC) [17] solves the problem of insertions\\nand substitutions. Recently proposed Omni-temporal Classification (OTC) [18] is a gen-\\neralized loss that combines all the previous work and proposes a CTC loss modification\\nthat is robust to any type of errors in transcripts.\\nRNN-Transducer robustness to the errors in training data is still an unsolved prob-\\nlem. The recent work about a graph-based framework for RNN-Transducer [1] proposes\\na generalized solution to develop different loss modifications based on WFSTs and also\\nproposes a W-Transducer loss that can deal with missing transcripts at the start and end\\nof the transcription (similar to W-CTC [15]). Moreover, an autoregressive prediction net-\\nwork can require modifications since its input in training time is a ground truth transcription\\nand can be sensitive to incorrect text. We are planning to increase the complexity of the\\nproject gradually, starting from the \"under-transcribed\" case, when the training texts can\\ncontain missing words (deletions) similar to STC [16]. Then, we will explore more complex\\ncases with insertions and substitutions, finally providing a general combined solution.\\n42 Literature Review\\n2.1 Introduction\\nWe start our review with the work on the CTC criterion and its modifications. Since both\\nCTC and RNN-T take into account all possible alignments between features extracted\\nfrom audio and text units, thus it is possible to apply some techniques to both of them.\\nThen, we discuss the RNN-T loss and the relevant work to improve its robustness to\\nerrors, along with the structure of the Transducer models and differences between CTC\\nand RNN-T that prevent direct application of CTC-based techniques.\\n2.2 CTC and Modifications\\n2.2.1 Connectionist Temporal Classification (CTC) loss\\nThe Connectionist Temporal Classification (CTC) was originally introduced in [5] as a\\nreplacement to a \"classical\" ASR pipeline based on hidden Markov Models (HMMs) and\\nis historically the first so-called \"end-to-end\" ASR system. The original work proposes a\\nsolution for the automatic learning for the alignment between the target units (phonemes\\nin this work, using the TIMIT [19] dataset) and the representation extracted by the neural\\nnetwork from the audio signal. The system utilizes RNNs (particularly bidirectional Long\\nShort-Term Memory (BLSTMs) networks [20]) as its backbone. Mel-Frequency Cepstrum\\nCoefficients are extracted from the input audio every 10ms, which forms the input for the\\nneural network. The output of the network has a softmax activation and is interpreted\\nfor each label as a probability of observing the label at the current time frame [5], as\\nin the classification task for each frame. The vocabulary is augmented with a special\\n⟨blank⟩symbol, and the algorithm considers all possible variants of transcriptions that\\nmap to the original text units after removing duplicated predictions and the ⟨blank⟩symbol.\\nThe loss is a minus log probability of all possible correct alignments (defined by the rule\\ndescribed above) given the audio features. Thus, training maximizes the log probability for\\nthe possible alignments without requiring a forced alignment, which was used in classical\\nHMM-based systems. The paper also proposes an efficient forward-backward algorithm\\nto calculate the CTC loss and gradients and shows the system’s efficiency in predicting\\nphonemic transcriptions of utterances.\\nThe paper about CTC loss [5] forms a basis for our research since we plan to ap-\\nply techniques to modify the possible alignments in the CTC framework to the RNN-\\nTransducer system. The work shows that it is not necessary to have one perfect align-\\nment as a target, but once we construct a mapping between the ground truth target and\\nthe inner latent alignment (in the paper - sequences with repeated labels and additional\\n⟨blank⟩symbols, that are removed in decoding), we can use a full-sum training (taking\\ninto account all possible alignments), and the system can effectively solve the ASR task.\\n2.2.2 Wild-card CTC (W-CTC)\\nWild-card CTC (W-CTC) was proposed in [15] to solve the problem when the utterance\\nis partially transcribed, and the transcription can be missing at the start, at the end, or\\non both sides. The work introduces a simple but efficient modification of CTC criterion\\ncomputation, using a special \"*\" (star) symbol that can be prepended to each transcrip-\\ntion, and means that at the start of the transcription, any possible sequence (of any size,\\n5including the empty sequence) of symbols can be missing. This allows a simple modi-\\nfication of the dynamic programming algorithm for CTC computation introduced in [5] to\\nhandle the alignments not only when the ground truth is entirely correct but also start and\\nend the alignments between text units and features extracted from audio at any part of the\\naudio, but with the assumption, that the part of the audio fully matches the transcription.\\nThe authors also use a TIMIT [19] dataset, randomly masking a portion of the start/end of\\nthe utterance transcriptions, thus showing the effectiveness of the approach (compared\\nto the original CTC) on partially transcribing ASR data in a synthetic setup. Additionally,\\nthe authors validate the effectiveness of W-CTC on Optical Character Recognition (OCR)\\nand Continuous Sign Language Recognition (CSLR) tasks.\\nThe paper is interesting as the first work which introduces a synthetic setup with under-\\ntranscribed data to study and develop training criteria for ASR robust to corrupted targets.\\nThe proposed W-CTC criterion uses a larger possible amount of alignments but still con-\\nverges and surpasses CTC even when a small portion of the data is corrupted. Also,\\nthe paper shows the application of the techniques to OCR and CSLR tasks, which shows\\nthe potential impact of modifications of losses used for ASR on areas outside the speech\\nrecognition field.\\n2.2.3 Star Temporal Classification (STC)\\nStar Temporal Classification [16] was proposed as a generalization for the previous work\\nwhen the transcription is only partial, and between any pair of labels, an arbitrary number\\nof words can be missing. The work uses the \"*\" star token to represent zero or more\\ntext tokens and considers alignments between the encoded signal features and a target,\\nwhere the \"*\" token is inserted between all labels and also appended to the start and\\nthe end of the utterance. Thus, the loss allows the network to output any sequence of\\nlabels corresponding to the presented corrupted ground truth text after removing some\\nof the words (the ASR system should emit all the words from the target but can insert\\nother words between them). This approach significantly increases the number of possible\\nalignments and does not allow the training of the neural network directly. The problem is\\nsolved by introducing a penalty for the \"*\" token λ, with exponential decay during training,\\nstarting from the high values and decreasing once the network converges. The penalty\\nhyperparameter is also adjusted based on the number of missing words in transcriptions.\\nThe authors demonstrate the approach using synthetic data derived from LibriSpeech [2],\\nusing partially masked transcripts with different probabilities (up to 70%). The approach is\\npractical even when using greedy decoding, but the authors also show that decoding with\\nan n-gram language model (LM) and rescoring hypotheses with Transformer [11] based\\nLM also improves the system’s quality. For the implementation, unlike previous work, the\\nGTN [21] framework for differentiable WFSTs is used, which simplifies the development\\nof the new loss.\\nThis paper is crucial for our work since it allows us to solve the problem with deletions\\nin the transcribed texts for CTC. The critical part is that the direct solution (considering all\\npossible alignments in the loss) leads to failure, and using the adjustable penalty is crucial\\nto solve the issue. Also, the construction of LibriSpeech-based data is an important part.\\nWe will consider an analogous approach for investigating similar cases for RNN-T.\\n62.2.4 Bypass Temporal Classification (BTC)\\nBypass Temporal Classification [17] solves part of a weakly-supervised setup with unre-\\nliable transcripts: substitutions and insertions. Similar to the previous work, the authors\\npropose CTC target graph modification to allow the alignments when some text units\\nfrom the target are changed (substitutions) or not used (insertions in the text, thus allow-\\ning deletion from the alignment). Remarkably, the ground truth is represented as a linear\\nWFST, where the forward arcs contain ground truth labels, and each arc also presents\\na parallel \"bypass\" arc with a penalty, which allows to skip the token by emitting zero or\\nmore tokens. The authors also use the penalty with an exponential decay to make the\\nnetwork learn using the increased number of possible alignments. The authors evaluate\\ntheir solution on both TIMIT [19] and LibriSpeech [2] datasets, constructing the training\\ndata with substitutions and insertions separately and combining them. Interestingly, it is\\nshown that for the CTC criterion, the impact of insertions is larger than for substitutions,\\nand with 50% of insertions, it is impossible to train a system with the pure CTC criterion.\\nThis paper solves the remaining piece of the puzzle of training a CTC-based system\\nwith partially incorrect transcripts. We will also investigate similar approaches for RNN-T.\\nMoreover, this paper encouraged us to start the exploration of the problem by comparing\\nthe impact of different errors on the training criterion behavior and starting our solution\\nfrom the most disruptive problem.\\n2.2.5 Omni-temporal Classification (OTC)\\nThe Omni-temporal Classification [18] finally combines the approaches to construct a\\nuniversal loss to handle all the possible cases of errors. The authors utilize the \"bypass\"\\narcs to skip frames, along with self-loops that represent substitutions and insertions, and\\ntwo separate penalties for them following the previous work. The authors used synthetic\\ndata generated from the train-clean-100 subset of LibriSpeech [2] and also trained a sys-\\ntem on the original subsets from LibriVox [22] that were originally used to construct the\\nLibriSpeech data, using segmentation and filtering with pretrained models, as described\\nin [2]. The combination of the losses leads to significant improvements for all types of\\nerrors and allows training of the ASR system even on the original unsegmented data with\\nerrors without a cleaning pipeline.\\nThis work contains a detailed exploration of the impacts of different penalty values\\non the quality of the network, which can provide some insights for working with similar\\nalignment-based modification approaches for RNN-T. The code is published, and we can\\ntry to reproduce the setup if necessary. Moreover, we can try this criterion as a part of the\\npotential solution with hybrid CTC-RNN-T models for a \"CTC head,\" which we will discuss\\nfurther. Moreover, we found it curious that despite the beneficial impact of the loss when\\ntraining ASR system with unfiltered raw data from LibriVox, the gap still exists for the\\ncarefully filtered LibriSpeech data (e.g., authors report [18] 12.5% vs 8.2% WER on test-\\nother for these setups), which can indicate that the problem is still not fully solved even for\\nCTC criterion, and multi-stage pipeline is still essential to obtain the best performance.\\n2.3 RNN-Transducer and Modifications\\n2.3.1 RNN-T loss\\nRecurrent Neural Network Transducer (RNN-Transducer, RNN-T) [3] was developed as an\\nimprovement of the CTC [5] systems to overcome problems related to conditional inde-\\n7pendent assumption. The system contains three components: (1) transcription network,\\nwhich is usually referred to as an \"Encoder\"(e.g., [23]), that operates on the features\\nderived from audio and is similar to the one used in CTC system; (2) prediction network,\\nthat outputs the predictions based on the previously decoded symbol in inference time,\\nand utilizes a ground truth text in training time; and (3) a combination of the outputs of the\\nprevious two networks that makes final predictions, which is usually referred as a Joint\\nnetwork in the literature [23]. The prediction network is autoregressive and provides a\\ncrucial improvement for the ASR system. The system also uses a ⟨blank⟩symbol, but the\\nmeaning differs from CTC: this symbol means the system should end decoding the current\\nframe and transition to the next frame from the encoder. Unlike CTC, repeated symbols\\nare not allowed, but the advantage is that the system can predict more than one text unit\\nfor each frame. The RNN-T loss function takes into account all possible monotonic align-\\nments between encoder and prediction network outputs, which makes the system similar\\nto CTC with implicit alignment learning.\\nThe crucial difference for the RNN-T system is that the prediction network is autore-\\ngressive (usually uses LSTM as a backbone), making it significantly more challenging to\\ndeal with imperfect transcripts. Since ground truth text is used during training (so-called\\n\"teacher forcing\" algorithm), corrupted tokens can prevent the correct alignment learn-\\ning. The autoregressive nature also prevents applying CTC-based approaches directly to\\nRNN-T and may require modifications of the network itself along with the training criterion.\\n2.3.2 Graph-based RNN-Transducer framework\\nThe recent work about RNN-Transducer modifications [1]1brings the connection between\\nWFSTs and Transducer architecture, allowing representing the RNN-T loss computation\\nwith the graph, where the arc weights are taken from the Joint network output, and thus\\nthe direct application of graph algorithms, including full-sum alignment learning, are pos-\\nsible. The work presents two approaches for representing original and modified RNN-T\\ngraphs, either as a direct grid (\"Grid-Transducer\") or as a composition of acoustic and\\ntextual schemas (\"Compose-Transducer\"), which after the composition and connect oper-\\nations exactly matches the Grid-Transducer representation (\"connect\" operation removes\\nthe states that do not belong to any path from the start and the end state; it is optional\\nfor loss computation since such states do not affect the alignment probabilities since not\\nbelonging to any alignment). The composition is slower to compute but allows for the\\ndevelopment of losses using graphs that can be easily debugged visually. The authors\\nbuild the framework on top of k2 [14] library for differentiable WFSTs and show that the\\nloss computation can be as efficient as the optimized CUDA-based code. Moreover, the\\nauthors present a W-Transducer, which can solve the problem of deletions at the start\\nand the end of the utterance, similar to the W-CTC discussed above. The graph for the\\nloss uses two groups of skip-connections from the start of the time grid to all other time\\nsteps and from each time step in the grid to the last, which allows to use the alignments\\nwhere some frames at the start and the end are skipped, but the network should emit the\\nfull training text. The effectiveness of this approach was demonstrated on LibriSpeech\\ndata by randomly removing 20% and 50% of the labels from text from both sides of each\\nutterance.\\nIn our work, we will use the proposed WFST framework for training RNN-T on noisy\\ntargets to simplify the development of the losses. W-Transducer solves the easiest case\\nwhen the autoregressive prediction network does not use a corrupted input and thus\\n1Disclaimer: the author of the Final Project participated in the development of this approach and is a co-author of the paper.\\n8can remain unmodified. We are planning to work with more complex cases, and such\\nmodifications can be required along with the loss customization. We will discuss this\\napproach in more detail in Section 3.6.\\n2.3.3 Stateless RNN-T\\nThe work [8] proposes \"RNN-T with StateLess Prediction Network (RNNT -SLP),\" revising\\nthe need for recurrent networks for the prediction network part of the RNN-T system.\\nThe authors questioned the popular opinion that the prediction network behaves similarly\\nto the language model in traditional ASR systems. They tried pretraining the recurrent\\nnetwork on text-only data to predict the next symbol and initializing the prediction network\\nwith pretrained weights and found that such pretraining does not lead to any improvement.\\nMoreover, replacing the prediction network with a simple \"stateless\" module, in which\\nprediction is based only on the last symbol (which is similar to 2-gram LM), leads to a\\ncomparable overall system performance with the RNN-based system.\\nDespite there are other works that show that the prediction network can behave like a\\nlanguage model, especially in a factorized architecture [24, 25], we are interested in this\\nwork showing the possibility of using simpler networks for this part of the model. Since\\na stateless prediction network can be significantly more robust for corrupted targets, in\\nwhich prediction is dependent only on a small context, we consider this work an important\\noption for changing the prediction network architecture.\\n2.3.4 Hybrid CTC-Transducer models\\nAs a last item of our review, we will discuss the hybrid architecture, which uses both RNN-\\nT and CTC losses, proposed in [26]. The work proposes training the neural transducer\\nwith an auxiliary CTC loss on top of the encoder (as a separate \"head\"), combining the\\nsystems for further accelerating the decoding: if the CTC head predicts the blank label,\\nsuch frame can be skipped in decoding, resulting in a significant inference acceleration\\nwith tiny quality degradation. The work [27] proposes the use of such hybrid systems\\nto accelerate not only inference but also the training speed, using more lightweight CTC\\nhead prediction to compute RNN-T loss with the smaller number of possible alignments\\n(pruning RNN-T loss lattice).\\nThis work can be relevant for our research due to the discussed above OTC criterion,\\nwhich is robust to noisy targets: in a hybrid training setup, we can use the prediction of the\\nadditional head, trained with the OTC loss, to identify corrupted tokens and thus modify\\nthe input for prediction network or the loss based on this information.\\n3 Project Design\\nIn our project, we plan to investigate the behavior of the RNN-Transducer architecture in\\na weakly supervised setup when part of the data is corrupted and modify the RNN-T loss\\nto improve the system’s quality. We will consider loss modification techniques that do not\\nrequire any change in the overall neural architecture and decoding algorithms to make\\nthem compatible with current production systems.\\n93.1 Objectives\\nWe define the following objectives for our work:\\n• (1) Investigate how using partially incorrect transcripts impacts the quality of the\\nRNN-Transducer system, separating deletions, substitutions, and insertion cases\\n• (2) Investigate techniques that allow to improve the performance of the system in\\nthe conditions described above\\n• (3) Combine the solutions provided in (2) to solve the general case of training the\\nASR Transducer-based system with unreliable transcript and evaluate the final so-\\nlution when it is unknown what type of errors the data contains.\\n3.2 Metrics\\nThe key metric for assessing the quality of an ASR system is word error rate (WER) [28].\\nThe additional metric commonly used to assess the speed of the ASR system is a real-\\ntime factor (RTF), which indicates how much audio the system can process given the\\nfixed time, and usually, the tradeoff between speed and quality is important when consid-\\nering different models. In our project, we focus only on modifications that have a minor\\nimpact on the inference speed after the model is trained; thus, we report only WER in our\\nexperiment.\\nThe word error rate is a specification of a Levenshtein distance [28, 29], defined for\\ntwo sequences of words (hypothesis and ground truth), and is calculated as a number of\\nsubstitutions (SUB), insertions (INS) and deletions (DEL) divided by the number of ground\\ntruth (correct) words.\\nWER =SUB +INS +DEL\\nCORRECT\\nLower WER means that the system makes more accurate predictions. We will also use\\nthe term \"accuracy,\" usually defined as accuracy = 1−WER (higher accuracy value\\nmeans better prediction).\\nSince we are working with the conditions that show the degradation of the standard\\nASR training pipeline, we introduce two additional metrics. WER difference (WERD) in-\\ndicates the system degradation and is a difference between the WER that the system\\nachieves in a particular setup and the WER of the baseline on the non-modified (original)\\ndata.\\nWERD =WER modified _data−WER original _data\\nWe are interested in minimizing the degradation of the system, thus defining the relative\\nimprovement of the system as WERDR.\\nWERDR =WERD RNN−T−WERD Proposed\\nWERD RNN−T\\n3.3 Data\\nThe primary data for the project is a LibriSpeech [2] corpus, which consists of 3 subsets\\nfor training data (960 hours total), two development sets ( dev-clean anddev-other , 5.4 and\\n5.3 hours respectively), and two test sets ( test-clean andtest-other , 5.4 and 5.1 hours).\\nWe will use the full training part to generate artificial training data by corrupting the texts\\n10with artificial deletions, substitutions, and insertions. We will use the dev-other set for\\nvalidation during training and choosing the optimal checkpoints, and we will finally assess\\nour models on the test-other . As it is common in the ASR research, we will report WER\\nfor all development and test sets.\\n3.4 Backbone Encoder Models\\nCurrently, the dominating architecture [7] for the encoder is the Conformer [9] model. The\\noriginal Conformer-encoder processes the input sequence of vectors and produces the\\nrepresentation 4 times smaller by the time dimension (4x subsampling). For our initial\\nexperiments, we will use Fast Conformer [12] (114M parameters), which subsamples\\nthe input by the factor of 8 without accuracy degradation by using depth-wise separable\\nconvolutions [30], which allows faster training and inference.\\n3.5 Tools and Frameworks\\nWe are planning to use NeMo [31] framework for experiments, which is based on PyTorch-\\nLightning [32] for easiness of extending the models, and train them on clusters. NeMo\\nprovides stubs for ASR models and capabilities to use WandB [33] platform for experiment\\nmanagement. When required, we will use pure PyTorch [34] and k2 [14] library to modify\\nlosses and prediction network, also using a WFST framework for RNN-Transducer [1]\\nimplemented in NeMo.\\n3.6 RNN-Transducer Details and Graph-based representation\\nRNN-Trasducer [3] model schema is shown in Fig. 1. The model consists of three parts.\\nEncoder neural network transforms a sequence of input feature vectors derived from an\\naudio into a latent representation - a sequence of vectors ⟨e0, e1, ..., e t−1⟩with the length\\nofT. The number of output vectors is usually smaller than the input due to applying\\nstrided convolutions or pooling layers along the time axis, which reduces the output size\\nby a fixed factor (subsampling) and makes the model more computationally efficient. The\\nPrediction network (usually a recurrent network) in training time takes a ground truth\\nsequence of text units as an input padded with a special start-of-sequence symbol ⟨SOS⟩\\n(in practice usually ⟨blank⟩symbol is reused for this purpose) and produces a latent repre-\\nsentation – sequence of vectors ⟨p0, p1, ..., p u⟩with the length of U+ 1(Uis the number of\\ntext units in the text representation). The Joint network combines all combinations of ei\\nwithpjvectors and produces a 3-dimensional tensor for the size Tx(U+ 1)xV, where Vis\\na vocabulary size augmented with the ⟨b⟩(⟨blank⟩) symbol (in practice, the 4-dimensional\\ntensor is used due to additional batch dimension). Thus, ji,jrepresents the vector pro-\\nduced from a combination of piandpj. In practice, the Joint network is relatively simple\\nand computes the sum of vectors, non-linearity (e.g., ReLU), and a projection into the\\nspace of size V, (e.g., ji,j=Project (ReLU (ei+pi))). If the vectors eiendpjare of differ-\\nent sizes, they are firstly projected to the space with the same dimension. The Softmax\\nactivation produces a distribution over the vocabulary units, and the dynamic program-\\nming algorithm is used to compute the probability over all possible paths in the graph (in\\nthe code, all computations are produced in log scale for numerical stability purposes).\\nEach path represents a possible alignment, where blank symbols can be inserted be-\\ntween labels (e.g., for the Fig.1 1 \"C ⟨b⟩A T⟨b⟩ ⟨b⟩ ⟨b⟩\" is the possible path for the target\\n11text \"CAT\" represented as \"C,\" \"A,\" and \"T\" units). The number of ⟨b⟩labels is the num-\\nber of frames the Encoder produces. The minus log probability of all possible alignments\\nforms an RNN-T loss value.\\nForgreedy decoding , since the ground truth text is unknown, a nested loop is used:\\nfor each encoder vector, the algorithm sequentially obtains the next prediction with the\\nmaximum probability, starting from ⟨SOS⟩symbol, and computes the Prediction network\\noutput for the new decoded symbol, combining with the current encoder vector and com-\\nputing Joint network output. Once the ⟨b⟩symbol is found, this symbol is not fed into the\\nPrediction network, but the inner loop stops, and the decoding process starts for the next\\nencoder vector. More complex decoding algorithms exist, but greedy decoding is widely\\nused in practice due to the best speed and near-optimal quality in most scenarios [7], so\\nwe are focusing on this approach.\\nThe computation of the RNN-T loss can be represented with WFSTs , as shown in [1].\\nThe original work presents the approach more theoretically, and here we will discuss the\\npractical implementation, which is a basis for our work. A weighted finite state transducer\\nis a graph with states and arcs, whose arcs represent transitions from input to output\\nlabels with a corresponding weight. In k2 [14] library, it is allowed to store an arbitrary\\nnumber of labels (not only an input/output label for each arc). Thus, we use a tuple\\nof labels as input labels for some graphs in our representation 2. We can construct a\\ncomputational graph using 3 labels for each transition: output (ground truth) label, unit\\nindex (in a sequence of ground truth units), and encoder vector index, see 2c. Given these\\nlabels by using \"index select\"2operation, we can populate a lattice with the weights from\\nthe Joint network output, and apply a differentiable computation of the full-sum loss3. This\\nis a \"Grid-Transducer,\" described in [1]. To simplify the development, we can construct\\ntwo schemas (\"Compose-Transducer\"). Unit schema 2a represents text units, and each\\ntransition has a tuple of input symbols (Unit :Unit_Index )and an output symbol Unit.⟨b⟩\\nunits represent self-loops, and other units represent transitions over the ground truth text.\\nThe time schema 2b is a simple linear graph representing transition over time and self-\\nloops for each state with all the vocabulary symbols as inputs and time index as output.\\nIt is much easier to visually debug the representation with the separated schemas, and\\ntheir composition matches the final lattice 2c. We are planning to experiment with the\\nseparated schemas to make the RNN-T loss more robust to corrupted targets, but for the\\nfinal stage, we are planning to provide an efficient code for lattice construction.\\n3.7 Plan\\n3.7.1 Initial Project Plan\\nOur work initial work plan published in the \"Draft Report\" (Midterm) is shown in Fig. 3.\\n(a) Minimal RNN-T Implementation . After initial preparation, which was already done\\nbefore finishing this report (initial exploration, project proposal, literature review), we will\\nimplement a minimal codebase to experiment with RNN-T. During our exploration, we\\nfound that RNN-T models in NeMo provide a lot of functionality but are not easy to extend.\\nSo, we will implement the lightweight pipeline with the following capabilities:\\n• Lightweight Joint and Prediction networks\\n• Greedy Decoding\\n2https://pytorch.org/docs/stable/generated/torch.index_select.html\\n3https://k2-fsa.github.io/k2/python_api/api.html?highlight=get_tot_scores#k2.Fsa.get_tot_scores\\n12Figure 1: RNN-Transducer Schema\\n• Full model training pipeline\\n• Compatibility with NeMo encoders (including Conformer-based encoders as de-\\nscribed above), since we are not planning to customize the encoder part.\\nThen, we plan to train the baseline on the original LibriSpeech data with a Fast Con-\\nformer encoder to check that our model can achieve comparable quality with the native\\nimplementation of RNN-T in NeMo.\\n(b) Impact of imperfect transcripts . We will generate the data derived from Lib-\\nriSpeech with 20% and 50% of deletions, substitutions, and insertions separately (6 sets\\nin total) and train the models to study the model’s behavior in such conditions.\\n(c) \"Deletions\" case. Our preliminary studies showed that the deletions in the tran-\\nscripts are the hardest case for RNN-T. We will try to solve this case by modifying the loss\\nfunction. At this stage, we focus on improving the performance and not trying to find the\\nperfect hyperparameters and/or provide the fastest implementation for the loss function.\\n(d) \"Insertions\" and \"substitutions.\" We will explore different approaches discussed\\nin the literature review to improve the system’s performance in such conditions.\\n(e) Combined Solution . Combining the solutions (c) and (d) will allow for solving the\\nuniversal problem of partially correct transcripts. We will generate additional training data\\nfor cases with all types of errors.\\n13(a) RNN-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) RNN-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) RNN-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 2: WFSTs for RNN-Transducer, following [1]\\n(f) Conformer Medium (4x subsampling). At this stage, we will apply our solution to\\nthe Conformer Medium model to study its behavior when the encoder reduces the input 4\\ntimes.\\n(g) Fast loss implementation. This task focuses on the quality and speed of our\\ncode. We will provide a clean and fast \"ready to use\" solution for our loss and prediction\\nnetwork modifications.\\n(h) Study hyperparameters in detail. This task will allow us to get insights about the\\nhyperparameters of the systems proposed in (c)-(e).\\n3.7.2 Project Plan Reflection\\nThe sections (a)-(b) were crucial for our project but imposed minimal risks due to well-\\nexplored existing solutions. The most critical risks came from (c) and (d) cases since,\\naccording to our knowledge, no solutions existed for such tasks. To mitigate these risks,\\nwe also considered using a hybrid CTC-Transducer architecture with OTC loss, which can\\nsolve the problem, at least for the CTC head, and we can use its predictions to train the\\nTransducer part instead of corrupted ground truth labels. Combined solution (e) was a\\nkey part of finalizing our project. We considered parts (f)-(h) as a fair improvement but not\\nan essential contribution to our work and planned to focus on them only after finishing the\\nmain part.\\nIn our final stage, we elaborated a successful solution for all discussed cases, provid-\\n14Figure 3: Project Plan (Gantt Chart)\\ning a \"drop-in\" replacement for the RNN-Transducer loss for the cases without changing\\nthe model architecture at all. We also provided a fast implementation for the losses. Due\\nto a lack of computational resources and difficulties training the system within the novel\\nsetup, we focused more on exploring parameters for Bypass Transducer loss, as dis-\\ncussed in Section 5.4. We omitted experiments with Conformer Medium (f) and left them\\nfor future work.\\n4 Implementation\\nThe implementation is published in the GitHub repository :https://github.com/artbataev/\\nuol_final . In this section, we describe the implementation with the links to the origi-\\nnal files. Additional visualization of the produced lattices for all proposed losses can be\\nfound in the Jupyter Notebook https://github.com/artbataev/uol_final/blob/main/\\nnotebooks/Loss_Demo.ipynb .\\n4.1 Project Implementation Overview\\nThe main goal of the project is to provide a solution to deal with different types of er-\\nrors in the RNN-Transducer framework. We want to make our models comparable and\\ncompatible with the publicly available state-of-the-art models and want further to propose\\nsolutions to the NeMo [31] framework. So, we reuse components from NeMo, focusing\\non customization and modifications of the necessary parts.\\nFirstly, we make a minimal necessary code of RNN-T for our experiments, providing\\nimplementation containing the RNN-T model and customizable Joint and Prediction net-\\nworks. We reuse Conformer [9] blocks from NeMo [31] for the Encoder network.\\nThe Prediction network is a 1-layer LSTM with 640 hidden units, implemented\\ninMinPredictionNetwork class. The Joint network, as discussed in Section 3.6, applies\\n15two projections of the output of Encoder and Prediction networks (2 linear layers) to the\\ndimension of 640, sums the vectors, applies ReLU non-linearity, and projects the out-\\nput (one more linear layer) into 1025-dimensional space (1024 BPE units and ⟨b⟩). It is\\nimplemented in MinJoint class.\\nWe also implement a greedy decoding algorithm for evaluation in min_rnnt/decoding.py .\\nIn our experimental setup, we are following the Fast Conformer [12] training pipeline4.\\nThe encoder has 108.7M parameters, prediction network 3.9M, and Joint 1.4M (totally\\n114M).\\n4.2 Data Preprocessing\\nWe preprocess LibriSpeech [2] data and apply speed perturbation with rates 0.9and 1.1\\n(3x audio data), using the published preprocessing script5to make our pipeline compara-\\nble with published models.\\nWe use log-mel filterbanks extracted from audio every 10ms with the window 25ms\\nand apply SpecAugment [35] in training. We use the vocabulary of 1024 BPE [36] tokens\\nextracted using SentencePiece [37] library for text units.\\n4.3 Model and Training Pipeline\\nWe set up training of our model for 200 epochs using AdamW [38] optimizer with Co-\\nsine annealing [39] learning rate schedule with a linear warmup for 40 epochs and the\\nmaximum learning rate of 5e−3. For experiments except for the baseline, we stopped\\ntraining the model after 60 epochs since we are interested in the relative difference in\\nmodel quality, and achieving the best possible accuracy is not our priority at this stage.\\nWe are reporting the results for the best checkpoint chosen on the dev-other validation\\nset. For all experiments, we maintain a global batch size of 2048. We are training models\\non clusters using NVIDIA A100 (mixed-precision with bfloat16) and V100 GPUs (float32\\nfull-precision), and depending on the availability of the resources varying local batch size\\nfrom 8 to 32 to fit into memory and adjusting gradient accumulation to make the global\\nbatch size constant. We did not observe any difference in quality for a fixed global batch\\nsize when using an arbitrary number of nodes, varying local batch size, and using mixed\\nor full precision. So, we are not reporting these details for each experiment.\\n4.4 Proposed Losses\\nIn our work, we propose three modifications of the RNN-T loss:\\n• Star-Transducer to dial with arbitrary deletions\\n• Bypass-Transducer to solve the case of insertions\\n• Target-Robust-Transducer, which is the combination of the previous modifications,\\nallows to mitigate the problems of substitutions in target texts and also can be used\\nas a universal loss when the type of errors is unknown.\\n4github.com/NVIDIA/NeMo/blob/v1.21.0/examples/asr/conf/fastconformer/fast-conformer_transducer_bpe.yaml\\n5https://github.com/NVIDIA/NeMo/blob/v1.21.0/scripts/dataset_processing/get_librispeech_data.py\\n16The modifications are implemented in minrnnt/losses subpackage. All the classes\\nfollow Graph-RNNT [1] framework and inherit GraphRnntLoss class from the NeMo [31]\\nframework and reuse its methods. The implementation uses k2 [14] library. As described\\nin Section 3.6, the computational lattice can be constructed as a composition of temporal\\nand unit schemas, implemented in get_temporal_schema and get_unit_schema respec-\\ntively. The faster implementation constructs the lattice directly in get_grid method. In\\nthe initial development, we used the composition and then made the get_grid implemen-\\ntation as a faster option. We also customize the forward method to assign appropriate\\nscores to the arcs corresponding to special tokens, as described below.\\nFor all losses, we add unit tests (in tests directory) to make the sanity check for the\\nfollowing:\\n• Graphs produced by composition of the temporal and unit schemas are equivalent\\nto the graph produced by get_grid method.\\n• When the weight of special arcs is −∞, this is the equivalent of removing such\\narcs from a computational graph ( e−∞= 0, such a transition does not contribute to\\nloss computation); and the loss should be equivalent to original RNN-T loss. This\\nis tested by comparing the loss value and gradient based on random input for the\\nproposed loss and etalon RNN-T implementation.\\nThe graph construction is debugged visually in the Jupyter Notebook [40] using auto-\\nmatic visualization from the k2 [14] library with GraphViz package [41].\\n4.4.1 Star-Transducer (Star-T)\\nWe propose a simple but effective modification of the RNN-T loss computational graph\\nto solve the problem of deletions. Star Transducer takes into account, along with the\\nalignments with blank labels, the sequences when the blank label is substituted with a\\nspecial \"skip frame\" ⟨sf⟩symbol, which can be viewed as an allowance to skip frames\\nproduced by the encoder in training time. This approach is similar to the \"*\" token used\\nin Star Temporal Classification [16] loss for CTC. For such frames, the transcription is\\nmissing in the ground truth, and the core idea was to allow skipping such frames when\\nconsidering all possible alignments for loss computation. We add parallel arcs to those\\nwith⟨b⟩label to achieve this, as shown in 4c. Unlike other arcs, the weight for this arc is a\\nhyperparameter and assigned directly after populating the lattice with other weights.\\nThe Star-Transducer loss is implemented in the GraphStarTransducerLoss class.\\n4.4.2 Bypass-Transducer (Bypass-T)\\nFor dealing with insertions, we propose a modification of the RNN-T computational graph,\\nadding arcs with a special \"skip token\" ⟨st⟩symbol, inspired by Bypass Temporal Classifi-\\ncation [17] approach. These arcs are parallel to the arcs with tokens. This means that the\\nloss can consider alignments where some tokens are skipped. Fig. 5 shows the temporal\\nand unit schemas and the full constructed lattice.\\nThe Bypass-Transducer loss is implemented in the GraphBypassTransducerLoss class.\\nIn our experiments, we found that assigning constant weight similar to the Star-Transducer\\napproach does not work. With small absolute values (e.g, 0or−3) the model is prone\\nto produce deletions, and the system behaves worse than the original RNN-T. With high\\n17(a) Star-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) Star-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) Star-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 4: WFSTs for Star-Transducer. ⟨sf⟩is a special symbol indicating skipping the\\nframe.\\nabsolute weight values (e.g., −20), such transitions do not contribute to the loss compu-\\ntation: e−20is close to zero, and the loss is close to the original RNN-T. Similar to the ap-\\nproaches applied in the paper about BTC [17], we apply a schedule to the penalty weight\\n(skip_token _penalty ), combined with the probability derived from the output of the Joint\\nnetwork. For the probability we considered different options ( skip_token _mode parameter\\nin the implementation):\\n• \"constant\": only penalty constant, similar to one used in the Star-Transducer loss.\\n• \"mean\": mean probability for all labels (in log scale) excluding blank, similar to\\nBTC [17].\\n• \"max\": maximum log-probability for all labels excluding blank.\\n• \"maxexcl\": maximum of the log probabilities of all labels excluding blank and ground\\ntruth labels.\\n• \"sumexcl\": logarithm of the sum of the probabilities (in log scale) of all labels exclud-\\ning blank and ground truth labels.\\nWe found that the \"mean\" and \"max\" options were not better than the original RNN-T\\nloss. The \"maxexcl\" option was the first working solution used in the Preliminary Report.\\nThe intuition behind the \"sumexcl\" option is to assign the \"unused\" probability of outputs.\\n18(a) Bypass-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) Bypass-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) Bypass-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 5: WFSTs for Bypass-Transducer. ⟨st⟩is a special symbol indicating skipping the\\ntoken.\\nThe \"sumexcl\" option allows for the alignments to be considered when the network outputs\\na high probability for any token other than the target as \"appropriate.\" We found that the\\n\"sumexcl\" option outperforms other cases, as discussed further in Section 5.4.\\n4.4.3 Target-Robust-Transducer (TRT)\\nTarget-Robust-Transducer loss is a combination of Star-Transducer and Bypass-Transducer.\\nWe add both types of arcs that allow skipping frames and tokens, as shown in Fig. 6. It\\nis worth mentioning that assigning −∞weight for \"skip frame\" arcs makes the loss iden-\\ntical to Bypass-Transducer (skipping frames is not allowed in this case), and −∞ weight\\nfor \"skip token\" arcs makes it similar to Star-Transducer (skipping tokens is not allowed).\\nThis makes this loss a universal replacement for the previous two modifications (but the\\nsystem makes more computations since the arcs are still present, even with −∞weight).\\nWe also test this behavior in unit tests.\\nThe Target-Robust-Transducer loss is implemented in the class\\nGraphTargetRobustTransducerLoss . The implementation combines hyperparameters and\\ncode for GraphStarTransducerLoss and GraphBypassTransducerLoss .\\n19(a) Target-Robust-Transducer Unit Schema. Labels:\\n(text_unit, unit _position ) :text_unit\\n(b) Target-Robust-Transducer Time Schema. Labels:\\ntext_unit :frame _number\\n(c) Target-Robust-Transducer Lattice. Labels: (text_unit, unit _position ) :frame _number\\nFigure 6: WFSTs for Target-Robust-Transducer. ⟨sf⟩is a special symbol indicating skip-\\nping the frame. ⟨st⟩is a special symbol indicating skipping the token.\\nTable 1: Baseline on LibriSpeech, WER [%].\\nSourcedev test\\nclean other clean other\\nNeMo 2.0 5.0 2.2 5.0\\nOurs (200 epochs) 2.1 4.9 2.2 5.1\\nOurs (60 epochs) 2.6 6.8 2.8 6.8\\nOurs (100 epochs) 2.4 5.9 2.5 6.0\\n5 Evaluation\\n5.1 Baseline\\nThe results for our implementation are shown in Table 1. For comparison, we use a\\npublicly available Fast Conformer checkpoint6trained on LibriSpeech data for 200 epochs.\\nOur implementation provides results comparable to those of the state-of-the-art pipeline.\\n6https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/stt_en_fastconformer_transducer_large_ls\\n20Table 2: Training RNN-T on data with errors, Fast Conformer, 60 epochs, WER [%].\\nType Corrupt %dev test WERD ↓\\nclean other clean other\\n– 2.6 6.8 2.8 6.8\\nDEL 20% 4.3 9.9 4.7 10.3 3.5\\nDEL 50% 79.2 81.7 80.3 81.4 74.6\\nSUB 20% 4.0 9.4 3.9 9.7 2.9\\nSUB 50% 11.5 23.2 11.2 23.8 17.0\\nINS 20% 4.0 10.3 4.2 10.2 3.4\\nINS 50% 5.1 12.7 5.3 13.5 6.7\\nSo we can proceed further and investigate the system behavior of corrupted targets.\\nAdditionally, we show the results for 60 and 100 epochs (also using the best checkpoint\\nselected on dev-other for these epochs): to save computational resources, we evaluate\\ndifferent cases, training the models for 60 epochs, and for the final case with arbitrary\\nerrors we train the system for 100 epochs.\\n5.2 Error Impact Exploration\\nTo explore the training pipeline on partially incorrect transcripts, we generate additional\\ntraining data sets by mutating the original training texts with the mutation probability pm\\nof 20% and 50%. We are randomly removing words for the \"deletions\" case. We use\\nrandomly selected words from the training vocabulary for substitutions and insertions,\\nsubstituting/inserting words with the probability pm.\\nTable 2 shows the training results on corrupted transcripts. With a small amount of\\ncorruption, all cases lead to system degradation, but the difference between cases is\\ntiny (from 2.9% to 3.5% absolute WER degradation on test-other). We found that the\\ndeletions are most disruptive for the high corruption rate of 50%, and the ASR system\\ncan not achieve a reasonable quality (81.4% WER on test-other compared to 6.8% on\\noriginal data). Thus, we prioritized the work with this part of the problem. Substitutions\\nare the next hard case for RNN-T, which is the opposite of observations for the behavior\\nof CTC systems in [18].\\n5.3 Dealing with Deletions: Star Transducer\\nFor the setup when the ground truth transcripts contain deletions, we apply Star-Transducer\\nloss as a drop-in replacement for the RNN-T loss. The results of training the model are\\nshown in Table 3. In both scenarios, we can close the gap between the baseline for more\\nthan 70%: 77.1% WERDR for 20% deletions and 94.4% for 50%. We found that the train-\\ning is stable even without penalty, but applying the small constant penalty for the \"skip\\nframe\" transition ( −0.5) improves the quality when the number of deletions is low. We\\nwere surprised that such a simple solution works and that modifying the autoregressive\\nprediction network is unnecessary. This can mean that the encoder and joint are more\\nsensitive to incorrect transcripts than the prediction network.\\n21Table 3: Star-Transducer (Star-T) Loss for deletions, Fast Conformer, 60 epochs, WER\\n[%].\\nLossSkipDEL %dev test WERD ↓WERDR ↑\\nWeight clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T - 20% 4.3 9.9 4.7 10.3 3.5\\nStar-T 0 20% 3.9 8.2 4.3 8.5 1.7 51.4%\\nStar-T -0.5 20% 3.1 7.5 3.4 7.6 0.8 77.1%\\nRNN-T - 50% 79.2 81.7 80.3 81.4 74.6\\nStar-T 0 50% 5.1 10.6 5.2 11.0 4.2 94.4%\\nStar-T -0.5 50% 5.4 12.4 5.9 12.5 5.7 92.4%\\nTable 4: Bypass-Transducer (Bypass-T) Loss for insertions, Fast Conformer, 60 epochs,\\nWER [%].\\nLossSkipINSdev test WERD ↓WERDR ↑\\nWeight clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T – 20% 4.0 10.3 4.2 10.2 3.4\\nBypass-T -6 20% 3.0 7.5 3.3 7.9 1.1 67.6%\\nRNN-T – 50% 5.1 12.7 5.3 13.5 6.7\\nBypass-T -6 50% 3.9 10.3 4.3 10.5 3.7 44.8%\\nBypass-T -5 50% 3.6 9.2 4.0 9.4 2.6 61.2%\\n5.4 Dealing with Insertions: Bypass Transducer\\nFor insertions case, we apply the Bypass-Transducer loss described in Section 4.4.2. The\\nresults are shown in the Table 4. The transition weight for the \"skip token\" arcs is a sum of\\nthe constant weight and the total log-probability of all outputs excluding blank and target\\n(\"sumexcl\" option), as discussed in the Section 4.4.2. The training starts with a constant\\nweight of −20.0and is adjusted with the decay after each epoch (starting 3rd epoch):\\nweight next =min (max_weight, weight ∗decay ). We use decay = 0.9for all experiments.\\nTable 4 also reports the maximum constant penalty applied in training. The proposed loss\\ncan restore more than 60% of the system quality for the texts with insertions. Further\\nevaluation of the \"sumexcl\" and \"maxexcl\" options for assigning the weight can be found\\nin Appendix B.\\n5.5 Dealing with Substitutions: Target-Robust-Transducer\\nWe apply Target-Robust-Transducer for the case with substitutions since any \"substitu-\\ntion\" can be viewed as a combination of \"deletion\" and \"insertion.\" The results are shown\\nin Table 5. In the preliminary experiments, we found that assigning low absolute values\\nfor weights ( 0for skip frame as in Star-Transducer, and −5or−6for skip token as in\\n22Table 5: Target Robust Transducer (TRT) Loss for substitutions, Fast Conformer, 60\\nepochs, WER [%].\\nLossSkipSUBdev test WERD ↓WERDR ↑\\ntoken,frame clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T - 20% 4.0 9.4 3.9 9.7 2.9\\nTRT -8,-0.5 20% 3.4 8.2 3.8 8.5 1.7 41.4%\\nRNN-T - 50% 11.5 23.2 11.2 23.8 17.0\\nTRT -8,-0.5 50% 8.2 16.3 8.5 17.0 10.2 45.3%\\nTRT -8,-1 50% 6.9 16.0 7.1 15.8 9.0 47.1%\\nTable 6: Target Robust Transducer (TRT) Loss for arbitrary errors, Fast Conformer, 60\\nand 100 epochs, WER [%]. 50% of data is corrupted, using 15% for each class of errors.\\nLoss Epochs ERRdev test WERD ↓WERDR ↑\\nclean other clean other\\nRNN-T 60 - 2.6 6.8 2.8 6.8\\nRNN-T 60 50% 4.2 10.2 4.3 10.1 3.3\\nTRT 60 50% 3.3 8.0 3.6 8.4 1.6 51.5%\\nRNN-T 100 – 2.4 5.9 2.5 6.0\\nRNN-T 100 50% 3.5 9.4 3.8 9.5 3.5\\nTRT 100 50% 2.9 7.0 3.2 7.0 1.0 71.4%\\nBypass-Transducer) results in fast model overfitting, but when the penalty is more sig-\\nnificant, the training is stable. Since the loss can skip both frames and tokens, apply-\\ning a more significant penalty is reasonable. We use −8for skip frame penalty and the\\n\"sumexcl\" option for assigning the weight, which we found the best when experimenting\\nwith Bypass-Transducer, along with the penalty schedule, as discussed above. With the\\nproposed loss, we can restore more than 40% of the system degradation on test-other :\\nwe achieve WERDR of 41.4% for 20% substitutions and 47.1% for 50%.\\n5.6 Arbitrary Errors\\nFor evaluating the system trained with Target-Robust-Transducer loss, we construct the\\nextra training data by corrupting only 50% of all utterances. For each corrupted utterance,\\nwe apply random substitutions, insertions, and deletions with probability for each type of\\n15%. We consider such case closer to the actual conditions used for production systems\\nwhen the well-curated datasets are mixed with unreliable data from different sources.\\nAlso, we train the system for longer (100 epochs). As shown in Table 6, we are able to re-\\nstore more than 51% system quality when the system is trained for 60 epochs (compared\\nto RNN-T baseline also trained for 60 epochs). When trained for an extra 40 epochs,\\nthe system can restore more than 71% quality (WERDR 71.4%). In the Appendix A, we\\n23also publish the learning curves for the system demonstrating its effectiveness in reducing\\nsubstitutions and deletions on the dev-clean data.\\n6 Conclusion\\nIn our project, we trained speech recognition neural systems on the LibriSpeech [2]\\ndataset. We explored the system’s robustness to errors in target texts by artificially cor-\\nrupting the ground truth target texts from the dataset. We also explored different RNN-T\\nloss modifications to solve the problem of quality degradation in the discussed scenarios\\nand proposed three losses:\\n•Star-Transducer , which mitigates the effect of missing words in transcripts and is\\nable to restore more than 90% of the system quality in such case\\n•Bypass-Transducer , which allows insertions (extra words) in the transcripts and\\nallows the restoration of more than 60% of the quality in such cases compared to\\n\"clean\" transcripts\\n•Target-Robust-Transducer , which combines the approaches applied in the previ-\\nous two losses. This loss can deal with arbitrary types of errors. It improves the\\nsystem’s quality when some words of transcripts are incorrect (substitutions), miti-\\ngating more than 40% of the quality loss for this case. For arbitrary types of errors,\\nwe also show that it can restore more than 70% of the quality compared to the\\nbaseline with the well-transcribed data.\\nOur work is based on the previous solutions for CTC loss [16, 17, 18] and Graph-RNN-\\nT framework [1], and proposes a novel valuable solution for RNN-Transducer-based ASR\\nsystems. We demonstrated the effectiveness of the losses using the Fast Conformer [12]\\nmodel.\\nThe proposed Target-Robust-Transducer system can be applied in real-world scenar-\\nios when training models on a large amount of data from unreliable sources that usually\\ncontain transcription errors.\\nWe also see direct applications for Star-Transducer beyond the discussed case with\\nmissing words in transcripts. Modern ASR systems are trained not only to provide tran-\\nscription (in words) but also to provide punctuation, e.g., Whisper [42]. Since many cu-\\nrated ASR corpora do not contain punctuation (e.g., LibriSpeech [2] which we use in our\\nwork), such missing punctuation can be viewed as \"deletions\" in the transcripts, and with\\nStar-Transducer loss the model can be trained directly on a mixture of datasets with and\\nwithout punctuation.\\nIn further work, we plan to investigate other models (e.g., Conformer [9] with 4x sub-\\nsampling) and datasets. We also plan to apply the losses to train ASR systems on a large\\nscale for production usage.\\nThe losses are planned to be proposed to the open-source NeMo [31] framework.\\n7 Report Parameters and Additional Notes\\nThe implementation is published in the GitHub repository: https://github.com/artbataev/\\nuol_final .\\nThe report contains 6 tables and 6 figures. The appendix contains an additional 1\\ntable and 1 figure. We comply with word limits for each section.\\n248 Acknowledgments\\nI want to express my gratitude to my employer, NVIDIA Corporation, for providing compu-\\ntational resources for this research.\\nReferences\\n[1] A. Laptev, V. Bataev, I. Gitman, and B. Ginsburg, “Powerful and extensible wfst frame-\\nwork for rnn-transducer losses,” in ICASSP 2023 - 2023 IEEE International Confer-\\nence on Acoustics, Speech and Signal Processing (ICASSP) , 2023.\\n[2] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “LibriSpeech: an ASR corpus\\nbased on public domain audio books,” in ICASSP , 2015.\\n[3] A. Graves, “Sequence transduction with Recurrent Neural Networks,” in ICML: work-\\nshop on representation learning , 2012.\\n[4] J. Li, “Recent advances in end-to-end automatic speech recognition,” APSIPA Trans-\\nactions on Signal and Information Processing , vol. 11, no. 1, 2022.\\n[5] A. Graves, S. Fernández, F . Gomez, and J. Schmidhuber, “Connectionist temporal\\nclassification: labelling unsegmented sequence data with recurrent neural networks,”\\ninProceedings of the 23rd international conference on Machine learning , 2006, pp.\\n369–376.\\n[6] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network\\nfor large vocabulary conversational speech recognition,” in 2016 IEEE international\\nconference on acoustics, speech and signal processing (ICASSP) . IEEE, 2016, pp.\\n4960–4964.\\n[7] R. Prabhavalkar, T. Hori, T. N. Sainath, R. Schluter, and S. Watanabe, “End-to-end\\nspeech recognition: A survey,” IEEE/ACM Transactions on Audio, Speech, and Lan-\\nguage Processing , vol. 32, pp. 325–351, 2023.\\n[8] M. Ghodsi, X. Liu, J. Apfel, R. Cabrera, and E. Weinstein, “Rnn-transducer with\\nstateless prediction network,” in ICASSP 2020 - 2020 IEEE International Conference\\non Acoustics, Speech and Signal Processing (ICASSP) , 2020, pp. 7049–7053.\\n[9] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y . Zhang, J. Yu, W. Han, S. Wang, Z. Zhang,\\nY . Wu, and R. Pang, “Conformer: Convolution-augmented Transformer for speech\\nrecognition,” in Interspeech , 2020.\\n[10] Huggingface: Open ASR leaderboard. [Online]. Available: https://huggingface.co/\\nspaces/hf-audio/open_asr_leaderboard\\n[11] A. Vaswani, N. M. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser,\\nand I. Polosukhin, “Attention is all you need,” in Neural Information Processing Sys-\\ntems , 2017.\\n[12] D. Rekesh, N. R. Koluguri, S. Kriman, S. Majumdar, V. Noroozi, H. Huang,\\nO. Hrinchuk, K. Puvvada, A. Kumar, J. Balam et al. , “Fast conformer with linearly\\nscalable attention for efficient speech recognition,” in 2023 IEEE Automatic Speech\\nRecognition and Understanding Workshop (ASRU) . IEEE, 2023, pp. 1–8.\\n25[13] A. Laptev, S. Majumdar, and B. Ginsburg, “Ctc variations through new wfst topolo-\\ngies,” in Interspeech , 2021.\\n[14] D. Povey, P . ˙Zelasko, and S. Khudanpur, “Speech recognition with next-generation\\nKaldi (k2, Lhotse, Icefall),” Interspeech: tutorials , 2021.\\n[15] X. Cai, J. Yuan, Y . Bian, G. Xun, J. Huang, and K. Church, “W-CTC: a connectionist\\ntemporal classification loss with wild cards,” in ICLR , 2022.\\n[16] V. Pratap, A. Hannun, G. Synnaeve, and R. Collobert, “Star Temporal Classification:\\nSequence classification with partially labeled data,” in NeurIPS , 2022.\\n[17] D. Gao, M. Wiesner, H. Xu, L. P . Garcia, D. Povey, and S. Khudanpur, “Bypass Tem-\\nporal Classification: Weakly Supervised Automatic Speech Recognition with Imper-\\nfect Transcripts,” in Proc. INTERSPEECH 2023 , 2023, pp. 924–928.\\n[18] D. Gao, H. Xu, D. Raj, L. P . G. Perera, D. Povey, and S. Khudanpur, “Learning\\nfrom flawed data: Weakly supervised automatic speech recognition,” arXiv preprint\\narXiv:2309.15796 , 2023.\\n[19] J. S. Garofolo, “Timit acoustic phonetic continuous speech corpus,” Linguistic Data\\nConsortium, 1993 , 1993.\\n[20] A. Graves and J. Schmidhuber, “Framewise phoneme classification with bidirectional\\nlstm and other neural network architectures,” Neural networks , vol. 18, no. 5-6, pp.\\n602–610, 2005.\\n[21] A. Y . Hannun, V. Pratap, J. Kahn, and W.-N. Hsu, “Differentiable weighted finite-state\\ntransducers,” ArXiv , vol. abs/2010.01003, 2020.\\n[22] Librivox: Free public domain audiobooks. [Online]. Available: https://librivox.org\\n[23] Y . He, T. N. Sainath, R. Prabhavalkar, I. McGraw, R. Álvarez, D. Zhao, D. Rybach,\\nA. Kannan, Y . Wu, R. Pang, Q. Liang, D. Bhatia, Y . Shangguan, B. Li, G. Pundak,\\nK. C. Sim, T. Bagby, S. yiin Chang, K. Rao, and A. Gruenstein, “Streaming end-to-\\nend speech recognition for mobile devices,” ICASSP 2019 - 2019 IEEE International\\nConference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 6381–6385,\\n2018.\\n[24] E. Variani, D. Rybach, C. Allauzen, and M. Riley, “Hybrid autoregressive transducer\\n(hat),” ICASSP , 2020.\\n[25] Z. Meng, T. Chen, R. Prabhavalkar, Y . Zhang, G. Wang, K. Audhkhasi, J. Emond,\\nT. Strohman, B. Ramabhadran, W. R. Huang, E. Variani, Y . Huang, and P . J. Moreno,\\n“Modular hybrid autoregressive transducer,” SLT, 2022.\\n[26] Z. Tian, J. Yi, Y . Bai, J. Tao, S. Zhang, and Z. Wen, “Fsr: Accelerating the infer-\\nence process of transducer-based models by applying fast-skip regularization,” arXiv\\npreprint arXiv:2104.02882 , 2021.\\n[27] Y . Wang, Z. Chen, C. yong Zheng, Y . Zhang, W. Han, and P . Haghani, “Accelerating\\nrnn-t training and inference using ctc guidance,” ICASSP 2023 - 2023 IEEE Interna-\\ntional Conference on Acoustics, Speech and Signal Processing (ICASSP) , pp. 1–5,\\n2022.\\n26[28] Word Error Rate, “Word error rate — Wikipedia, the free encyclopedia,” 2023,\\n[Online; accessed 2024-01-05]. [Online]. Available: https://en.wikipedia.org/wiki/\\nWord_error_rate\\n[29] Levenshtein Distance, “Levenshtein distance — Wikipedia, the free encyclopedia,”\\n2023, [Online; accessed 2024-01-05]. [Online]. Available: https://en.wikipedia.org/\\nwiki/Levenshtein_distance\\n[30] F . Chollet, “Xception: Deep learning with depthwise separable convolutions,” 2017\\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pp. 1800–\\n1807, 2016.\\n[31] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman,\\nS. Beliaev, V. Lavrukhin, J. Cook, P . Castonguay, M. Popova, J. Huang, and\\nJ. Cohen, “NeMo: a toolkit for building AI applications using neural modules,”\\narXiv:1909.09577 , 2019.\\n[32] W. Falcon and The PyTorch Lightning team, “PyTorch Lightning,” Mar. 2019.\\n[Online]. Available: https://github.com/Lightning-AI/lightning\\n[33] Weights & biases: The developer-first mlops platform. [Online]. Available:\\nhttps://wandb.ai/\\n[34] A. Paszke, S. Gross, F . Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\\nN. Gimelshein, L. Antiga et al. , “Pytorch: An imperative style, high-performance deep\\nlearning library,” Advances in neural information processing systems , vol. 32, 2019.\\n[35] D. S. Park, W. Chan, Y . Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le,\\n“SpecAugment: A simple data augmentation method for automatic speech recogni-\\ntion,” Interspeech , 2019.\\n[36] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of rare words\\nwith subword units,” in Proceedings of the 54th Annual Meeting of the Association\\nfor Computational Linguistics , 2016.\\n[37] T. Kudo and J. Richardson, “SentencePiece: A simple and language independent\\nsubword tokenizer and detokenizer for neural text processing,” in Proceedings\\nof the 2018 Conference on Empirical Methods in Natural Language Processing:\\nSystem Demonstrations , E. Blanco and W. Lu, Eds. Brussels, Belgium:\\nAssociation for Computational Linguistics, Nov. 2018, pp. 66–71. [Online]. Available:\\nhttps://aclanthology.org/D18-2012\\n[38] I. Loshchilov and F . Hutter, “Decoupled weight decay regularization,” in ICLR , 2019.\\n[39] ——, “SGDR: Stochastic gradient descent with warm restarts,” in ICLR , 2017.\\n[40] T. Kluyver, B. Ragan-Kelley, F . Pérez, B. Granger, M. Bussonnier, J. Frederic, K. Kel-\\nley, J. Hamrick, J. Grout, S. Corlay, P . Ivanov, D. Avila, S. Abdalla, and C. Willing,\\n“Jupyter notebooks – a publishing format for reproducible computational workflows,”\\ninPositioning and Power in Academic Publishing: Players, Agents and Agendas ,\\nF . Loizides and B. Schmidt, Eds. IOS Press, 2016, pp. 87 – 90.\\n27[41] J. Ellson, E. Gansner, L. Koutsofios, S. C. North, and G. Woodhull, “Graphviz— open\\nsource graph drawing tools,” in Graph Drawing , P . Mutzel, M. Jünger, and S. Leipert,\\nEds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2002, pp. 483–484.\\n[42] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, “Robust\\nspeech recognition via large-scale weak supervision,” in International Conference on\\nMachine Learning . PMLR, 2023, pp. 28 492–28 518.\\n28Appendices\\nA Arbitrary Errors - Learning Curve\\nWe provide an additional plot with the learning curve, demonstrating WER and its com-\\nponents for RNN-T and Target Robust Transducer training with arbitrary errors for 100\\nepochs, as discussed in Section 5.6. We can see in Figure 7 that the number of in-\\nsertions produced in all cases is similar, but the number of deletions and substitutions\\nproduced by the system trained on corrupted data with the TRT loss is significantly lower\\nthan for RNN-T and is close to the number of errors produced by the RNN-T on the original\\nnon-corrupted data. The screenshot is produced by WandB [33].\\nFigure 7: Arbitrary errors: learning curves for RNN-T (original and corrupted data) and\\nTarget Robust Transducer (corrupted data).\\nB Bypass-Transducer: Extended evaluation of hyperpa-\\nrameters\\nIn this appendix section, we show the extended hyperparameter evaluation of the op-\\ntions for Bypass-Transducer loss regarding assigning weights for skip token transitions.\\nIn initial experiments, as discussed in Section 4.4.2, we tried different options and found\\nthat the system is trainable only with \"maxexcl\" and \"sumexcl\" options. In the system\\nexploration process, we found the \"sumexcl\" option, which considers total \"unassigned\"\\nlog-probability (log-probability for all outputs excluding blank and target labels) to provide\\nthe best value. We provide an extended version of the Table 4. The results in 7 show that\\nthe \"sumexcl\" option outperforms the \"maxexcl\" by a significant margin.\\n29Table 7: Bypass-Transducer Loss for insertions, Fast Conformer, 60 epochs, WER [%].\\nExtended evaluation.\\nLossSkipINSdev test WERD ↓WERDR ↑\\nWeight,Mode clean other clean other\\nRNN-T - – 2.6 6.8 2.8 6.8\\nRNN-T – 20% 4.0 10.3 4.2 10.2 3.4\\nBypass-T -6,maxexcl 20% 3.2 7.9 3.3 8.0 1.2 65.7%\\nBypass-T -6,sumexcl 20% 3.0 7.5 3.3 7.9 1.1 67.6%\\nRNN-T – 50% 5.1 12.7 5.3 13.5 6.7\\nBypass-T -6,maxexcl 50% 4.4 10.3 4.1 10.7 3.9 41.8%\\nBypass-T -6,sumexcl 50% 3.9 10.3 4.3 10.5 3.7 44.8%\\nBypass-T -5,maxexcl 50% 4.1 10.2 4.5 10.4 3.6 46.3%\\nBypass-T -5,sumexcl 50% 3.6 9.2 4.0 9.4 2.6 61.2%\\nC Losses Visualization\\nWe additionally publish the Jupyter Notebook, which visualizes the lattices of the pro-\\nposed losses. The notebook can be found in the repository\\nhttps://github.com/artbataev/uol_final/blob/main/notebooks/Loss_Demo.ipynb .\\n30'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import Concatenate\n",
    "# read text from pdf\n",
    "raw_text = ''\n",
    "for i, page in enumerate(pdfreader.pages):\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "        raw_text += content\n",
    "raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d277d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Initialize the connection to your database:\n",
    "cassio.init(token=astra_db_app_token,database_id=astra_db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8916ade2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r7/xbd75fl13wvf24ktcp53cf1h0000gn/T/ipykernel_88321/483216564.py:2: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
      "  llm = OpenAI(openai_api_key=os.environ['openai_key'])\n",
      "/var/folders/r7/xbd75fl13wvf24ktcp53cf1h0000gn/T/ipykernel_88321/483216564.py:3: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding = OpenAIEmbeddings(openai_api_key=os.environ['openai_key'])\n"
     ]
    }
   ],
   "source": [
    "## Create the LangChain embedding and LLM objects for later usage:\n",
    "llm = OpenAI(openai_api_key=os.environ['openai_key'])\n",
    "embedding = OpenAIEmbeddings(openai_api_key=os.environ['openai_key'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8597d010",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create langchain vector db\n",
    "astra_vector_store = Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name=\"qa_mini_demo\",\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66517c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the text chnks then embeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2a558b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50 headlines.\n"
     ]
    }
   ],
   "source": [
    "## load the dataset into vector db\n",
    "astra_vector_store.add_texts(texts[:50])\n",
    "\n",
    "print(\"Inserted %i headlines.\" % len(texts[:50]))\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
